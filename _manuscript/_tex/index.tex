\documentclass[manuscript,screen,review,anonymous]{acmart}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}

%% PANDOC PREAMBLE BEGINS

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
%% PANDOC PREAMBLE ENDS

\setlength{\parindent}{10pt}
\setlength{\parskip}{0pt}

\hypersetup{
  pdftitle={The Name of the Title Is Hope},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc, via quarto}}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI'26]{CHI Conference on Human Factors in Computing
Systems}{Apr 13--17, 2026}{Barcelona, Spain}
\acmPrice{}
\acmISBN{978-1-4503-XXXX-X/18/06}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Hope]{The Name of the Title Is Hope}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


  \author{H. Sherry Zhang}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
        \author{Roger D. Peng}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
      

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}
%%  
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
bla blabla    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
    <concept_id>10010405.10010497.10010504.10010505</concept_id>
    <concept_desc>Applied computing~Document analysis</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
  <concept>
    <concept_id>10010405.10010432.10010437.10010438</concept_id>
    <concept_desc>Applied computing~Environmental sciences</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
   <concept>
    <concept_id>10003120.10003121.10003126</concept_id>
    <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Applied computing~Document analysis}
\ccsdesc[500]{Applied computing~Environmental sciences}
\ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large language models}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\setlength{\parskip}{-0.1pt}

\section{Introduction}\label{introduction}

Something about ``analysis review'' - Roger thinks it's a better to have
a new word for this.

provide a baseline understand - place to start

demonstrate - analytically homogeneous - the table won't look like that

In this work, we design a tabular format to record the choices made by
analysts during data analysis. Using large language models, we
automatically extract these choices from a set of research papers
focused on specific topics, e.g.~air pollution modelling. This allows us
to analyze these choices as data -- tracking how they've changed over
time or query the possible methodologies used in similar studies. We
also introduce a workflow to cluster paper based on decision similarity,
using both the decisions themselves and the justifications authors
provide for their choices.

\section{Background}\label{background}

Data analysis as an complicated, iterative process to make sense
{[}ref{]} of the data collected. The iterative process of formulating
hypothesis \citet{jun2022}.

Choices are made at nearly every stage of data analysis, ranging from
variable pre-processing variables, variable and lag selection in model
formulation, to the specification of smoothing parameter during model
construction. These possible choices contribute to what
\citet{gelman2014} describe as the ``garden of forking paths''. These
choices can introduce substantial variability in results, which has been
demonstrated in many-analyst experiments, where independent teams
analyzing the same dataset to answer a pre-defined research question
often arrive at markedly different conclusions. A prominent example is
\citet{silberzahn2018} where researchers reported a wide range of point
estimates and 95\% confidence intervals for the effect of soccer
players' skin tone on the number of red cards awarded by referees (odds
ratio from 0.89 to 2.93). Similar findings have emerged in other
domains, including structural equation modeling \citep{sarstedt2024},
applied microeconomics \citep{huntington-klein2021}, neuroimaging
\citep{botvinik-nezer2020}, and ecology and evolutionary biology
\citep{gould2025}.

Another line of work focuses on developing software tools to support
analysts in making more informed decisions. For example, the
\texttt{Tisane} package \citep{jun2022} integrates conceptual ideas,
such as DAGs, and modelling structure (group/ cluster/ hierarchical
structure), to assist junior researchers in specifying GLM and GLMM
model. The \texttt{DeclareDesign} package \citep{blair2019} introduces
the MIDA framework for researchers to declare, diagnose, and redesign
their analyses to produce a distribution of the statistic of interest.
This approach has been applied in randomized controlled trial
\citep{bishop2024} .

The \texttt{multiverse} package

\begin{itemize}
\tightlist
\item
  facilitates the specification and execution of multiple parallel
  choices for sensitivity analysis, allowing researchers to
  systematically explore how different choices affect results and to
  report the range of plausible outcomes that arise from alternative
  analytic paths.
\end{itemize}

Study decisions in data analysis:

\begin{itemize}
\tightlist
\item
  interview analysts and researchers to provide recommendation for data
  analysis practices \citep{kale2019, alspaugh2019, liu2020}.
\item
  \citet{liu2020} provides visualization to communicate the decision
  processes through the Analytic Decision Graphs (ADG)
\item
  \citet{simson2025} conducts a participatory AI study to demonstrate
  the ``garden of forking paths'' of decisions in data analysis and how
  it affects ML fairness
\end{itemize}

\section{Extracting decisions from data
analysis}\label{extracting-decisions-from-data-analysis}

\subsection{Decisions in data analysis}\label{sec-decisions}

\begin{itemize}
\tightlist
\item
  what constitute a decision in data analysis
\item
  adapt from the tidy data principle - each row is a decision
  \citet{wickham2014}
\item
  some decisions are related to how the variable is estimated spatially
  and temporally
\item
  model level decisions on how the model is estimated spatially (for
  multi-site analyses) and/or temporally (different treatments for years
  or seasons)
\item
  extract the exact text from the paper
\end{itemize}

An example decisions may look as follows:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ostro & 1 & Poisson regression & temperature & smoothing spline & degree
of freedom & parameter & NA & 3 degree of freedom \\
ostro & 2 & Poisson regression & temperature & smoothing spline & degree
of freedom & temporal & NA & 1-day lag \\
ostro & 3 & Poisson regression & relative humidity & LOESS & smoothing
parameter & parameter & to minimize Akaike's Information Criterion &
NA \\
ostro & 4 & Poisson regression & model & NA & NA & spatial & to account
for variation among cities & separate regression models fit in each
city \\
\end{longtable}

However, decisions statements are often implicit, and the justifications
may not directly align with the decisions themselves. We identify four
common anomalies:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Authors may combine multiple decisions into a single sentence}
  for coherence and conciseness of the writing. Consider the following
  excerpt from \citet{ostro2006}:
\end{enumerate}

\begin{quote}
Other covariates, such as day of the week and smoothing splines of 1-day
lags of average temperature and humidity (each with 3 df), were also
included in the model because they may be associated with daily
mortality and are likely to vary over time in concert with air pollution
levels.
\end{quote}

This sentence contains four decisions: two for temperature (the temporal
lag and the smoothing spline parameter) and two for humidity. These
decisions should be structured as separate entries.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{The justification does not directly address the decision
  choice.} In the example above, the stated rationale (``and are likely
  to vary over time in concert with air pollution levels'') supports the
  general inclusion of temporal lags but does not justify the specific
  choice of 1-day lag over alternatives, such as 2-day average of lags 0
  and 1 (lag01) and single-day lag of 2 days (lag2). As such, the reason
  field should be recorded as NA.
\item
  \textbf{Some decisions may be omitted because they are data-driven}.
  For instance, \citet{katsouyanni2001} states:
\end{enumerate}

\begin{quote}
The inclusion of lagged weather variables and the choice of smoothing
parameters for all of the weather variables were done by minimizing
Akaike's information criterion.
\end{quote}

In this case, while the method of selection (minimizing AIC) is
specified, the actual degree of freedom used is not. Such data-driven
decisions may be recorded with ``NA'' in the decision field, but the
reason field should still be recorded as ``by minimizing Akaike's
information criterion''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Information required to interpret the decision may be
  distributed across multiple sections}. In the previous example,
  ``weather variables'' refers to mean temperature and relative
  humidity, as defined earlier in the text. This requires
  cross-referencing across sections to identify the correct variables
  associated with each modeling choice.
\end{enumerate}

\subsection{Automatic reading of literature with
LLMs}\label{automatic-reading-of-literature-with-llms}

While decisions can be extracted manually from the literature, this
process is labor-intensive and time-consuming. Recent advances in Large
Language Models (LLMs) have demonstrated potential for automating the
extraction of structured information from unstructured text {[}ref{]}.
In this work, we use LLMs to automatically identify decisions made by
authors during their data analysis processes.

Text recognition from PDF document relies on Optical Character
Recognition (OCR) to convert scanned images into machine-readable text
-- capability currently offered by Antropic Claude and Google Gemini. We
instruct the LLM to generate a markdown file containing a JSON block
that records extracted decisions, which can then be read into
statistical software for further analysis. The exact prompt feed to the
LLM is provided in the Appendix. The \texttt{ellmer} package
\citep{ellmer} in R is used to connect to the Gemini and Claude API,
providing the PDF attachment and the prompt in a markdown file as
inputs. \textbf{?@fig-llm} shows the overall workflow for decision
extract using LLMs.

\subsection{Review the LLM output}\label{review-the-llm-output}

\begin{itemize}
\tightlist
\item
  something about result validation of LLM output
\item
  The sensitivity of the two models to the prompt and the model
  parameters, such as temperature and seed, is discussed in
  Section~\ref{sec-sensitivity}.
\end{itemize}

The shiny app is designed to provide users a visual interface to review
and edit the decisions extracted by the LLM from the literature. The app
allows three actions from the users: 1) \emph{overwrite} -- modify the
content of a particular cell, equivalently
\texttt{dplyr::mutate(xxx\ =\ ifelse(CONDITION,\ "yyy"\ ,\ xxx))}, 2)
\emph{delete} -- remove a particular cell,
\texttt{dplyr::filter(!(CONDITION))}, and 3) \emph{add} -- manually
enter a decision, \texttt{dplyr::bind\_rows()}. Figure~\ref{fig-shiny}
illustrates the \emph{overwrite} action in the Shiny application, where
users interactively filter the data and preview the rows affected by
their edits---in this case, changing the model entry from ``generalized
additive Poisson time series regression'' to the less verbose ``Poisson
regression''. Upon confirmation, the corresponding \texttt{tidyverse}
code is generated, and users can download the edited table and
incorporate the code into their R script.

\phantomsection\label{cell-fig-shiny}
\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface for editting
Large Language Model (LLM)-generated decisions (overwrite, delete, and
add). (1) the default interface after loading the input CSV file. (2)
The table view will update interactively upon the user-defined filter
condition -- expressed using \texttt{dplyr::filter()} syntax (e.g.,
\texttt{paper\ ==\ anderson2008size"}), (3) The user edits the
\texttt{model} column to ``Poisson regression'' and applies the change
by clicking the Apply changes button. The table view updates to reflect
the changes (4) After clicking the Confirm button, the corresponding
\texttt{tidyverse} code is generated, and the table view returns to its
original unfiltered view. The edited data can be downloaded by clicking
the Download CSV button.}

\end{figure}%

\textsubscript{Source:
\href{https://huizezhang-sherry.github.io/paper-decisions/index.qmd.html}{Article
Notebook}}

\section{Calculating paper
similarity}\label{calculating-paper-similarity}

\begin{itemize}
\tightlist
\item
  pre-processing

  \begin{itemize}
  \tightlist
  \item
    standardize statistical methods its corresponding parameters (LOESS,
    smoothing spline, etc)
  \item
    group variables into broader categories: time, temperature,
    humidity, PM
  \end{itemize}
\item
  identify the most frequent analysis decisions across papers
\item
  retain only papers that report more than x such decisions
\item
  measure similarity between decisions and their justificaiton using NLP

  \begin{itemize}
  \tightlist
  \item
    word embedding with attention mechanism, instead of bag of word,
  \item
    specific NLP models (default to \texttt{bert-base-uncased}),
    aggregation methods from word to text
  \end{itemize}
\item
  compute paper similarity score for each paper pair by aggregating
  decision-level compoarisons

  \begin{itemize}
  \tightlist
  \item
    check/ report on the number of decisions compared in each paper pair
  \end{itemize}
\item
  similarity score can serve as the distance matrix to cluster papers by
  their similarity on decision choices
\end{itemize}

\section{Results}\label{results}

\subsection{Air pollution mortality modelling}\label{sec-result}

Decision quality summary

\begin{itemize}
\tightlist
\item
  look at for one type of decision (time) - what are the choices made by
  different papers
\item
  look at whether decisions changes across time
\item
  Visualize the decision database: apply clustering algorithm and
  visualize the database through \texttt{sigma.js}
\end{itemize}

\subsection{Sensitivity analysis}\label{sec-sensitivity}

sensitivity of the pipeline: 1) LLM, 2) text model, 3) prompt, 4) LLM
parameters

\begin{itemize}
\item
  standard \texttt{BERT} \citep{devlin2019}, \texttt{Roberta}
  \citep{liu}: trained on a much larger dataset (160GB v.s. BERT's
  15GB), \texttt{transformer-xl} \citep{dai}, \texttt{xlnet} by Google
  Brain \citep{yang}, and two domain-trained BERT models:
  \texttt{sciBert} \citep{beltagy2019} and
  \texttt{bioBert}\citep{lee2020}, trained on PubMed and PMC data.
\item
  A section on reproducibility of LLM outputs: prompt experiment (see if
  there are papers discussing this: https://arxiv.org/pdf/2406.06608)
\end{itemize}

\section{Discussion}\label{discussion}

\begin{itemize}
\tightlist
\item
  Only prompting engineering is used to extract decisions from the
  literature. We expect that fine-tuning the model on statistical or
  domain-specific literature to yield more robust performance on the
  same document, though it would require substantially more training
  effort.
\item
  people from the NYU-LMU workshop are interested to have code script
  attached as well because people can do one thing in the script but
  report another in the paper - it would be interesting to compare the
  paper and the script with some syntax extraction.
\item
  Validation of the output:
\end{itemize}

the nature of the task: Our task involve a reasoning component in that
it requires casual reasoning to identify the decisions made by the
authors, and its justification/ rationale, rather than purely
summarizing the text through pattern-matching.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\renewcommand{\bibsection}{}
\bibliography{references.bib}

%% begin pandoc before-bib
%% end pandoc before-bib
%% begin pandoc biblio
%% end pandoc biblio
%% begin pandoc include-after
%% end pandoc include-after
%% begin pandoc after-body
%% end pandoc after-body

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
