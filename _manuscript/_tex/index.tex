\documentclass[manuscript,screen,review,anonymous]{acmart}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}

%% PANDOC PREAMBLE BEGINS

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
%% PANDOC PREAMBLE ENDS

\setlength{\parindent}{10pt}
\setlength{\parskip}{0pt}

\hypersetup{
  pdftitle={The Name of the Title Is Hope},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc, via quarto}}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI'26]{CHI Conference on Human Factors in Computing
Systems}{Apr 13--17, 2026}{Barcelona, Spain}
\acmPrice{}
\acmISBN{978-1-4503-XXXX-X/18/06}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title[Hope]{The Name of the Title Is Hope}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


  \author{H. Sherry Zhang}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
        \author{Roger D. Peng}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
      

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}
%%  
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
bla blabla    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
    <concept_id>10010405.10010497.10010504.10010505</concept_id>
    <concept_desc>Applied computing~Document analysis</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
  <concept>
    <concept_id>10010405.10010432.10010437.10010438</concept_id>
    <concept_desc>Applied computing~Environmental sciences</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
   <concept>
    <concept_id>10003120.10003121.10003126</concept_id>
    <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Applied computing~Document analysis}
\ccsdesc[500]{Applied computing~Environmental sciences}
\ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large language models}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\setlength{\parskip}{-0.1pt}

\begin{itemize}
\tightlist
\item
  Something about ``analysis review'' - Roger thinks it's a better to
  have a new word for this.
\item
  provide a baseline understand - place to start
\item
  demonstrate - analytically homogeneous - the table won't look like
  that
\end{itemize}

\section{Introduction}\label{introduction}

Decisions are everywhere in data analysis, from the initial data
collection, data pre-processing to the modelling choices. These
decisions will impact the final output of the data analysis, which may
lead to different conclusions and policy recommendations. When such
flexibility can be misused---through practices such as p-hacking,
selective reporting, or unjustified analytical adjustments---it can
inflate effect sizes or produce misleading results that meet
conventional thresholds for statistical significance. They have been
demonstrated through many-analysts experiments, where independent teams
analyzing the same dataset to answer a pre-defined research question
often arrive at markedly different conclusions. These practices not only
compromise the validity of individual studies but also threaten the
broader credibility of statistical analysis and scientific research as a
whole.

Multiple recommendations have been proposed to improve data analysis
practices, such as pre-registration and multiverse analysis. Bayesian
methods also offer a different paradigm to p-value driven inference for
interpreting statistical evidence. Most empirical studies of data
analysis practices focus on specially designed and simplified analysis
scenarios. While informative, these setups may not adequately capture
the complexity of the data analysis with significant policy
implications. {[}In practice, studying the data analysis decisions with
actual applications is challenging.{]} Analysts may no longer be
available for interviews due to job changes, and even when they are,
recalling the full set of decisions and thinking process made during the
analysis is often infeasible. Moreover, only until the last decades,
analysis scripts and reproducible materials were not commonly required
by journals for publishing. {[}As a result, it remains challenging to
study how analytical decisions are made. {]}

In this work, we focus on a specific class of air pollution modelling
studies that estimate the effect size of particulate matter (PM2.5 or
PM10) on mortality, typically using Poisson regression or generalized
additive models (GAMs). While individual modelling choices vary, these
studies often share a common structure: they adjust for meteorological
covariates such as temperature and humidity, apply temporal or spatial
treatments, like including lagged variables and may estimate the effect
by city or region before combining results. Because these studies
investigate similar scientific questions using a shared modelling
framework, they form a natural many-analyst setting. This allows us to
examine, in a real-world context, the range of analytical decisions made
by different researchers addressing the same underlying question.

In this work, we develop a structured tabular format to record the
analytical decisions made by researchers in the air pollution modelling
literature. Using large language models (LLMs), we automate the
extraction of these decisions from published papers. This allows us to
treat decisions as data -- allowing us to track them over time, compare
methodology across papers, and query commonly used approaches. We
further introduce a workflow to cluster studies based on decision
similarity, revealing three distinct groups of papers that reflect the
modelling strategies differs in the European and U.S. studies, which
offers a new way to visualize the field in the air pollution mortality
modelling.

The rest of the paper is organized as follows. In
Section~\ref{sec-background}, we review the background on data analysis
decisions. Section~\ref{sec-extract-decisions} describes the data
structure for recording decisions, the use of large language models to
process research papers, and the validation of LLM outputs. In
Section~\ref{sec-paper-similarity}, we present the method for
calculating paper similarity based on decision similarities.
Section~\ref{sec-result} reports the finding of our analysis, including
the clustering of papers according to similarity scores and sensitivity
analyses related to LLM providers, prompt engineering, and LLM
parameters. Finally, Section~\ref{sec-discussion} discusses the
implications of our study.

\section{Background}\label{sec-background}

\subsection{Decisions in data
analysis}\label{decisions-in-data-analysis}

\textbf{Question} Is ``decision'' going to be confusing with
``decision-making'' in decision theory

A data analysis is a process of making choices at each step, from the
initial data collection to model specification, and post-processing.
Each decision represents a branching point where analysts choose a
specific path to follow, and the vast number of possible choices
analysts can take forms what \citet{gelman2014} describe as the ``garden
of forking paths''. While researchers may hope their inferential results
are robust to the specific path taken through the garden, in practice,
different choices can lead to substantially different conclusions. This
has been empirically demonstrated through ``many analyst experiments'',
where independent research groups analyze the same dataset to the same
answer using their chosen analytic approach. A classic example is
\citet{silberzahn2018}, where researchers reported an odds ratio from
0.89 to 2.93 for the effect of soccer players' skin tone on the number
of red cards awarded by referees. Similar variability has been observed
in structural equation modeling \citep{sarstedt2024}, applied
microeconomics \citep{huntington-klein2021}, neuroimaging
\citep{botvinik-nezer2020}, and ecology and evolutionary biology
\citep{gould2025}. Many studies have been conducted on a relatively
smaller scale to interviews of analysts and researchers on data analysis
practices \citep{kale2019, alspaugh2019, liu_understanding_2020},
visualization of the decision process through the analytic decision
graphics (ADG) \citep{liu2020}. Recently, \citet{simson2025} describes a
participatory approach to decisions choices in fairness ML algorithms.

Software tools have also developed to incorporate potential alternatives
in the analysis workflow, including the \texttt{DeclareDesign} package
\citep{blair2019} and the \texttt{multiverse} package
\citep{multiverse}. The \texttt{DeclareDesign} package \citep{blair2019}
introduces the MIDA framework for researchers to declare, diagnose, and
redesign their analyses to produce a distribution of the statistic of
interest, which has been applied in the randomized controlled trial
study \citep{bishop2024}. The \texttt{multiverse} package
\citep{multiverse} provides a framework for researchers to
systematically explore how different choices affect results and to
report the range of plausible outcomes that arise from alternative
analytic paths.

\textbf{TODO} Something about the context on air pollution mortality
modelling @ Roger

\section{Extracting decisions from data
analysis}\label{sec-extract-decisions}

\subsection{Decisions in data analysis}\label{sec-decisions}

Decisions occur throughout the entire data analysis process -- from the
selection of variables and data source, to pre-processing steps to
prepare the data for modelling, to the model specification and variable
inclusion. In this work, we focus specifically on modelling decisions in
the air pollution mortality modelling literature. These include the
choice of modelling approach, covariate inclusion and smoothing, and
specifications of spatial and temporal structure. Consider the following
excerpt from \citet{ostro2006}:

\begin{quote}
Based on previous findings reported in the literature (e.g., Samet et
al.~2000), the basic model included a smoothing spline for time with 7
degrees of freedom (df) per year of data. This number of degrees of
freedom controls well for seasonal patterns in mortality and reduces and
often eliminates autocorrelation.
\end{quote}

This sentence encode the following components of a decision:

\begin{itemize}
\tightlist
\item
  \textbf{variable}: time
\item
  \textbf{method}: smoothing spline
\item
  \textbf{parameter}: degree of freedom (df)
\item
  \textbf{reason}: Based on previous findings reported in the literature
  (e.g., Samet et al.~2000); This number of degrees of freedom controls
  well for seasonal patterns in mortality and reduces and often
  eliminates autocorrelation.
\item
  \textbf{decision}: 7 degrees of freedom (df) per year of data
\end{itemize}

The decision above is regarding a certain parameter in the statistical
method, we categorize this as a ``parameter'' type decisions. Other
types of decisions - such as spatial modelling structure or the
inclusion of temporal lags - may not include an explicit method or
parameter, but still reference a variable and rationale, which we will
provide further examples below.

To record these decisions, we follow the tidy data principle
\citep{wickham2014}, where each variable should be in a column, each
observation in a row. In our context, each row represents a decision
made by the authors of a paper and an analysis often include multiple
decisions. To retain the original context of the decision, we extract
the original text in the paper, without paraphrase or summarization,
from the paper. Below we present an example of how to structure the
decisions made in a paper, using the paper by \citet{ostro2006}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ostro & 1 & Poisson regression & temperature & smoothing spline & degree
of freedom & parameter & NA & 3 degree of freedom \\
ostro & 2 & Poisson regression & temperature & smoothing spline & degree
of freedom & temporal & NA & 1-day lag \\
ostro & 3 & Poisson regression & relative humidity & LOESS & smoothing
parameter & parameter & to minimize Akaike's Information Criterion &
NA \\
ostro & 4 & Poisson regression & model & NA & NA & spatial & to account
for variation among cities & separate regression models fit in each
city \\
\end{longtable}

Most decisions in the published papers are not explicitly stated, this
could due to the coherence and conciseness of the writing or authors'
decision to include only necessary details. Here, we identify a few
common anomalies where decisions may be combined or omit certain fields:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Authors may combine multiple decisions into a single sentence}
  for coherence and conciseness of the writing. Consider the following
  excerpt from \citet{ostro2006}:
\end{enumerate}

\begin{quote}
Other covariates, such as day of the week and smoothing splines of 1-day
lags of average temperature and humidity (each with 3 df), were also
included in the model because they may be associated with daily
mortality and are likely to vary over time in concert with air pollution
levels.
\end{quote}

This sentence contains four decisions: two for temperature (the temporal
lag and the smoothing spline parameter) and two for humidity. These
decisions should be structured as separate entries.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{The justification does not directly address the decision
  choice.} In the example above, the stated rationale (``and are likely
  to vary over time in concert with air pollution levels'') supports the
  general inclusion of temporal lags but does not justify the specific
  choice of 1-day lag over alternatives, such as 2-day average of lags 0
  and 1 (lag01) and single-day lag of 2 days (lag2). As such, the reason
  field should be recorded as NA.
\item
  \textbf{Some decisions may be omitted because they are data-driven}.
  For instance, \citet{katsouyanni2001} states:
\end{enumerate}

\begin{quote}
The inclusion of lagged weather variables and the choice of smoothing
parameters for all of the weather variables were done by minimizing
Akaike's information criterion.
\end{quote}

In this case, while the method of selection (minimizing AIC) is
specified, the actual degree of freedom used is not. Such data-driven
decisions may be recorded with ``NA'' in the decision field, but the
reason field should still be recorded as ``by minimizing Akaike's
information criterion''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Information required to interpret the decision may be
  distributed across multiple sections}. In the previous example,
  ``weather variables'' refers to mean temperature and relative
  humidity, as defined earlier in the text. This requires
  cross-referencing across sections to identify the correct variables
  associated with each modeling choice.
\end{enumerate}

\subsection{Automatic reading of literature with
LLMs}\label{automatic-reading-of-literature-with-llms}

\textbf{TODO}: Prompt engineering: these models may paraphrase or
hallucinate unless explicitly told not to since it is generative in
nature based on the predicted probability of the next word from the text
and the instruction

\textbf{TODO}: The Prompt Report: A Systematic Survey of Prompt
Engineering Techniques \url{https://arxiv.org/pdf/2406.06608}

While decisions can be extracted manually from the literature, this
process is labor-intensive and time-consuming. Recent advances in Large
Language Models (LLMs) have demonstrated potential for automating the
extraction of structured information from unstructured text {[}ref{]}.
In this work, we use LLMs to automatically identify decisions made by
authors during their data analysis processes.

Text recognition from PDF document relies on Optical Character
Recognition (OCR) to convert scanned images into machine-readable text
-- capability currently offered by Antropic Claude and Google Gemini. We
instruct the LLM to generate a markdown file containing a JSON block
that records extracted decisions, which can then be read into
statistical software for further analysis. The exact prompt feed to the
LLM is provided in the Appendix. The \texttt{ellmer} package
\citep{ellmer} in R is used to connect to the Gemini and Claude API,
providing the PDF attachment and the prompt in a markdown file as
inputs.

\subsection{Review the LLM output}\label{review-the-llm-output}

\begin{itemize}
\tightlist
\item
  \textbf{TODO} something about result validation of LLM output
\end{itemize}

The shiny app is designed to provide users a visual interface to review
and edit the decisions extracted by the LLM from the literature. The app
allows three actions from the users: 1) \emph{overwrite} -- modify the
content of a particular cell, equivalently
\texttt{dplyr::mutate(xxx\ =\ ifelse(CONDITION,\ "yyy"\ ,\ xxx))}, 2)
\emph{delete} -- remove a particular cell,
\texttt{dplyr::filter(!(CONDITION))}, and 3) \emph{add} -- manually
enter a decision, \texttt{dplyr::bind\_rows()}. Figure~\ref{fig-shiny}
illustrates the \emph{overwrite} action in the Shiny application, where
users interactively filter the data and preview the rows affected by
their edits---in this case, changing the model entry from ``generalized
additive Poisson time series regression'' to the less verbose ``Poisson
regression''. Upon confirmation, the corresponding \texttt{tidyverse}
code is generated, and users can download the edited table and
incorporate the code into their R script.

\phantomsection\label{cell-fig-shiny}
\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface for editting
Large Language Model (LLM)-generated decisions (overwrite, delete, and
add). (1) the default interface after loading the input CSV file. (2)
The table view will update interactively upon the user-defined filter
condition -- expressed using \texttt{dplyr::filter()} syntax (e.g.,
\texttt{paper\ ==\ anderson2008size"}), (3) The user edits the
\texttt{model} column to ``Poisson regression'' and applies the change
by clicking the Apply changes button. The table view updates to reflect
the changes (4) After clicking the Confirm button, the corresponding
\texttt{tidyverse} code is generated, and the table view returns to its
original unfiltered view. The edited data can be downloaded by clicking
the Download CSV button.}

\end{figure}%

\section{Calculating paper similarity}\label{sec-paper-similarity}

Once the decisions have been extracted and validated, this opens up a
structured data for analyzing these information. For example, we can
compare whether author's choices at different times changes, or across
decisions varies at different regions. In this section, we present a
method to calculate paper similarity based on the decisions shared in
the paper pairs. The goal is to construct a distance metric based on
similarity of the decision choice among papers that could be further
used for clustering paper based on choices made by different authors in
the literature. An overview of the method is illustrated in
Figure~\ref{fig-similarity-diag}.

\phantomsection\label{cell-fig-similarity-diag}
\begin{figure}[H]

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/similarity-diag.png}

}

\caption{\label{fig-similarity-diag}Workflow for calculating paper
similarity based on decision choices: (1) standardize variable names,
(2) identify most frequent variable-type decisions across all papers,
(3) identify papers with at least x identified decisions, (4) calculate
decisions similarity score on the \emph{decision} and \emph{reason}
fields using transformer language models, e.g.~BERT, (5) calculate paper
similarity score based on aggregating decision similarity scores.}

\end{figure}%

\begin{itemize}
\tightlist
\item
  \textbf{TODO} some discussion on what it means by for two papers to be
  similar based on decisions.
\end{itemize}

The calculation of paper similarity is based on the similarity of
decisions shared by each paper pair. A decision comparable in two papers
are the ones that share the same variable and type, e.g.~temperature and
parameter (a decisions on the choosing the statistical method
\emph{parameter} for the \emph{temperature} variable), or humidity and
temporal (any \emph{temporal} treatment, e.g.~choice of lag value for
the \emph{humidity} variable). While many decisions share a similar
variable, different authors may refer to them with slightly different
names, such as ``mean temperature'' and ``average temperature'', hence
variable names are first standardized to a common set of variable names.
For example, ``mean temperature'' and ``average temperature'' are both
standardized to ``temperature''. Notice that ``dewpoint temperature'' is
standardized into ``humidity'' since it is a proxy of temperature to
achieve a relative humidity (RH) of 100\%. For literature with a common
theme, there is usually a set of variables that shared by most papers
and additional variables are justified in individual research. For our
air pollution mortality modelling literature, we standardize the
following variable names:

\begin{itemize}
\tightlist
\item
  \textbf{temperature}: ``mean temperature'', ``average temperature'',
  ``temperature'', ``air temperature'', ``ambient temperature''
\item
  \textbf{humidity}: ``dewpoint temperature'' and its hyphenated
  variants, relative humidity'', ``humidity''
\item
  \textbf{PM}: ``pollutant'', ``pollution'', ``particulate matter'',
  ``particulate'', ``PM10'', ``PM2.5''
\item
  \textbf{time}: ``date'', ``time'', ``trends'', ``trend''
\end{itemize}

Depending on the specific pairs, papers have varied number of decisions
that can be compared and aggregated. While paper similarities can be
computed for all paper pairs, using the similarity of one or two pair of
decisions to represent paper similarity is less ideal. Hence, before
calculating the text similarity of decisions, we also include two
optional steps to identify and subset the most frequent decisions across
papers, and to retain only papers that report more than a certain number
of frequent decisions. Research questions in different fields may have
different levels of homogeneity, depending on the maturity of the field
and for air pollution mortality modelling, it is helpful to focus on
decisions and papers that share a substantial number of decisions.

To assign numerical value for the similarity of reason, we use a
transformer language model, such as BERT, to measure the semantic text
similarity between the decision itself and its justification. The
decision similarity is calculated by comparing the \emph{decision} and
\emph{reason} fields of the decisions in each paper pair. To obtain
paper similarity, we average the decision similarities across all
decisions in each paper pair and other method can be customized for
aggregation. The resulting paper similarity score can be used as a
distance matrix to cluster papers based on their decision choices to
understand the common practices in the investigated literature.

\section{Results}\label{sec-result}

\subsection{Air pollution mortality
modelling}\label{air-pollution-mortality-modelling}

\begin{itemize}
\tightlist
\item
  Given examples of the failure of LLM models for parsing and examples
  where authors are unclear about the delivery
\end{itemize}

The results follows examines {[}x{]} papers for modelling the effect of
particulate matters on mortality based on Gemini for parsing the
decision choices. The results from Anthropic Claude is reported in
Section~\ref{sec-sensitivity}.

Specify how much of validation and review has been done

Decision quality summary

\begin{itemize}
\tightlist
\item
  missingness of the reason and decisions for the paper - how often
  papers report decisions
\item
  look at for one type of decision (time) - what are the choices made by
  different papers
\item
  look at whether decisions changes across time (cluster diagram with
  year)
\item
  Visualize the decision database: apply clustering algorithm and
  visualize the clusters
\item
  a characterization of the field, what are the common variables
  included, what smoothing methods are used, what are the options for
  temporal lags often considered, how are models generally estimated
  spatially.
\item
  For \texttt{lee2006association}, it is not clear what specific
  smoothing method the sentence ``smooth function of the day of study''
  refers to.
\end{itemize}

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/unnamed-chunk-4-1.pdf}}

}

\caption{bla bla bla}

\end{figure}%

\subsection{Sensitivity analysis}\label{sec-sensitivity}

In this section, we examine the reproducibility for using LLMs for text
extraction tasks (Section~\ref{sec-llm-parameters}), discrepancies
between different LLM providers: Claude and Gemini
(Section~\ref{sec-llm-providers}), and the sensitivity of our paper
similarity calculation pipeline to the choice of text model used for
computing decision similarity scores (Section~\ref{sec-text-model}).

\subsubsection{LLM parameters}\label{sec-llm-parameters}

Model parameters such as temperature, top-p, and top-k control the
randomness of the output in LLMs. For our text extraction task, we find
setting a temperature of 1 generally generate stable outputs. For each
paper, we repeat the text extraction 5 times, with temperature = 1, to
assess the reproducibility of Gemini. From Table~\ref{tbl-gemini-1},
among the 62 papers investigated, 31 (50\%) yields the same number of
decisions extracted across all five runs. For two papers, three versions
are produced. In \citet{ito2006}, a set of weather models is used for
sensitivity analysis, while Gemini generates variations in the extent to
which these decisions are incorporated. In \citet{huang2009}, the term
``smoothing function'' appears prior to the use of the actual penalized
spline to describe the smoothing method. Gemini may capture one or both
of these in its extraction.

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-gemini-1}Number of different decisions from Gemini}

\tabularnewline

\toprule\noalign{}
Number & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 31 \\
2 & 29 \\
3 & 2 \\

\end{longtable}

\begin{longtable}[]{@{}rrr@{}}

\caption{\label{tbl-gemini-2}Number of differences in the reason and
decision fields across Gemini runs for papers with consistent number of
decisions across runs}

\tabularnewline

\toprule\noalign{}
Num. of difference & Count & Proportion (\%) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 12 & 3.87 \\
2 & 8 & 2.58 \\
3 & 0 & 0.00 \\
4 & 24 & 7.74 \\
5 & 12 & 3.87 \\
6 & 0 & 0.00 \\
7 & 0 & 0.00 \\
8 & 6 & 1.94 \\
9 & 6 & 1.94 \\
10 & 10 & 3.23 \\
11 & 6 & 1.94 \\

\end{longtable}

We further examine text similarity in the ``reason'' and ``decision''
fields for the 31 papers that have the same number of decisions across
runs. Table~\ref{tbl-gemini-2} summarizes the number of differences
observed in each pairwise comparison (5 * 4 / 2 = 10 comparisons for
each of the 31 paper for two fields yields 620 pairwise comparison).
Among all comparisons, 73\% produce the identical text in reason and
decision. The discrepancies come from the following reasons:

\begin{itemize}
\tightlist
\item
  Gemini extracted different length for the same decision, e.g.~in
  \citet{kan2007}, some runs may extract ``singleday lag models
  underestimate the cumulative effect of pollutants on mortality 2day
  moving average \textbf{of current and previous day concentrations}
  (lag=01)'', while others extract ``singleday lag models underestimate
  the cumulative effect of pollutants on mortality 2day moving average
  (lag=01)''. Similarity, for decisions, some runs may yield ``10 df for
  total mortality'', while other runs yield ``10 df''. Similar
  extraction appears in \citet{breitner2009}.
\item
  In \citet{burnett1998}, Gemini fails to extract reasons in the first
  run, while the remaining four runs are identical. Similarly, in
  \citet{ueda2009} and \citet{castillejos2000} , runs 1 and 5 fail to
  extract the reasons and produce the same incomplete version, whereas
  runs 2, 3, and 4 produce accurate versions with reasons populated.
\end{itemize}

\subsubsection{LLM providers}\label{sec-llm-providers}

Reading text from PDF document requires Optical Character Recognition
(OCR) to convert scanned images into machine-readable text. This
capability is currently supported by Antropic Claude and Google Gemini.
We perform the text extraction task of decisions from the literature
with both models.

{[}I'm not sure if we should say this\ldots{]} Our experience shows that
Claude tends to produce more verbose output, which may include more
decisions during data pre-processing and secondary data analysis steps,
while the output from Gemini is more relevant for the modelling choices.

For example, in \citet{dockery1992}, the term ``weather variables'' is
used to include both temperature and dew point temperature --- a measure
for humidity. Gemini lumps both decisions under ``weather variable'' and
fails to reference back to the actual variables the choices are made on.
On the other hand, Claude treats the variable definition for cold day,
hot day indicator etc as decision choices and include them in the
output. In \citet{huang2009}, Claude blends some pre-processing steps of
variables used in the secondary analysis (use of 24 hr average on
varaible NO2, O3, SO2), while Gemini does not. While in \citet{mar2000},
Gemini includes the temporal treatment (lag days of 0 to 4 days) for air
pollution exposure variables studied in the secondary analysis.

\subsubsection{Text model}\label{sec-text-model}

We have conducted sensitivity analysis on the text model for obtaining
the decision similarity score from the Gemini outputs. The tested
language models tested include

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  BERT by Google \citep{devlin2019},
\item
  RoBERTa by Facebook AI \citep{liu}, trained on a larger dataset (160GB
  v.s. BERT's 15GB),
\item
  XLNnet by Google Brain \citep{yang}, and
\end{enumerate}

two domain-trained BERT models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  sciBERT \citep{beltagy2019}, trained on scientific literature, and
\item
  bioBERT \citep{lee2020}, trained on PubMed and PMC data.
\end{enumerate}

Figure~\ref{fig-text-density} presents the distribution of the decision
similarity (left) and paper similarity (right) for each text model. At
decision level, the BERT model produces the widest variation across all
five models, while the similarity scores from XLNet are all close to 1.
These scores are not comparable across models since the difference of
the underlying transformer architecture. However, the paper similarity
scores from each model are comparable and Figure~\ref{fig-text-mds}
shows the multi-dimensional scaling (MDS) of the paper similarity scores
from each text model: all showing a similar clustering pattern of the
three main smoothing methods.

\phantomsection\label{cell-fig-text-density}
\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-density-1.pdf}}

}

\caption{\label{fig-text-density}Distribution of decision similarity
(left) and paper similarity (right) scores for five different text
models (BERT, BioBERT, RoBERTa, SciBERT, and XLNet). The default
language model, BERT, produces the widest variation across the five
models, while the similarity scores form XLNet are all close to 1. The
model BioBERT, RoBERTa, and SciBERT yield decision similar scores mostly
between 0.7 to 1.}

\end{figure}%

\phantomsection\label{cell-fig-text-mds}
\begin{figure}[H]

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-mds-1.pdf}}

}

\caption{\label{fig-text-mds}The multi-dimensional scaling (MDS) of the
paper similarity scores from each text model: all showing a similar
clustering pattern of the three main smoothing methods. The points are
colored by the most common method used in the paper, and the hulls are
drawn around each method group.}

\end{figure}%

\section{Discussion}\label{sec-discussion}

\begin{itemize}
\item
  Address how sensitivity analysis is/ is not relevant
\item
  Only prompting engineering is used to extract decisions from the
  literature. We expect that fine-tuning the model on statistical or
  domain-specific literature to yield more robust performance on the
  same document, though it would require substantially more training
  effort.
\item
  people from the NYU-LMU workshop are interested to have code script
  attached as well because people can do one thing in the script but
  report another in the paper - it would be interesting to compare the
  paper and the script with some syntax extraction.
\item
  Validation of the output:
\end{itemize}

the nature of the task: Our task involve a reasoning component in that
it requires casual reasoning to identify the decisions made by the
authors, and its justification/ rationale, rather than purely
summarizing the text through pattern-matching.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\renewcommand{\bibsection}{}
\bibliography{references.bib}

%% begin pandoc before-bib
%% end pandoc before-bib
%% begin pandoc biblio
%% end pandoc biblio
%% begin pandoc include-after
%% end pandoc include-after
%% begin pandoc after-body
%% end pandoc after-body

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
