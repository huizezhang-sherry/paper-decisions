\documentclass[manuscript,screen,review,anonymous]{acmart}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}

%% PANDOC PREAMBLE BEGINS

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
%% PANDOC PREAMBLE ENDS

\setlength{\parindent}{10pt}
\setlength{\parskip}{0pt}

\hypersetup{
  pdftitle={An LLM-based pipeline for understanding decision choices in data analysis from published literature},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc, via quarto}}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI'26]{CHI Conference on Human Factors in Computing
Systems}{Apr 13--17, 2026}{Barcelona, Spain}
\acmPrice{}
\acmISBN{978-1-4503-XXXX-X/18/06}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{An LLM-based pipeline for understanding decision choices in data
analysis from published literature}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


  \author{H. Sherry Zhang}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
        \author{Roger D. Peng}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
      

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}
%%  
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Decision choices, such as those made when building regression models,
and their rationale are essential for interpreting results and
understanding uncertainty in an analysis. However, these decisions are
rarely studied because tracing every alternatives considered by authors
is often impractical, and reworking a completed analysis is generally of
limited interest. Consequently, researchers must manually review large
bodies of published analyses to identify common choices and understand
how choices are made. In this work, we propose a workflow to
automatically extract analytic decisions and their reasons from
published literature using Large Language Models. Our method also
introduces a paper similarity measure based on decision similarity and
visualization methods using clustering algorithms. As an example, this
workflow is applied to analyses studying the effect of particulate
matter on mortality. This approach enables scalable and automated
studies of decision choices in applied data analysis, providing an
alternative to existing qualitative and interview-based studies.    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
    <concept_id>10010405.10010497.10010504.10010505</concept_id>
    <concept_desc>Applied computing~Document analysis</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
 <concept>
    <concept_id>10003120.10003121.10011748</concept_id>
    <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Applied computing~Document analysis}
\ccsdesc[500]{Human-centered computing~Empirical studies in HCI}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large language models}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\setlength{\parskip}{-0.1pt}

\section{Introduction}\label{introduction}

TODO: need references

Decisions are made at every stage of data analysis, from initial data
collection and preprocessing to modeling. One might expect well-trained
researchers to make similar choices when faced with the same analytical
task, yet evidence suggests otherwise. Many-analyst experiments show
that independent analysts often arrive at markedly different
conclusions, even when analyzing the same dataset to answer the same
research question \citep{silberzahn2018, botvinik-nezer2020, gould2025}.
This variation in analytical decision-making, described by
\citet{gelman2014} as the ``garden of forking paths,'' can undermine the
quality and credibility of reported results and hinder comparability
across studies. For junior researchers who lack guidance, this
variability may lead to over reliance on default statistical software
settings or arbitrary choices made without clear justification.

A common approach to investigate uncertainty in decision choices is
sensitivity analysis, where researchers systematically vary key
decisions in their analysis to assess the robustness of their findings.
Multiverse analysis extends this idea by evaluating \emph{all} plausible
combinations of analytical choices to examine how results vary across
the full decision space \citep{multiverse, blair2019}. However, what an
analyst consider ``reasonable'' is subjective and may not reflect the
full range of options commonly used in practice. Even when a reasonable
set of alternatives is tested, the sensitivity analysis may be of
limited interest to other researchers facing a similar problem, who are
seeking evidence to inform comparable decision choices and their
rationale. Ideally, decision-making in applied research would be studied
by following experienced analysts throughout the entire analysis process
to capture their reasoning. In reality, this is rarely feasible and not
scalable.

While individual studies may not capture the full range of reasonable
decision options, crowdsourcing decisions from a collection of studies
on a shared theme creates a ``many-analyst'' setting that reveals how
analysts make choices and justify them in practice. Classic research
training typically involves reading through the literature to understand
how decisions are made and to learn the common choices. This process now
has the possibility to be automated at scale given recent LLMs' ability
to follow instructions to extract structured information from
unstructured text. In this work, we propose a new approach for studying
data analysis decision choices by automatically extracting decisions
from scientific literature using Large Language Models (LLMs). We
develop a tabular schema to record decisions, automate the extraction
process with LLMs, and introduce a new paper similarity measure based on
decision similarity, which serves as a distance metric for dimension
reduction methods to visualize papers group according to their decision
patterns.

We apply this workflow to a set of 56 air pollution modelling studies
estimating the effect of particulate matter (PM2.5 or PM10) on mortality
and hospital admissions. This type of studies is typically analyzed
using Poisson generalized linear models (GLMs) or generalized additive
models (GAMs). Analysis of the extracted decisions reveals common
choices for decisions considered in this type of studies such as the
number of knots or degree of freedom for smoothing methods and the
temporal lags for time and weather variables. Multi-dimensional scaling
on the paper similarity distance finds three distinct clusters
corresponding to different smoothing methods -- LOESS, natural spline,
and smoothing spline -- used in European and U.S. studies. These
findings align with the APHENA project \citep{APHENA}, which synthesizes
research from multiple studies in Europe and North America by expert
investigators.

In this workflow, we also provide detailed documentation on the
validation and standardization of LLM outputs. Because LLMs generate
results probabilistically, it is not yet clear how these outputs should
be validated for downstream analysis in practice. We outline the
validation and standardization process, including the use of a developed
Shiny application in R for reviewing decisions, the types of edits made
through validation, and secondary standardization of decisions.
Additionally, we conduct sensitivity across different LLM providers and
assess the reproducibility of the text extraction from single LLM
models. We aim to offer guidance for future studies seeking to extract
structured information from unstructured text using LLMs.

In summary, the contribution of this work includes:

\begin{itemize}
\item
  A scalable and automated approach to study data analysis decision
  choices through extracting of decisions from published scientific
  literature using LLMs,
\item
  A new method to construct paper similarities based on the decision and
  the semantic similarity of their rationale,
\item
  A data schema for summarizing decisions made in applied studies in a
  tidy format,
\item
  A shiny GUI tool for validating LLM outputs for editing tables, and
\item
  A dataset of decisions and rationale, along with metadata, compiled
  from 62 studies in air pollution mortality modelling.
\end{itemize}

\section{Related work}\label{sec-background}

\subsection{Analytic decision making in data
analysis}\label{analytic-decision-making-in-data-analysis}

Data analysis is a complex and iterative process
\citep{jun2022, jun2022hypothesis, jun2019} that involves multiple
stages, including data collection, cleaning, visualization, modeling,
and communication. At each stage, analysts make decisions informed by
domain practices, statistical knowledge, and feedback from the data.
These decisions, such as which variables to include in a model, how to
handle missing data, and which statistical methods to use, act as
branching points in the analysis workflow. The full set of possible
paths through these branching points form what \citet{gelman2014}
describe as the ``garden of forking paths''. While one might expect
well-trained researchers will often converge to similar decision
choices, empirical evidence suggests otherwise. ``Many analyst
experiments'' show that independent research groups analyze the same
dataset to address the same research questions can arrive at widely
different conclusions. For example \citet{silberzahn2018} asks 29 groups
of analysts to conduct an analysis to address the same research
questions \emph{whether soccer players with dark skin tone are more
likely than those with light skin tone to receive red cards from
referees}. Researchers reported an estimated effect size from 0.89 to
2.93 in odds ratio, 70\% of the teams found a statistically significant
positive effect while others don't, and 21 unique combinations of
covariates are used by 29 different analyses. Similar experiements has
been observed in structural equation modeling \citep{sarstedt2024},
applied microeconomics \citep{huntington-klein2021}, neuroimaging
\citep{botvinik-nezer2020}, and ecology and evolutionary biology
\citep{gould2025}.

Examples like the above illustrate how analytical decisions introduce
uncertainty into data analysis. These uncertainties have been widely
discussed for policy recommendation \citep{APHENA} and their
applications in health, finance, and other domains. To help researchers
avoid misusing their ``researcher degree of freedom'', guidelines and
checklists have been developed, informed by demonstrations of how such
misuse can lead to p-hacking and inflated effect size
\citep{wicherts2016, simmons2011}. Pre-registration is a common practice
in medicine and biostatistics, yet \citet{pang2022} found that this is
not well-adopted among HCI researchers. Given the nuanced nature of data
analysis, more work have examined how analysts make decisions in
practice, through interviews in both academia and industry. These
studies include qualitative analysis of analytical decisions
\citep{kale2019}, interviews with data analysts about exploratory data
analysis practice in industry \citep{alspaugh2019, kandel2012},
interviews with data workers on how they consider alternatives in data
analysis \citep{liu_understanding_2020}, and interviews researchers
about their analysis decisions in published studies \citep{liu2020}.
Participatory studies, like in the many analyst experiments, are also
used to support participatory input to democratize decisions in fairness
machine learning \citep{simson2025}.

In addition to qualitative studies, software tools have developed to
help researchers account for alternatives and uncertainties in the
analysis workflow and make informed analytical decisions. Examples
include \texttt{Tea} \citep{jun2019}, which support general statistical
analysis; \texttt{Tisane} \citep{jun2022}, which guides choices in
generalized linear mixed-effects models (GLMMs); and
\texttt{MetaExplore} \citep{kale2023}, which allows for meta-analysis to
account for decision uncertainty (epistemic uncertainty) during the
studies. The \texttt{DeclareDesign} package \citep{blair2019} in R
introduces the MIDA framework for researchers to declare, diagnose, and
redesign their analyses to produce a distribution of the statistic of
interest, which has been applied in the randomized controlled trial
study \citep{bishop2024}. Multiverse analysis provides a framework for
researchers to conduct multiverse analysis to systematically explore how
different choices affect results and to report the range of plausible
outcomes that arise from alternative analytic paths. Downstream works
expand on how to author and visualize multiverse analysis
\citep{liu2021}, create R software for multiverse analysis using
tidyverse syntax \citep{multiverse, götz2024}, and debugging tools
\citep{gu2023}.

\subsection{Automatic information extraction with
LLMs}\label{automatic-information-extraction-with-llms}

\subsection{Visualization on scientific
literature}\label{visualization-on-scientific-literature}

With the growing volume of scientific publications and the difficulty of
navigating the literature to stay informed, there is increasing interest
in developing tools to visualize and recommend scientific papers. These
systems link papers based on their similarity and relevance, typically
determined by keywords \citep{isenberg2017}, citation information
(e.g.~citation list, co-citation) \citep{chen2006}, or combinations with
other relevant paper metadata (e.g.~author, title)
\citep{bethard2010, chou2011, dörk2012, heimerl2016}. Recent approaches
incorporate text-based information using topic modelling
\citep{alexander2014}, argumentation-based information retrieval
\citep{tbahriti2006}, and text embedding \citep{narechania2022}. While
metadata and high-level text-based information are useful for finding
relevant papers, researchers also need tools that help them \emph{make
sense} of the literature rather than simply \emph{locating} it. In
applied data analysis, one interest is to understand how studies differ
or align in their analytical approaches. Capturing the decisions and
reasoning expressed in analyses on a shared theme enables the
calculation of similarity metrics based on these choice and their
underlying rationale, which supports clustering and visualizing paper to
identify common practices in the field.

\section{Methods}\label{sec-extract-decisions}

In this section, we present the workflow for extracting decisions from
published literature using Large Language Models (LLMs). We first
describe the data structure for recording decisions, followed by the
four main steps: 1) automatic extraction from literature with LLMs, 2)
validation and standardization of LLM outputs, 3) calculation of paper
similarity, and 4) visualization paper similarity using clustering or
dimension reduction methods. The section concludes with an illustration
summarizing the workflow.

\subsection{Record decisions in data analysis}\label{sec-decisions}

In the study of the health effects of outdoor air pollution, one area of
interest is the association between short-term, day-to-day changes in
particulate matter air pollution and daily mortality counts. This
question has been studied extensively by researchers across the globe
and in the US, it serves to provide scientific evidence for to guide
public policy on setting the National Ambient Air Quality Standards
(NAAQS) for air pollutants. While individual modelling choices vary,
these studies often share a common structure: they adjust for
meteorological covariates such as temperature and humidity, apply
temporal or spatial treatments, like including lagged variables and may
estimate the effect by city or region before combining results. This
naturally forms a ``many-analyst'' experiment setting where different
researchers analyze similar data to address the same scientific question
and the analyses are documented in published papers.

Consider the following excerpt from \citet{ostro2006} that describes the
modelling approach to provide evidence of an association between daily
counts of mortality and ambient particulate matter (PM10):

\begin{quote}
Based on previous findings reported in the literature (e.g., Samet et
al.~2000), the basic model included a smoothing spline for time with 7
degrees of freedom (df) per year of data. This number of degrees of
freedom controls well for seasonal patterns in mortality and reduces and
often eliminates autocorrelation.
\end{quote}

This sentence encode the following components of a decision:

\begin{itemize}
\tightlist
\item
  \textbf{variable}: time
\item
  \textbf{method}: smoothing spline
\item
  \textbf{parameter}: degree of freedom (df)
\item
  \textbf{reason}: Based on previous findings reported in the literature
  (e.g., Samet et al.~2000); This number of degrees of freedom controls
  well for seasonal patterns in mortality and reduces and often
  eliminates autocorrelation.
\item
  \textbf{decision}: 7 degrees of freedom (df) per year of data
\end{itemize}

To record these decisions in a tabular format, we follow the tidy data
principle \citep{wickham2014}, which states each variable should be in a
column and each observation in a row. For our purpose, each row
represents a decision made by the authors in a paper and an analysis
often include multiple decisions. To retain the original context of the
decision, we extract the original text in the paper, without paraphrase
or summarization. The decision choice above is a parameter choice of a
statistical method applied to the variable. Analyses also include other
types of decisions, such as temporal and spatial treatments, for
example, the choice of lagged exposure for certain variables or whether
the model is estimated collectively or separated for individual
locations. These decisions don't have a specific method or parameter,
but should still be recorded with the variable, type (spatial or
temporal), reason, and decision fields.

Given the writing style and the quality of the analysis itself, multiple
decisions may be combined in one sentence and certain fields,
e.g.~decision and reason, may be omitted. Consider the following excerpt
from \citet{ostro2006}:

\begin{quote}
Other covariates, such as day of the week and smoothing splines of 1-day
lags of average temperature and humidity (each with 3 df), were also
included in the model because they may be associated with daily
mortality and are likely to vary over time in concert with air pollution
levels.
\end{quote}

This sentence contains four decisions: two for temperature (the temporal
lag and the smoothing spline parameter) and two for humidity and should
be structured as separate entries:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ostro & 1 & temperature & smoothing spline & degree of freedom &
parameter & 3 degree of freedom & NA \\
ostro & 2 & relative humidity & smoothing spline & degree of freedom &
parameter & 3 degree of freedom & NA \\
ostro & 3 & temperature & NA & NA & temporal & 1-day lags & NA \\
ostro & 4 & relative humidity & NA & NA & temporal & 1-day lags & NA \\
\end{longtable}

Notice in the example above, the reason field are recorded as NA. This
is because the stated rationale (``and are likely to vary over time in
concert with air pollution levels'') only supports the general inclusion
of temporal lags but does not justify the specific choice of 1-day lag
over other alternatives, for example, 2-day average of lags 0 and 1 and
single-day lag of 2 days. Similar scenario can happen when a direct
decision is missing while a reason is provided (``done by minimizing
Akaike's information criterion''), as in \citet{katsouyanni2001}:

\begin{quote}
The inclusion of lagged weather variables and the choice of smoothing
parameters for all of the weather variables were done by minimizing
Akaike's information criterion.
\end{quote}

\subsection{Extract decisions automatically from literature with
LLMs}\label{extract-decisions-automatically-from-literature-with-llms}

Manually extracting decisions from published papers is labor-intensive
and time-consuming. With Large Language Models (LLMs), it has become
possible to automatically extract structured information from
unstructured text by supplying a set of PDF documents and a prompt for
instruction. Text recognition from PDF document relies on Optical
Character Recognition (OCR) to convert scanned images into
machine-readable text -- capability currently offered by Antropic Claude
and Google Gemini. In the prompt, we assign the LLM a role as an applied
statistician and instruct it to generate a markdown file containing a
JSON block that extract decisions from the PDF in the format described
in Section~\ref{sec-decisions}. We also provide a set of instructions
and examples on the potential missing of reason and decision fields.
Prompt engineering techniques \citep{chen2025, xu} are used to optimize
the prompt script. The full prompt feed to the LLM is provided in the
Appendix. We use the \texttt{chat\_PROVIDER()} functions from the
\texttt{ellmer} package \citep{ellmer} in R to obtain the output with
Gemini and Claude API.

\subsection{Validate and standardize LLM
outputs}\label{validate-and-standardize-llm-outputs}

The LLM outputs need to be validated and standardized before further
analysis. Validation focuses on ensuring the correctness of the
extracted decisions by LLMs, while standardization aims to ensure
consistency in variable and model names across papers, given authors may
express the same concept in different ways. For example, ``mean
temperature'', ``average temperature'', and ``temperature'' all refer to
the same variable, which can be all standardized to ``temperature'' for
consistency. To help with the validation and standardization process, we
developed a Shiny application that provides an interactive interface for
users to review and edit the LLM outputs. A Shiny application takes a
CSV of extracted decisions as input and allows three types of edits: 1)
\emph{overwrite} -- modify the content of a particular cell, 2)
\emph{delete} -- remove a particular irrelevant decision, and 3)
\emph{add} -- manually enter a missing decision. Figure~\ref{fig-shiny}
illustrates the \emph{overwrite} action for standardizing the variable
NCtot (The number concentration of urban background particles
\textless100 nm in diameter) to ``pollution'': the user enters a
predicate function in the filter condition box on the left panel, and
the filtered data will appear interactively in the right panel. The user
can then specify the variable to overwrite and the new value and the
corresponding cells in the right panel will be updated. This change need
to be confirmed by pressing the ``Apply changes'' button to update the
full dataset. The corresponding \texttt{tidyverse} \citep{tidyverse}
code will then be generated in the left panel to be included in an R
script, and the edited table can be downloaded for future analysis.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=0.8\textheight]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface to validate
and standardize Large Language Model (LLM)-generated output. (1) the
default interface after loading the input CSV file. (2) The table view
will update interactively to reflect the edit: for paper with handle
``andersen2008size'' and id in 4, 5, 6, replace the variable NCtot with
``pollutant''. (3) After clicking the Confirm button, the corresponding
\texttt{tidyverse} code is generated, and the table view returns to its
original unfiltered view with the edits applied. The edited data can be
downloaded by clicking the Download CSV button.}

\end{figure}%

\subsection{Calculate paper similarity and
visualization}\label{sec-paper-similarity}

Once the output has been extracted and validated, the decisions can be
treated as data for further analysis. In this section, we construct a
distance metric between pairs of papers based on the similarity of their
decision choices. This metric can then be used as a distance matrix
among papers for clustering, dimension reduction, and visualization.

For each paper pair, a decision is considered comparable if the papers
share the same variable and decision type, for example, a parameter
decision on temperature or the temporal decision on humidity. For two
decisions to be considered similar, both the decision choice and the
rationale are taken into account. A similar choice indicates a similar
final decisions are made in the analysis, whereas a similar reason
reflects a shared rationale or justification for the choice, even when
the choices themselves differ, potentially due to differences in the
underlying data. To assign numerical value for measuring the similarity,
we use the semantic similarity from text model, using the \texttt{text}
package \citep{text}. We first obtain the text embedding for all the
reason and decisions and calculate the cosine similarity between the
matched reason and decisions. For parameter type decisions, the
statistical method used also contributes to the similarity of the
decision. Since semantic similarity cannot fully capture the difference
betweenit statistical methods (the difference between smoothing spline
and natural spline is not well represented by the textual difference of
``smoothing'' and ``natural''), method similarity is encoded as binary:
1 if the two papers used the same method, and 0 otherwise. The paper
similarity is then computed as the average similarity across all the
matched methods, decisions, and reasons. The resulting paper similarity
metric can be interpreted as a distance measure to cluster and visualize
papers based on their decision choices.

Because analyses vary in the decisions they report, the number of
matched decisions differs across paper pairs. In practice, some studies
may not fully report the decision and reason for every choice made,
leading to missing data for the matched decisions. Although paper
similarity can be calculated based on all available matched decisions,
cares should be taken for pairs with only a small number of matches, as
the paper similarity may be overly influenced by one or two decisions.
To address this, users may focus on a set of decisions shared across
papers and on papers that report a minimal number of these decisions
when calculating paper similarity.

\subsection{Summary}\label{summary}

Figure~\ref{fig-workflow} summarises the entire workflow for extracting
decisions from published literature using Large Language Models (LLMs)
and analyzing the extracted decisions. Once researchers have identified
a set of literature of interest and a prompt to instruct LLMs to extract
decisions from the literature. The outputs from LLM need to be validated
and standardized before further analysis due to authors' varied writing
styles. The validated data can then be used to conduct exploratory
analysis of decision choices and one task is to calculate paper
similarity based on the decision similarity. This paper similarity
measure can be seen as a distance metric among papers, which can be used
for clustering and dimension reduction for visualizing papers.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/workflow.png}

}

\caption{\label{fig-workflow}The workflow for extracting decisions from
published literature using Large Language Models (LLMs) and analyzing
the extracted decisions. The workflow consists of four main steps: (1)
Extract decisions automatically from literature with LLMs, (2) Validate
and standardize LLM outputs, (3) Calculate paper similarity and
visualization, and (4) visualization with clustering or dimension
reduction methods.}

\end{figure}%

\section{Results}\label{sec-result}

From the 56 studies examining the effect of particulate matters
(\(\text{PM}_{10}\) and \(\text{PM}_{2.5}\)) on mortality and hospital
admission, we focus on the baseline model reported in each paper,
excluding secondary models (e.g.~lag-distributed models) and sensitivity
analysis. We also exclude decisions on other pollutants, such as
nitrogen dioxide (\(\text{NO}_2\)). This yields 242 decisions extracted
using Gemini, averaging approximately 4 decisions per paper.

\subsection{Validation and standardization of LLM
outputs}\label{sec-res-validation}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.9355}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.0645}}@{}}

\caption{\label{tbl-review}Summary of validation and standardization
edits made during the review process.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Count
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Remove decisions out of scope: other pollutants and sensitivity analysis
& 50 \\
Edit made to recode smoothing parametser unit to per year & 45 \\
Duplicates & 9 \\
Fix incorrect capture & 9 \\
Edit made due to decisions are too general, e.g.~minimum of 1 df per
year was required & 6 \\
Remove decisions related to definition of variables, e.g.~season & 5 \\
Total & 124 \\

\end{longtable}

Table~\ref{tbl-review} summarizes the number of edits made during the
review process using the Shiny application. These edits fall into two
main categories: 1) correcting LLM outputs and 2) standardizing
extracted decision. The first category includes fixing incorrect
captures, removing non-decision (e.g.~definition of variables), removing
duplication, excluding irrelevant decisions (e.g.~sensitivity analyses),
and excluding decisions whose stated reasons reflect general guidelines
rather than actual choices (e.g.~``minimum of 1 degree of freedom per
year is required'').

Standardization addresses variation in how authors express variable
names and decisions. For example, variable names such as ``mean
temperature'' and ``average temperature'' refer to the same variable and
should be aligned for comparison for later decision similarity
calculation. Variable names are manually standardized into four main
categories:

\begin{itemize}
\tightlist
\item
  \textbf{temperature}: ``mean temperature'', ``average temperature'',
  ``temperature'', ``air temperature'', ``ambient temperature''
\item
  \textbf{humidity}: ``dewpoint temperature'' and its hyphenated
  variants, relative humidity'', ``humidity''
\item
  \textbf{PM}: ``pollutant'', ``pollution'', ``particulate matter'',
  ``particulate'', ``PM10'', ``PM2.5''
\item
  \textbf{time}: ``date'', ``time'', ``trends'', ``trend''
\end{itemize}

Notice that ``dewpoint temperature'' is standardized under humidity
because it serves as a proxy for temperature in achieving a 100\%
relative humidity.

Decisions themselves also require standardization. For example, the
smoothing parameter (number of knots and degree of freedom) may be
expressed \emph{per year} or \emph{in total}, and temporal lag decision
may be expressed in different formats (e.g.~``6-day average'', ``mean of
lags 0+1'', ``lagged exposure up to 6 days''). Smoothing parameter units
are manually recoded to a \emph{per year} basis for consistency,
asreflected in Table~\ref{tbl-review}. Temporal decision show a wider
variety, generally falling into two categories:

\begin{itemize}
\tightlist
\item
  \textbf{multi-day average lags}, such as ``6-day average'', ``3-d
  moving average'', ``mean of lags 0+1'', ``cumulative lags, mean
  0+1+2'' and
\item
  \textbf{single-day lags}, such as ``lagged exposure up to 6 days'',
  ``lag days from 0 to 5''.
\end{itemize}

This variability makes manual standardization impractical, hence we
apply a secondary LLM process (claude-3-7-sonnet-latest) using the
\texttt{ellmer} package to convert temporal decisions into a consistent
format: \texttt{multi-day:\ lag\ {[}start{]}-{[}end{]}} and
\texttt{single-day:\ lag\ {[}start{]},\ …\ ,lag\ {[}end{]}}. For
instance, ``6-day average'' is converted to ``multi-day: lag 0-5'' and
``lagged exposure up to 6 days'' is converted to ``single-day: lag 0,
lag 1, lag 2, lag 3, lag 4, lag 5''.

\subsection{Exploratory analysis of decision
choices}\label{exploratory-analysis-of-decision-choices}

\begin{table}

\caption{\label{tbl-missing-decisions}Missingness of decision and reason
fields in the Gemini-extracted decisions. Most decisions report the
choice (35.5 + 57.1 = 92\%), but 57.1\% lacks a stated reason.}

\centering{

\begin{tabular}{lll}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision} \\
\cmidrule(l{3pt}r{3pt}){2-3}
Reason & Non-missing & Missing\\
\midrule
Non-missing & 90 (37.2\%) & 14 (5.8\%)\\
Missing & 134 (55.4\%) & 4 (1.7\%)\\
\bottomrule
\end{tabular}

}

\end{table}%

As raised in Section~\ref{sec-decisions}, not all decisions reported in
the literature include both the decision choice and the rationale. Some
decisions may only report the choice without a stated reason, while
others may provide a reason without specifying the exact choice made.
Table~\ref{tbl-missing-decisions} summarizes the missingness of the
decisions and reason for the extracted decisions. While 2\% of decisions
are complete for both decision and reasons, 55\% of decisions lack a
stated rationale for the choice. This reflects a common reporting
practice in the field, where authors often present the decision itself
without providing a justification, e.g.~``We decide to use \(x\) degree
of freedom for variable \(y_1\) and \(y_2\)''. This also includes cases
where authors provide general guidelines for selecting the parameter,
but the rationale is too broad to justify the specific choice made
(hence validated as \texttt{NA} in Section~\ref{sec-res-validation}).

\begin{longtable}[]{@{}llr@{}}

\caption{\label{tbl-most-common-decisions}Count of variable-type
decisions in the Gemini-extracted decisions. The most commonly reported
decision are the parameter choices and temporal lags for for time, PM,
temperature, and humidity.}

\tabularnewline

\toprule\noalign{}
Variable & Type & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
time & parameter & 44 \\
PM & temporal & 39 \\
temperature & parameter & 35 \\
humidity & parameter & 25 \\
temperature & temporal & 23 \\
humidity & temporal & 19 \\
PM & parameter & 9 \\
time & temporal & 3 \\

\end{longtable}

Table~\ref{tbl-most-common-decisions} lists the eight most frequently
reported decision: parameter and temporal choice for time, PM,
temperature, and humidity. While a wider list of variables have been
used in the analysis, these four variables are most commonly included in
baseline models. Parameter choices for time, temperature, and humidity
are typically made on the use of smoothing parameter for the smoothing
method (natural spline and smoothing spline), whereas temporal choices
are commonly reported for PM, temperature, and humidity for the number
of lag to consider in the model.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6778}}@{}}

\caption{\label{tbl-humidity-temperature-decisions}Options captured for
parameter choices for time, humidity, and temperature variables in the
Gemini-extracted decisions. The choices for natural spline knots are
generally less varied than the degree of freedom choices for smoothing
spline. Choices for temperature and humidity tend to be close, given
they are both weather related variables, while the choices for time are
more varied inherently.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
natural spline & humidity & 3, 4 \\
natural spline & temperature & 3, 4, 6 \\
natural spline & time & 1, 1.5, 3, 4, 6, 7, 8, 12, 15, 30 \\
smoothing spline & humidity & 2, 3, 4, 6, 8, 50\% of the data \\
smoothing spline & temperature & 2, 3, 4, 6, 8, 50\% of the data \\
smoothing spline & time & 1, 3, 4, 5, 6, 7, 7.7, 8, 9, 10, 12, 30, 100,
5\% of the data \\

\end{longtable}

Table~\ref{tbl-humidity-temperature-decisions} presents the
parameter-related decisions extracted for spline methods (natural and
smoothing spline) applied to variable time, humidity and temperature.
These decisions concern the number of knots or degree of freedom, with
all values standardized to a \emph{per year} scale for consistency. The
selection of knot for natural spline has less variation than the degree
of freedom choices for smoothing spline. Choices for temperature and
humidity are generally similar, given they are both weather related
variables, whereas choices for time are more varied. This tabulation
provides a reference set for common parameter choices for future studies
and help to identify anomalies and special treatment in practice. For
example, the choice of 7.7 degree of freedom reported in
\citet{castillejos2000} may prompt analysts to seek further
justification. By cross comparing with other reporting, some decisions
appear ambiguous. For example, in \citet{moolgavkar2000} and
\citet{moolgavkar2003}, the reported value of 30 and 100 degrees of
freedom for time may be understandable for experienced domain
researcher, it could be unclear for junior analysts as to whether they
apply to the full 9 year period or on a per-year basis. We also observe
a different report style from \citet{schwartz2000}, where smoothing
spline parameters are expressed as a proportion of the data (``5\% of
the data'' and ``5\% of the data'') rather than fixed numerical value.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2278}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1519}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6203}}@{}}

\caption{\label{tbl-temporal-decisions}Options captured for temporal lag
choices for PM, temperature, and humidity variables in the
Gemini-extracted decisions. Both single-day lags and multi-day average
lags are commonly used, generally considering up to five days prior (lag
5).}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Lag type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
multi-day average & PM & lag 0-1, 0-2, 0-3, 0-4, 0-5, 0-6 \\
multi-day average & humidity & lag 0-1, 0-2, 0-3, 0-5, 1-5, 2-4 \\
multi-day average & temperature & lag 0-1, 0-2, 0-3, 0-5, 2-4 \\
single-day lag & PM & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
13 \\
single-day lag & humidity & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13 \\
single-day lag & temperature & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13 \\

\end{longtable}

Similarly, Table~\ref{tbl-temporal-decisions} summarizes the temporal
lag choices for PM, temperature, and humidity. For single-day lags, the
lags are considered up to 13 days (approximately two weeks). For
multi-day averages, 3-day and 5-day averages are most common, although
other choices such as 2-4 day average are also observed as in
\citet{lópez-villarrubia2010}:

\begin{quote}
In particular, lags 0 to 1 and lags 2 to 4 averages of temperature,
relative humidity, and barometric pressure were considered as
meteorological variables.
\end{quote}

\subsection{Paper similarity and
clustering}\label{paper-similarity-and-clustering}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-mds-1.pdf}}

}

\caption{\label{fig-mds}The multi-dimensional scaling (MDS) based on
paper similarity distance for \texttt{length(good\_pp)} air pollution
mortality modelling papers, colored by the smoothing method used. The
MDS reveals the three distinct groups of papers, corresponds to LOESS,
natural spline, and smoothing spline. These groups corresponds to the
different modelling strategies debated in the European and U.S. studies,
as documented in the APHENA project \citep{APHENA}.}

\end{figure}%

Given the number of decisions reported in
Table~\ref{tbl-most-common-decisions}, we focus on the six most common
variable-type decisions for calculating paper similarity: parameter
choices for time, temperature, and humidity, and temporal lag choices
for PM, temperature, and humidity. We also restrict our analysis to
papers that report at least three of these six decisions, resulting in
48 papers for the similarity analysis. This ensures that the paper
similarity metric is based on a sufficient number of comparable
decisions. We use the default text embedding model (BERT) in the
\texttt{text} package and cosine similarity to compute the similarity
score. Sensitivity analysis on different text embedding model is checked
in Section~\ref{sec-text-model}. Paper similarity is then calculated as
the average of decision similarity for each paper pair. The resulting
distance matrix is then used for multi-dimensional scaling (MDS) in
Figure~\ref{fig-mds}. The two MDS dimension reveals three clusters
correspond to the three smoothing methods used in these analyses: LOESS,
natural spline, and smoothing spline. This grouping aligns with the
modelling strategies seen in large-scale analysis, such as the U.S.
NMMAPS study \citep{samet2000} and the European APHEA
\citep{katsouyanni1996} and APHEA2 \citep{katsouyanni2001} project.

To reconcile these differences, the APHENA project \citep{APHENA} was
launched with the aim to ``assess the consistency across Europe and
North America when estimated using a common analytic protocol and to
explore possible explanations for any remaining variation''. While
multi-dimensional scaling in Figure~\ref{fig-mds} shows the match of
three clusters with three smoothing methods, this is not inconsistent
with the APHENA project \citep{APHENA} that the amount of smoothing to
have a more important role than the method of smoothing for estimating
the effect of PM on public health variables. The similarity metric we
proposed focuses on the variation of choices across analyses, without
directly assessing how those choices influence results. By pooling
decision choices from multiple studies with LLMs, it becomes much easier
to reveal common practices and difference in research practices,
highlighting decisions that require further sensitivity analyses to
assess their impact. The different smoothing methods revealed in
Figure~\ref{fig-mds} are consistent with the analysis by
\citet{peng2006} and \citet{touloumi2006} that compares different
smoothing methods and rationale for selecting smoothing parameters.

\subsection{Sensitivity analysis}\label{sensitivity-analysis}

A series of sensitivity analysis has been conducted to explore the
reproducibility for using LLMs for text extraction tasks
(Section~\ref{sec-llm-reproducibility}), discrepancies in decision
extraction between different LLM models: Gemini
(\texttt{gemini-2.0-flash}) and Claude
(\texttt{claude-3-7-sonnet-latest}) (Section~\ref{sec-llm-models}), and
the sensitivity of text model for computing the semantic decision
similarity (Section~\ref{sec-text-model}).

\subsubsection{LLM reproducibility}\label{sec-llm-reproducibility}

We assess the reproducibility of Gemini's text extraction
(\texttt{gemini-2.0-flash}) by repeating the task five times for each of
the 62 papers and perform pairwise comparison between runs. This
generates \(5 \times 4 /2 \times 62 = 620\) possible comparisons for
both ``reason'' and ``decisions'' fields. Comparisons where the runs
produced a different number of decisions were excluded, as this would
require manual alignment. After filtering, 449 out of 620 (72\%)
remained. Table~\ref{tbl-gemini-1} prints the decisions in
\citet{andersen2008} across two runs and all the four decisions are
identical with no difference.

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-gemini-1}Example comparing Gemini's text extraction
for \citet{andersen2008} across two runs. The extracted decisions are
identical in both runs.}

\tabularnewline

\toprule\noalign{}
Variable & Run1 & Run2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NCtot & 6day average (lag 05) & 6day average (lag 05) \\
calendar time & 3 4 or 5 dfyear & 3 4 or 5 dfyear \\
dew-point temperature & 4 or 5 df & 4 or 5 df \\
temperature & 4 or 5 df & 4 or 5 df \\

\end{longtable}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-gemini-2}Number of differences in the reason and
decision fields across Gemini runs for papers with consistent number of
decisions across runs.}

\tabularnewline

\toprule\noalign{}
Num. of difference & Count & Proportion (\%) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 358 & 79.73 \\
1 & 12 & 2.67 \\
2 & 8 & 1.78 \\
3 & 0 & 0.00 \\
4 & 24 & 5.35 \\
5 & 12 & 2.67 \\
6 & 3 & 0.67 \\
7 & 0 & 0.00 \\
8 & 10 & 2.23 \\
9 & 6 & 1.34 \\
10 & 10 & 2.23 \\
11 & 6 & 1.34 \\
Total & 449 & 100.00 \\

\end{longtable}

Table~\ref{tbl-gemini-2} summarizes the number of differences observed
in each pairwise comparison. Among all comparisons, 80\% produce the
identical text in reason and decision. The discrepancies come from the
following two reasons: 1) Gemini extracted different length for the same
decision, e.g.~in \citet{kan2007}, some runs may extract ``singleday lag
models underestimate the cumulative effect of pollutants on mortality
2day moving average \textbf{of current and previous day concentrations}
(lag=01)'', while others extract ``singleday lag models underestimate
the cumulative effect of pollutants on mortality 2day moving average
(lag=01)''. Similarity, for decisions, some runs yield ``10 df for total
mortality'', while other runs yield ``10 df''. 2) Gemini fails to
extract reasons in some runs but not others, e.g.~in
\citet{burnett1998}, the first run generates NAs in the reasons, but the
remaining four runs are identical. In \citet{ueda2009} and
\citet{castillejos2000} , runs 1 and 5 fail to extract the reasons and
produce the same incomplete version, whereas runs 2, 3, and 4 produce
accurate versions with reasons populated.

\subsubsection{LLM models}\label{sec-llm-models}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-claude-gemini-1.pdf}}

}

\caption{\label{fig-claude-gemini}Comparison of decisions extracted by
Claude and Gemini. Each point represents a paper, with the x- and y-axes
showing the number of decisions extracted by Claude and Gemini,
respectively. The dashed 1:1 line marks where both models extract the
same number of decisions. More points fall below this line, suggesting
Claude extracts more decisions -- often including noise from data
pre-processing or secondary data analysis steps -- which requires
additional manual validation.}

\end{figure}%

Reading text from PDF document requires Optical Character Recognition
(OCR) to convert images into machine-readable text, which currently is
only supported by Antropic Claude (\texttt{claude-3-7-sonnet-latest})
and Google Gemini (\texttt{gemini-2.0-flash}). We compare the number of
decisions extracted by Claude and Gemini across all 62 papers in
Figure~\ref{fig-claude-gemini}. Each point represents a paper, with the
x- and y-axes showing the number of decisions extracted by Claude and
Gemini, respectively. The dashed 1:1 line marks where both models
extract the same number of decisions. While both models extract
decisions irrelevant to our analysis, such as sensitivity analyses and
secondary analyses, Claude's extractions tend to include more of these
irrelevant decisions, examples of these include 1) the definition of
``cold day'' and ``hot day'' indicators in \citet{dockery1992}
(``defined at the 5th/ 95th percentile''), 2) decisions relate to other
pollutants: \(\text{NO}_2\), \(\text{O}_3\), and \(\text{SO}_2\) using a
``24 hr average on variable'' in \citet{huang2009}, and 3) the
definition of black smoke and in \citet{katsouyanni2001} for secondary
analysis (``restrict to days with BS concentrations below 150
\(\mu g/m^2\)''). While Gemini also capture these irrelevant decisions,
such as ``0-4 lag days'' for air pollution exposure variables (CO, EC,
\(\text{K}_S\), \(\text{NO}_2\), \(\text{O}_3\), OC, Pb, S,
\(\text{SO}_2\), TC, Zn) in \citet{mar2000}. However, these cases are
less frequent than Claude's extraction and has been validated and
standardized in Section~\ref{sec-res-validation}.

For both Claude and Gemini, we find they fail to link the general term
``weather variables'' to the specific weather variables (e.g.
\citet{dockery1992} and \citet{burnett2004} for Gemini and
\citet{dockery1992} and \citet{katsouyanni2001} for Claude). Although
our prompt specified that some decisions may require linking information
across sentences and paragraphs to identify the correct variable, this
instruction doesn't appear to be applied consistently.

\subsubsection{Text model}\label{sec-text-model}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-model-1.pdf}}

}

\caption{\label{fig-text-model}Distribution of decision similarity
(left) and multi-dimensional scaling (MDS) of the paper similarity
scores (right) computed for five different text models (BERT, BioBERT,
RoBERTa, SciBERT, and XLNet). The default language model, BERT, produces
the widest variation across the five models, while the similarity scores
form XLNet are all close to 1. The model BioBERT, RoBERTa, and SciBERT
yield decision similar scores mostly between 0.7 to 1. All the text
models shows a similar clustering structure based on the three main
smoothing methods (LOESS, natural spline and smoothing spline).}

\end{figure}%

We have conducted sensitivity analysis on the text model for obtaining
the decision similarity score from the Gemini outputs. The tested
language models tested include 1) BERT by Google \citep{devlin2019}, 2)
RoBERTa by Facebook AI \citep{liu}, trained on a larger dataset (160GB
v.s. BERT's 15GB), 3) XLNnet by Google Brain \citep{yang}, and two
domain-trained BERT models: 4) sciBERT \citep{beltagy2019}, trained on
scientific literature, and 5) bioBERT \citep{lee2020}, trained on PubMed
and PMC data.

Figure~\ref{fig-text-model} shows the distribution of the decision
similarity and the corresponding multi-dimensional scaling
visualization, where distance are calcualted from the paper similarity
for each text model. At decision level, the BERT model produces the
widest variation across all five models, while the similarity scores
from XLNet are all close to 1. While the raw scores are not directly
comparable across models due to the difference in the underlying
transformer architecture, the multi-dimensional scaling (MDS) based on
paper similarity scores shows a similar clustering pattern corresponding
to the three main smoothing methods (LOESS, natural spline, and
smoothing spline).

\section{Discussion}\label{sec-discussion}

\subsection{Quality of LLM extraction and
validation}\label{quality-of-llm-extraction-and-validation}

Numerous studies have demonstrate the capability of LLMs in various text
extraction tasks, but validation is still needed to ensure the quality
of the extracted information. Most application tends to focus on
comparing with human annotated samples and compute F1 scores from
precision and recall.

While the extraction of decisions from literature could be largely
automated with LLMs, manual validations remains essential to ensure the
quality of the extracted decisions for further analysis. The quality
from the LLM ouput directly affects the amount of manual effort needed
for validation and standardization. Using a default temperature of 1 and
instructing the model to extract original text rather than paraphrase,
we find hallucination is not a major issue with Claude and Gemini for
this application. While prompt engineering is used in this work to
optimize the prompt for decision extraction, an alternative is to
fine-tune a local model to improve LLM performance. Such approach could
be beneficial for a systematic literature review, although it would
require a labelled decision dataset for training and significantly more
training efforts.

\subsection{Extracting other types of
decisions}\label{extracting-other-types-of-decisions}

As a demonstration, we focus on the modelling decision for the baseline
model in the air pollution epidemiology literature. Analyses in this
fields often fit multiple models for different health outcomes. Other
models, such as distributed lag models and multi-pollutant models are
also commonly used to estimate relative risks and the interaction among
pollutants. These factors increase the complexity of the decision
extraction for LLM, as for additional models,authors often describe only
the differences from the baseline model specification, assuming other
decisions remain unchanged. The LLMs will need to be able to link the
decisions across different models and identify the full set of decision
for each model for cross-comparison among papers.

Apart from modelling choices, decisions in data pre-processing are also
interesting to compare. For example, in \citet{braga2001}, air pollution
measures are aggregated from multiple PM10 monitors within the same
location into a single value. Different decisions on how values are
extracted, imputed, and aggregated are also shown to affect the results.
However, these decisions are often not well documented in the literature
than the modelling decisions, making it difficult for LLMs to extract
them. Proper documentation and reporting of these decisions in future
research are needed before our workflow could be applied to
pre-processing decisions.

With the advocacy for reproducibility in science, it is expected that
more papers will share their code and data. Code availability can serve
as a supplementary source for understanding the choices made in the
analysis and cross-check against the description in the manuscript.
However, decision choices could be extracted from the scripts, but the
rationale behind these choices may not be easily discernible given the
lack of comments in the current practice.

\subsection{Generalizability of the
workflow}\label{generalizability-of-the-workflow}

\section{Conclusion}\label{sec-conclusion}

In this paper, we aim to study how analysts make decisions in their data
analysis practice. While classic interviews are often conducted in small
scale with toy examples, we developed a pipeline for automatically
extracting decisions using LLMs (Claude and Gemini) from scientific
literature. We also introduced a method for calculating paper similarity
through comparing the similarities among decisions and the similarity
metric can be used as a distance to cluster papers by their decision
choices and visualization with dimension reduction algorithms, such as
multidimensional scaling. We applied this pipeline to a set of air
pollution modelling literature that associates daily particulate matter
and daily mortality and hospital admission. From the extracted modelling
decisions, we identify the most common decision choices in this type of
analysis and the paper similarity score calculation revealed the three
clusters of paper corresponding to different modelling strategies. These
findings are all consistent with the general understanding of the field,
as documented in the APHENA project \citep{APHENA} and other
methodological comparison studies \citep{peng2006, touloumi2006}.

While sensitivity analyses are commonly used to assess the robustness of
findings to different analytical choices, the set of choices tested is
often limited and selected subjectively by the authors. Our approach
offers a new perspective by pooling decisions made in analyses across
studies in the fields. This allows for a holistic account on the
alternatives in the field and identification of both consensus and
divergence within the field, providing insights for future research and
methodological development.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%% begin pandoc before-bib
%% end pandoc before-bib
%% begin pandoc biblio
%% end pandoc biblio
%% begin pandoc include-after
%% end pandoc include-after
%% begin pandoc after-body
%% end pandoc after-body

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
