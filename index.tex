% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{interact}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{setspace}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
\usepackage{orcidlink}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={An LLM-based Pipeline for Understanding Decision Choices in Data Analysis from Published Literature},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  pdfkeywords={decision choices, data analysis, Large Language Models},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{An LLM-based Pipeline for Understanding Decision Choices in Data
Analysis from Published Literature}
\author{H. Sherry Zhang$\textsuperscript{1}$, Roger D.
Peng$\textsuperscript{1}$}

\thanks{CONTACT: H. Sherry
Zhang. Email: \href{mailto:hsherryzhang@utexas.edu}{\nolinkurl{hsherryzhang@utexas.edu}}. }
\begin{document}
\captionsetup{labelsep=space}
\maketitle
\textsuperscript{1}  University of Texas at Austin, Austin, USA
\begin{abstract}
Decision choices, such as those made when building regression models,
and their rationale are essential for interpreting results and
understanding uncertainty in an analysis. However, these decisions are
rarely studied because tracing every alternatives considered by authors
is often impractical, and reworking a completed analysis is generally of
limited interest. Consequently, researchers must manually review large
bodies of published analyses to identify common choices and understand
how choices are made. In this work, we propose a workflow to
automatically extract analytic decisions and their reasons from
published literature using Large Language Models. Our method also
introduces a paper similarity measure based on decision similarity and
visualization methods using clustering algorithms. As an example, this
workflow is applied to analyses studying the effect of particulate
matter on mortality. This approach enables scalable and automated
studies of decision choices in applied data analysis, providing an
alternative to existing qualitative and interview-based studies.
\end{abstract}
\begin{keywords}
\def\sep{;\ }
decision choices\sep data analysis\sep 
Large Language Models
\end{keywords}


\setstretch{2}
\section{Introduction}\label{introduction}

Data analysis is a complex and iterative process, and decisions are made
at every stage of data analysis, from initial data collection,
pre-processing, to modeling. One might expect well-trained researchers
to make similar choices when faced with the same analytical task, yet
evidence suggests otherwise. ``Many-analyst'' experiments show that
independent analysts often arrive at markedly different conclusions,
even when analyzing the same dataset to answer the same research
question (Silberzahn et al. 2018; Botvinik-Nezer et al. 2020; Gould et
al. 2025). This variation in analytical decision-making, described by
Gelman and Loken (2014) as the ``garden of forking paths,'' can
undermine the quality and credibility of reported results and raise
uncertainty in the findings.

A common approach to investigate uncertainty in data analysis decisions
is sensitivity analysis, where researchers systematically vary key
decisions in their analysis to assess the robustness of their findings.
Multiverse analysis extends this idea by evaluating \emph{all} plausible
combinations of decision choices to examine how results vary across the
full decision space (Sarma et al. 2021; Blair et al. 2019). However,
what one analyst considers reasonable may not reflect the full range of
options used in practice. Even when a reasonable set of alternatives is
tested, the stability shown by sensitivity analysis may be less relevant
to other researchers with similar problems, who are often more
interested in understanding the rationale behind decision choices.
Ideally, decision-making in applied research can be studied by following
experienced analysts throughout the entire analysis process to capture
their reasoning. In reality, this is rarely feasible and not scalable.
While individual studies may not capture the full range of decision
choices used in practice, crowdsourcing decisions from a collection of
studies on a shared theme creates a ``many-analyst'' setting that
reveals how analysts make choices and justify them in practice. This
process now has the possibility to be automated at scale, given recent
advances in information extraction with Large Language Models (LLMs)
(Harrod et al. 2024; Katz et al. 2024; Farzi et al. 2024; Hu et al.
2024; Sciannameo et al. 2024; Gu et al. 2025; Schilling-Wilhelmi et al.
2025; Gupta et al. 2024; Li et al. 2024; Baddour et al. 2024; Polak and
Morgan 2024).

In this work, we propose a new approach to studying data analysis
decisions by automatically extracting decisions from scientific
literature using LLMs. We develop a tabular schema to record decisions,
automate the extraction process with LLMs, and introduce a new paper
similarity measure based on decision similarity. This similarity measure
can serve as a distance metric in dimension reduction methods to
visualize papers according to their decisions. We apply this workflow to
a set of 56 air pollution modeling studies that estimate the effect of
particulate matter (PM2.5 or PM10) on mortality and hospital admissions,
typically analyzed using Poisson generalized linear models (GLMs) or
generalized additive models (GAMs). Analysis of the extracted decisions
reveals common choices in this class of studies, including the use of
smoothing methods on PM and weather variables and the temporal lags for
time and weather variables. Multi-dimensional scaling on the paper
similarity distance finds three distinct clusters corresponding to the
smoothing methods used: LOESS, natural spline, and smoothing spline.
These findings align with the APHENA project (Katsouyanni et al. 2009),
which synthesizes research from multiple studies in Europe and North
America. In this workflow, we also provide detailed documentation on the
validation and standardization of LLM outputs. We outline the validation
and standardization process, including the use of a developed Shiny
application in R for reviewing decisions and the types of edits made
through validation, the use of a secondary LLM to standardize reported
choices of temporal lag decisions, and sensitivity analysis on
reproducibility across runs and model providers.

In summary, the contribution of this work includes:

\begin{itemize}
\item
  A scalable and automated approach to study data analysis decisions
  through extracting of decisions from published scientific literature
  using LLMs,
\item
  A new method to construct paper similarities based on decision choices
  and the semantic similarity of their rationales,
\item
  Practices for validating and standardizing LLM outputs, including a
  shiny GUI tool for editing outputs, the use of secondary LLM for
  standardizing unstructured responses, and sensitivity analysis on
  reproducibility across runs and model providers,
\item
  A data schema for recording decisions in data analysis in a tidy
  format, and
\item
  A dataset of decisions, along with metadata, compiled from 56 studies
  in air pollution mortality modeling literature.
\end{itemize}

\section{Related work}\label{sec-background}

\subsection{Analytic decision making in data
analysis}\label{analytic-decision-making-in-data-analysis}

Data analysis is a complex and iterative process (Jun, Seo, et al. 2022;
Jun, Birchfield, et al. 2022; Jun et al. 2019) that involves data
collection, data cleaning, visualization, modeling, and communication.
At each stage, analysts make decisions informed by domain practices,
statistical knowledge, and the data. These decisions, such as which
variables to include in a model, how to handle missing data, and how
hyper-parameters are chosen, act as branching points in the analysis
workflow. The full set of possible paths through these branching points
forms what Gelman and Loken (2014) describes as the ``garden of forking
paths''. While one might expect well-trained researchers to make similar
choices when facing similar decisions, empirical evidence suggests
otherwise. ``Many analyst experiments'' show that independent research
groups analyzing the same dataset to address the same research questions
can arrive at widely different conclusions. For example, Silberzahn et
al. (2018) asks 29 teams of analysts to conduct an analysis to address
the same research questions \emph{whether soccer players with dark skin
tone are more likely than those with light skin tone to receive red
cards from referees}. Researchers reported an estimated effect size from
0.89 to 2.93 in odds ratio, with 21 unique combinations of covariates
used among all 29 analyses. 70\% of the teams found a statistically
significant positive effect, while others didn't. This great discrepancy
among researchers when performing data analysis tasks is also observed
in other domains, for example, in structural equation modeling (Sarstedt
et al. 2024), applied microeconomics (Huntington-Klein et al. 2021),
neuroimaging (Botvinik-Nezer et al. 2020), and ecology and evolutionary
biology (Gould et al. 2025).

Examples like the above illustrate how analytical decisions introduce
uncertainty into data analysis. These uncertainties have been widely
discussed in the literature, given their impact for policy
recommendation (Katsouyanni et al. 2009) and domain applications, e.g.,
fairness machine learning (Simson et al. 2025). Through experiments
(Wicherts et al. 2016; Simmons et al. 2011), research has shown that
analysts' decisions can lead to p-hacking and inflated effect size when
not properly used. Hence, guidelines and checklists have been developed
to recommend the best practices to guide statistical analysis. In
medicine and biostatistics, pre-registration is a common practice to
regulate analysts making decisions after seeing the data. Given the
nuanced nature of data analysis, more work has examined how analysts
make decisions in practice through interviews in both academia and
industry. These studies include qualitative analysis of the decisions
made (Kale et al. 2019; Y. Liu et al. 2020), interviews with data
analysts about exploratory data analysis practice in industry (Alspaugh
et al. 2019; Kandel et al. 2012), and about how they consider
alternatives in data analysis (J. Liu et al. 2020).

In addition to qualitative studies, software tools have been developed
to help researchers account for alternatives and uncertainties and make
informed decisions in data analysis. Examples include \texttt{Tea} (Jun
et al. 2019), which supports general statistical analysis;
\texttt{Tisane} (Jun, Seo, et al. 2022), which guides choices in
generalized linear mixed-effects models (GLMMs); and
\texttt{MetaExplore} (Kale et al. 2023), which accounts for epistemic
uncertainty (decision uncertainty) in meta-analysis. The
\texttt{DeclareDesign} package (Blair et al. 2019) proposes the MIDA
framework for researchers to declare, diagnose, and redesign their
analyses to account for uncertainties of reporting the statistic of
interest. Multiverse analysis proposes a different method to allow
researchers to evaluate all plausible combinations of decision choices
to examine how results vary in the full decision space. Work has been
done on the software tools to support multiverse analysis (Sarma et al.
2021; Götz et al. 2024) and visualization of multiverse results (Liu et
al. 2021), and debugging tools (Gu et al. 2023).

\subsection{Automatic information extraction with
LLMs}\label{automatic-information-extraction-with-llms}

In natural language processing, information extraction is a task focused
on extracting structured information from unstructured text. Earlier
approaches in information extraction tasks relied on rule-based systems
and regular expressions. More recent advances, including conditional
random fields (Lafferty et al., n.d.), word embeddings such as word2vec
(Mikolov et al. 2013), and transformer-based architectures like BERT
(Devlin et al. 2019), have led to the current use of LLM to extract
information with prompts. Using LLMs to extract unstructured text offers
the advantage of automating the process at scale. Applications have been
seen in epidemiology data (Harrod et al. 2024), scientific literature
(Katz et al. 2024), clinical data (Farzi et al. 2024; Hu et al. 2024;
Sciannameo et al. 2024; Gu et al. 2025), chemistry knowledge
(Schilling-Wilhelmi et al. 2025), and polymer science (Gupta et al.
2024), climate extreme impact (Li et al. 2024), phenotypes (Baddour et
al. 2024), and material properties (Polak and Morgan 2024). An easier
task in information extraction is called Named Entity Recognition (NER)
to identify short span information (1-4 tokens) like person names and
locations from unstructured text (Nadeau and Sekine 2007). An example of
this is extracting patients' information and vitals in clinical data.
Extracting decisions from published literature is a more general task
than NER, since justification of a decision typically spans more than
just a few words. Our task also requires linking information across
sentences, sometimes sections, to correctly identify the variables a
decision refers to.

\subsection{Visualization on scientific
literature}\label{visualization-on-scientific-literature}

With the growing volume of scientific publications and the difficulty of
navigating the literature, there is an increasing interest in developing
systems to visualize and recommend scientific papers. These systems link
papers based on their similarity and relevance, typically determined by
keywords (Isenberg et al. 2017), citation information (Chen 2006), e.g.,
citation list and co-citation, or combinations with other relevant paper
metadata (Bethard and Jurafsky 2010; Chou and Yang 2011; Dörk et al.
2012; Heimerl et al. 2016), e.g., author and title. Recent approaches
incorporate text-based information using topic modeling (Alexander et
al. 2014), argumentation-based information retrieval (Tbahriti et al.
2006), and text embedding (Narechania et al. 2022). While metadata and
high-level text-based information are useful for finding relevant
papers, researchers also need tools that help them \emph{make sense} of
the literature rather than simply \emph{locating} it. In applied data
analysis, one interest is to understand how studies differ or align in
their decision choices. Capturing the decision choices and reasons that
justify the choices from analyses enables the calculation of similarity
among papers and can be piped into dimension reduction methods and
visualization for a global view of analysis practice in the field or
recommend similar papers based on decision similarities.

\section{Methods}\label{sec-extract-decisions}

In this section, we present the workflow for extracting decisions from
published literature using LLMs. We first describe the data structure
for recording decisions, followed by the four main steps in the
workflow: 1) automatic extraction of decisions from literature with
LLMs, 2) validation and standardization of LLM outputs, 3) exploratory
data analysis on analytic decisions, and 4) visualization of paper
similarity using clustering or dimension reduction methods. The section
concludes with an illustration summarizing the workflow.

\subsection{Record decisions in data analysis}\label{sec-decisions}

To analyze decisions, we first need to translate free-text descriptions
of decisions in academic papers into a tabular format. We record
decision following the tidy data principle (Wickham 2014), which states
that each variable forms a column and each observation forms a row. For
our purpose, each row represents a single decision made in a paper, and
an analysis typically involves multiple decisions.

A decision generally consists of three components: the context, the
reason, and the decision itself. To characterize the context of a
decision, additional information is often required, such as what
variable the decision is acted on, what statistical method the decision
uses, and what parameter of the method is being decided. Some decisions
do not involve a specific method or parameter, for example, a model may
be estimated separately for each city rather than jointly across all
cities. To account for this, we introduce a type identifier to
distinguish whether a decision is parameter-, spatial-, or
temporal-based. Method and parameter specifications are not required for
spatial or temporal decisions at the model level. The resulting
structure for recording decisions is as follows:

\begin{itemize}
\tightlist
\item
  \texttt{type}: one of ``parameter'', ``spatial'', or ``temporal''.
\item
  \texttt{variable}: the variable to which the statistical method is
  applied.
\item
  \texttt{method}: the statistical method used, (e.g.~``LOESS'',
  ``smoothing spline'', ``natural spline'').
\item
  \texttt{parameter}: the parameter of the method being decided,
  (e.g.~``degrees of freedom'', ``number of knots'').
\item
  \texttt{reason}: the justification for the decision.
\item
  \texttt{decision}: the final choice made.
\item
  \texttt{reference}: any cited sources supporting the decision.
\end{itemize}

For the ease of tracking, we extract the original text describing each
decision verbatim, without paraphrasing or summary.

\subsection{Extract decisions from literature with
LLMs}\label{extract-decisions-from-literature-with-llms}

Manually extracting decisions from published papers is labor-intensive
and time-consuming. Large language models (LLMs) make it possible to
automate this process by providing a collection of PDF documents along
with a structured prompt. In the prompt, the LLM is assigned the role of
an applied statistician and instructed to extract decisions from each
PDF document according to the format described in
Section~\ref{sec-decisions}. The prompt also notes that the reason and
decision fields may be missing, with examples provided. We use the
\texttt{ellmer} package (Wickham et al. 2025) in R to interface with
Anthropic Claude and Google Gemini for this task and the full prompt is
provided in the Appendix.

\subsection{Validate and standardize LLM
outputs}\label{validate-and-standardize-llm-outputs}

The LLM outputs need to be validated and standardized before further
analysis. Validation focuses on ensuring the extracted decisions are
correct, while standardization ensures that semantically equivalent
terms are represented in a consistent form. For example, the expressions
``mean temperature'', ``average temperature'', and ``temperature'' all
refer to the same variable and are standardized to ``temperature''. To
help with the validation and standardization process, we developed a
Shiny application for interactively reviewing and editing the LLM
outputs. The Shiny application takes a CSV file as the input and allows
users to perform three types of edits: 1) \emph{overwrite}: modify the
content of a particular cell, 2) \emph{delete}: remove an irrelevant row
(decision), and 3) \emph{add}: manually enter a row (decision).

Figure~\ref{fig-shiny} illustrates the \emph{overwrite} action for
standardizing the variable ``NCtot'' (number concentration of particles
\textless100 nm in diameter) to ``pollution''. The user enters a
predicate function in the filter condition box on the left panel, and
the filtered data will appear on the right panel. The user can then
specify the variable to overwrite and the new value. The corresponding
cells on the right panel will be updated. This change needs to be
confirmed by pressing the ``Apply changes'' button to update to the full
dataset. The corresponding \texttt{tidyverse} (Wickham et al. 2019) code
will then be generated on the left panel to be included in an R script,
and the edited table can be downloaded for future analysis.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=0.8\textheight]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface to validate
and standardize Large Language Model (LLM)-generated output. (1) The
default interface after loading the input CSV file. (2) The table view
will update interactively to reflect the edit: for paper with handle
``andersen2008size'' and id in 4, 5, 6, modify the variable name
\emph{NCtot} to \emph{pollutant}. (3) After clicking the Confirm button,
the corresponding \texttt{tidyverse} code for the modification is
generated, and the table view returns to its original unfiltered view
with the edit applied. The edited data can be downloaded by clicking the
Download CSV button.}

\end{figure}%

\subsection{Conduct exploratory data analysis on analytic
decisions}\label{sec-paper-similarity}

Once the output has been extracted and validated, these decisions can be
treated as data for exploratory data analysis. Analysts may summarize
the missingness of decision choices and their associated reasons to
better understand the reporting practice in the field. They may also
examine the most frequent reported choices to identify common analytical
practices. Grouping similar decision choices can further highlight
patterns of uncommon practices that may require closer examination.

Analysts may also be interested in whether there is structure in the
decision choices across papers. For example, do certain papers tend to
make similar choices? To address this question, we propose a paper
similarity measure based on decision similarity. A decision is
considered comparable between two papers if they share the same variable
and decision type, for example, a parameter decision on temperature. To
quantify similarity between matched decisions, we consider three
aspects: 1) similarity of the decision choice, 2) similarity of the
stated reasons, and 3) for parameter decisions, similarity of the
statistical method used. Similarity in choice and method reflects the
same analytical decision, while similarity in reason reflects a shared
rationale, even when the choices differ due to differences in data.

To measure similarity in choices and reasons, we obtain text embeddings
and compute cosine similarity using the BERT model via the \texttt{text}
package (Kjell et al. 2023) in \texttt{R}. Method similarity is encoded
as a binary indicator (1 if the two papers used the same method, and 0
otherwise) since semantic similarity in text cannot reliably distinguish
statistical methods. For example, the textual difference between
``smoothing spline'' and ``natural spline'' does not fully capture their
methodological distinction. The overall paper similarity is computed as
the average similarity across all matched methods, decisions, and
reasons.

Because each individual component is between 0 and 1, the overall
similarity also lies between 0 and 1, with a value of 1 indicating
identical decisions across all matched decisions. This metric does not
account for the number of matched decisions between two papers, and a
high (low) similarity score may be driven by a small number of matched
decisions. Analysts may therefore choose to restrict attention to a set
of common decisions shared across papers, or to pairs of papers that
report a minimal number of matched decisions when computing paper
similarity.

This similarity score can be used as a distance metric in dimension
reduction methods, such as multidimensional scaling (MDS), to visualize
how papers relate to each other based on their decisions across the
literature.

\subsection{Summary}\label{summary}

Figure~\ref{fig-workflow} summarises the workflow proposed for
extracting and analyzing decisions from published literature using LLMs.
After identifying a set of relevant papers, a prompt has been designed
to guide the LLM in extracting decisions from the documents. The
extracted outputs are then validated and standardized before further
analysis. The resulting dataset can then be used for exploratory data
analysis and in addition, we propose a paper similarity metric to
quantify similarity in decision-making across papers. This measure can
be interpreted as a distance metric between papers and used for
clustering and dimension reduction to visualize patterns in
decision-making across the literature.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/workflow.png}

}

\caption{\label{fig-workflow}The workflow for extracting decisions from
published literature using Large Language Models (LLMs) and analyzing
the extracted decisions. The workflow consists of four main steps: (1)
Extract decisions from literature with LLMs, (2) Validate and
standardize LLM outputs, (3) Conduct exploratory data analysis on
analytic decisions, including calculating paper similarity score.}

\end{figure}%

\section{Application}\label{sec-result}

In the study of the health effects of outdoor air pollution, one area of
interest is the association between short-term, day-to-day changes in
particulate matter air pollution and daily mortality counts. This
question has been studied extensively by researchers across the globe,
and it serves to provide scientific evidence in the US to guide public
policy on setting the National Ambient Air Quality Standards (NAAQS) for
air pollutants. While individual modeling choices vary, these studies
often share a common structure: they adjust for meteorological
covariates, such as temperature and humidity, include lagged variables
to account for temporal correlations, and estimate the effect size by
city or region before pooling the results with random effect. This
naturally forms a ``many-analyst'' experiment setting to analyze
decisions in air pollution mortality modelling.

We apply the workflow to extract the decisions in 56 studies reviewed in
Atkinson et al. (2014) that estimate the effect of particulate matter
(\(\text{PM}_{10}\) and \(\text{PM}_{2.5}\)) on mortality and hospital
admission using Gemini (\texttt{gemini-2.0-flash}). We focus on the
baseline model reported in each paper, excluding secondary models (e.g.,
lag-distributed models), multi-pollutant models, and alternatives tested
in the sensitivity analysis, which are discussed in
Section~\ref{sec-discussion}. This yields 242 decisions extracted,
averaging 4 decisions per paper.

\subsection{Validation and standardization of LLM
outputs}\label{sec-res-validation}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.9355}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.0645}}@{}}

\caption{\label{tbl-review}Summary of validation and standardization
edits made during the review process.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Count
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Remove decisions out of scope: other pollutants and sensitivity analysis
& 50 \\
Edit made to recode smoothing parameter unit to per year & 45 \\
Duplicates & 9 \\
Fix incorrect capture & 9 \\
Edit made due to decisions are too general, e.g.~minimum of 1 df per
year was required & 6 \\
Remove decisions related to definition of variables, e.g.~season & 5 \\
Total & 124 \\

\end{longtable}

Table~\ref{tbl-review} summarizes the number of edits made during the
review process using the Shiny application. Validation includes fixing
incorrect captures, removing non-decision (e.g., definition of
variables), removing duplication, excluding irrelevant decisions (e.g.,
sensitivity analyses), and excluding decisions whose stated reasons
reflect general guidelines rather than actual choices (e.g., ``minimum
of 1 degree of freedom per year is required'').

Standardization is performed on the variable names of decisions and
choices. The variable name in the decisions are standardized into four
main categories:

\begin{itemize}
\tightlist
\item
  \textbf{temperature}: ``mean temperature'', ``average temperature'',
  ``temperature'', ``air temperature'', ``ambient temperature''
\item
  \textbf{humidity}: ``dewpoint temperature'' and its hyphenated
  variants, relative humidity'', ``humidity''
\item
  \textbf{PM}: ``pollutant'', ``pollution'', ``particulate matter'',
  ``particulate'', ``PM10'', ``PM2.5''
\item
  \textbf{time}: ``date'', ``time'', ``trends'', ``trend''
\end{itemize}

Notice that ``dewpoint temperature'' is standardized under humidity
because it serves as a proxy for temperature in achieving a 100\%
relative humidity.

Decisions themselves also require standardization. For example, the
smoothing parameter (number of knots and degree of freedom) may be
expressed as \emph{per year} or \emph{in total}, and temporal lag
decision may be expressed in different formats (e.g., ``6-day average'',
``mean of lags 0+1'', ``lagged exposure up to 6 days''). Decision
choices on the smoothing parameter are manually recoded to a \emph{per
year} basis, as in Table~\ref{tbl-review}. Temporal decisions show a
wider variety, which makes manual standardization impractical. However,
we observe that they generally fall into two categories:

\begin{itemize}
\tightlist
\item
  \textbf{multi-day average lags}: ``6-day average'', ``3-d moving
  average'', ``mean of lags 0+1'', ``cumulative lags, mean 0+1+2'', and
\item
  \textbf{single-day lags}: ``lagged exposure up to 6 days'', ``lag days
  from 0 to 5''
\end{itemize}

Hence we apply a secondary LLM (claude-3-7-sonnet-latest) to convert
temporal decisions into a consistent format:
\texttt{multi-day:\ lag\ {[}start{]}-{[}end{]}} and
\texttt{single-day:\ lag\ {[}start{]},\ …\ ,lag\ {[}end{]}}. This
converts ``6-day average'' into ``multi-day: lag 0-5'' and ``lagged
exposure up to 6 days'' into ``single-day: lag 0, lag 1, lag 2, lag 3,
lag 4, lag 5''.

\subsection{Exploratory analysis of decision
choices}\label{exploratory-analysis-of-decision-choices}

\begin{table}

\caption{\label{tbl-missing-decisions}Missingness of decision and reason
fields in the Gemini-extracted decisions. Most decisions report the
choice (35.5 + 57.1 = 92\%), but 57.1\% lacks a stated reason.}

\centering{

\begin{tabular}{lll}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision} \\
\cmidrule(l{3pt}r{3pt}){2-3}
Reason & Non-missing & Missing\\
\midrule
Non-missing & 90 (37.2\%) & 14 (5.8\%)\\
Missing & 134 (55.4\%) & 4 (1.7\%)\\
\bottomrule
\end{tabular}

}

\end{table}%

In practice, data analysis decisions in academic papers are generally
not presented individually in the format described in
Section~\ref{sec-decisions}. Authors may combine multiple related
decisions into a single sentence for brevity, or omit certain
components, not providing a reason for a decision or not stating the
exact choice made. Table~\ref{tbl-missing-decisions} summarizes the
missingness of the decisions and the reason. While 37\% of decisions are
complete in both decision choices and reasons, 55\% of decisions lack a
stated rationale for the choice. This reflects a common reporting
practice in the field, where authors often report the decision choice
used without an explicit reason.

\begin{longtable}[]{@{}llr@{}}

\caption{\label{tbl-most-common-decisions}Count of variable-type
decisions in the Gemini-extracted decisions. The most commonly reported
decision are the parameter choices and temporal lags for for time, PM,
temperature, and humidity.}

\tabularnewline

\toprule\noalign{}
Variable & Type & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
time & parameter & 44 \\
PM & temporal & 39 \\
temperature & parameter & 35 \\
humidity & parameter & 25 \\
temperature & temporal & 23 \\
humidity & temporal & 19 \\
PM & parameter & 9 \\
time & temporal & 3 \\

\end{longtable}

Table~\ref{tbl-most-common-decisions} lists the eight most frequently
reported decisions: parameter and temporal choice for \texttt{time},
\texttt{PM}, \texttt{temperature}, and \texttt{humidity.} While a wider
list of variables has been used in the analysis, these four variables
are most commonly included in baseline models. This includes the
smoothing parameter used for time, temperature, and humidity in the
smoothing method (natural spline and smoothing spline) and temporal lag
choices for PM, temperature, and humidity.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6778}}@{}}

\caption{\label{tbl-humidity-temperature-decisions}Options captured for
parameter choices for time, humidity, and temperature variables in the
Gemini-extracted decisions. The choices for natural spline knots are
generally less varied than the degree of freedom choices for smoothing
spline. Choices for temperature and humidity tend to be close, given
they are both weather related variables, while the choices for time are
more varied inherently.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
natural spline & humidity & 3, 4 \\
natural spline & temperature & 3, 4, 6 \\
natural spline & time & 1, 1.5, 3, 4, 6, 7, 8, 12, 15, 30 \\
smoothing spline & humidity & 2, 3, 4, 6, 8, 50\% of the data \\
smoothing spline & temperature & 2, 3, 4, 6, 8, 50\% of the data \\
smoothing spline & time & 1, 3, 4, 5, 6, 7, 7.7, 8, 9, 10, 12, 30, 100,
5\% of the data \\

\end{longtable}

Table~\ref{tbl-humidity-temperature-decisions} presents the number of
knots or degree of freedom used in two spline methods (natural and
smoothing spline) applied to variable \texttt{time},
\texttt{humidity},and \texttt{temperature}, with all values standardized
to a \emph{per year} scale. The choices of knots for natural spline have
less variation than the degree of freedom choices for smoothing spline.
Choices for temperature and humidity are generally similar, given that
they are both weather-related variables, whereas choices for time are
more varied. This tabulation provides a reference set for common
parameter choices for future studies and helps to identify anomalies and
special treatment in practice. For example, the choice of 7.7 degree of
freedom reported in Castillejos et al. (2000) may prompt analysts to
seek further justification for its use. By cross-comparing with other
reporting, some decisions appear ambiguous. For example, in Moolgavkar
(2000) and Moolgavkar (2003), the reported value of 30 and 100 degrees
of freedom for time may be understandable for experienced domain
researchers, but it can be unclear for junior analysts as to whether
they refer to the parameter used for the full study period or on a
per-year basis, which is often clear in other papers. We also observe a
different report style from Schwartz (2000), where smoothing spline
parameters are expressed as a proportion of the data (``5\% of the
data'' and ``5\% of the data''), rather than a fixed numerical value.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2278}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1519}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6203}}@{}}

\caption{\label{tbl-temporal-decisions}Options captured for temporal lag
choices for PM, temperature, and humidity variables in the
Gemini-extracted decisions. Both single-day lags and multi-day average
lags are commonly used, generally considering up to five days prior (lag
5).}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Lag type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
multi-day average & PM & lag 0-1, 0-2, 0-3, 0-4, 0-5, 0-6 \\
multi-day average & humidity & lag 0-1, 0-2, 0-3, 0-5, 1-5, 2-4 \\
multi-day average & temperature & lag 0-1, 0-2, 0-3, 0-5, 2-4 \\
single-day lag & PM & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
13 \\
single-day lag & humidity & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13 \\
single-day lag & temperature & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13 \\

\end{longtable}

Similarly, Table~\ref{tbl-temporal-decisions} summarizes the temporal
lag choices for \texttt{PM}, \texttt{temperature}, and
\texttt{humidity.} For single-day lags, the lags are considered up to 13
days (approximately two weeks) while for multi-day averages, 3-day and
5-day averages are the most common, although other choices such as 2-4
day average are also observed (López-Villarrubia et al. 2010).

\subsection{Paper similarity calculation, clustering analysis, and
visualization}\label{paper-similarity-calculation-clustering-analysis-and-visualization}

Given the number of decisions reported in
Table~\ref{tbl-most-common-decisions}, we focus on the six most common
variable-type decisions for calculating paper similarity: parameter
choices for time, temperature, and humidity, and temporal lag choices
for PM, temperature, and humidity. This choice allows us to compare
papers based on a common set of decisions that are widely reported in
the literature. We also restrict our analysis to papers that report at
least three of these six decisions, resulting in 48 papers for the paper
similarity calculation. This ensures that the paper similarity metric is
based on a sufficient number of comparable decisions. We use the default
text embedding model (BERT) in the \texttt{text} package and cosine
similarity to compute the similarity score. Sensitivity analysis on
different text embedding models is checked in
Section~\ref{sec-text-model}. Paper similarity is then calculated as the
average of decision similarity for each paper pair. The resulting
similarity score is then used as the distance matrix in
multi-dimensional scaling (MDS).

Figure~\ref{fig-mds} displays the multidimensional scaling of the papers
based on their pairwise (dis)similarities. The two MDS dimensions reveal
three separated groups that align with the smoothing strategy adopted in
the original analyses: LOESS, natural spline, and smoothing spline. The
visual separation is supported by the silhouette diagnostic in
Figure~\ref{fig-sil}, where all the studies exhibit a positive
silhouette, suggesting no mis-classification and an average silhouette
width is 0.723. This clustering pattern is consistent with the
historical methodological divide documented in the APHENA project.
Studies from the European APHEA (Katsouyanni et al. 1996) and APHEA2
(Katsouyanni et al. 2001) projects more commonly employed LOESS and
smoothing splines, whereas the U.S.-based NMMAPS study (Samet et al.
2000) predominantly relies on natural splines.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-mds-1.pdf}}

}

\caption{\label{fig-mds}The multi-dimensional scaling (MDS) based on
paper similarity distance for 56 air pollution mortality modeling
papers, colored by the smoothing method used. The MDS reveals the three
distinct groups of papers, corresponds to LOESS, natural spline, and
smoothing spline. These groups corresponds to the different modeling
strategies debated in the European and U.S. studies, as documented in
the APHENA project (Katsouyanni et al. 2009).}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-sil-1.pdf}}

}

\caption{\label{fig-sil}Medoid silhouette width for the three clusters
identified from the multi-dimensional scaling (MDS) using paper
similarity score as the distance. Each bar represents a study's medoid
silhouette value, with the dashed line indiciating the overall average
silhouette width (0.723). This diagnostic plot suggests strong
within-cluster cohesion and clear separation between clusters
corresponding to the three smoothing methods used: LOESS, natural
spline, and smoothing spline.}

\end{figure}%

\subsection{Sensitivity analysis}\label{sensitivity-analysis}

A series of sensitivity analysis have been conducted to explore the
reproducibility across runs (Section~\ref{sec-llm-reproducibility}),
model providers (Section~\ref{sec-llm-models}), and the sensitivity of
text model for computing the semantic decision similarity
(Section~\ref{sec-text-model}).

\subsubsection{LLM reproducibility}\label{sec-llm-reproducibility}

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-gemini-1}Example comparing Gemini's text extraction
for Andersen et al. (2008) across two runs. The extracted decisions are
identical in both runs.}

\tabularnewline

\toprule\noalign{}
Variable & Run1 & Run2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NCtot & 6day average (lag 05) & 6day average (lag 05) \\
calendar time & 3 4 or 5 dfyear & 3 4 or 5 dfyear \\
dew-point temperature & 4 or 5 df & 4 or 5 df \\
temperature & 4 or 5 df & 4 or 5 df \\

\end{longtable}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-gemini-2}Number of differences in the reason and
decision fields across Gemini runs for papers with consistent number of
decisions across runs.}

\tabularnewline

\toprule\noalign{}
Num. of difference & Count & Proportion (\%) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 358 & 79.73 \\
1 & 12 & 2.67 \\
2 & 8 & 1.78 \\
3 & 0 & 0.00 \\
4 & 24 & 5.35 \\
5 & 12 & 2.67 \\
6 & 3 & 0.67 \\
7 & 0 & 0.00 \\
8 & 10 & 2.23 \\
9 & 6 & 1.34 \\
10 & 10 & 2.23 \\
11 & 6 & 1.34 \\
Total & 449 & 100.00 \\

\end{longtable}

We assess the reproducibility across runs of Gemini
(\texttt{gemini-2.0-flash}) by repeating the text extract task five
times and performing pairwise comparison between runs. This generates
\(5 \times 4 /2 \times 62 = 620\) possible comparisons for both
``reason'' and ``decisions'' fields. Comparisons are excluded when two
runs produced a different number of decisions, since this would require
manual alignment. This leaves 449 out of 620 (72\%) extractions to
compare. Table~\ref{tbl-gemini-1} prints a comparison of decisions in
Andersen et al. (2008) across two runs, and all four decisions are
identical with no difference. Table~\ref{tbl-gemini-2} summarizes the
number of differences observed in each pairwise comparison. Among all
comparisons, 80\% produces the identical text in reason and decision.
The discrepancies mainly come from the following two reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Gemini extracted the same decision in different lengths. For example,
  in Kan et al. (2007), some runs may extract ``singleday lag models
  underestimate the cumulative effect of pollutants on mortality 2day
  moving average \textbf{of current and previous day concentrations}
  (lag=01)'', while others extract ``singleday lag models underestimate
  the cumulative effect of pollutants on mortality 2day moving average
  (lag=01)''.
\item
  Gemini fails to extract reasons in some runs but not others. For
  example, in Burnett et al. (1998), the first run generates \texttt{NA}
  in the reason, but the remaining four runs are identical, with the
  reason populated. In Ueda et al. (2009) and Castillejos et al. (2000)
  , runs 1 and 5 fail to extract the reason and produce the same
  incomplete version, whereas runs 2, 3, and 4 produce accurate versions
  with reason populated.
\end{enumerate}

\subsubsection{LLM models}\label{sec-llm-models}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-claude-gemini-1.pdf}}

}

\caption{\label{fig-claude-gemini}Comparison of decisions extracted by
Claude and Gemini. Each point represents a paper, with the x- and y-axis
showing the number of decisions extracted by Claude and Gemini,
respectively. The dashed 1:1 line marks where both models extract the
same number of decisions. More points fall below this line, suggesting
Claude extracts more decisions -- often including noise from data
pre-processing or secondary data analysis steps -- which requires
additional manual validation.}

\end{figure}%

We compare the number of decisions extracted by Gemini
(\texttt{gemini-2.0-flash}) and Claude
(\texttt{claude-3-7-sonnet-latest}) across all 62 papers. In
Figure~\ref{fig-claude-gemini}, each point represents a paper, with the
x- and y-axis showing the number of decisions extracted by Claude and
Gemini, respectively. The dashed 1:1 line marks where both models
extract the same number of decisions. In general, the two models produce
a similar number of decisions. However, more points fall below this
line, suggesting Claude extracts more decisions, often including noise
from data pre-processing or secondary data analysis steps. Examples of
papers with large discrepancies include Mar et al. (2000) (Claude: 10
vs.~Gemini: 28), Ito et al. (2006) (Claude: 25 vs.~Gemini: 19), Ko et
al. (2007) (Claude: 8 vs.~Gemini: 16), among others. For both Claude and
Gemini, we find they sometimes fail to link the general term ``weather
variables'' to the specific weather variables (e.g., Dockery et al.
(1992) and Burnett et al. (2004) for Gemini and Dockery et al. (1992)
and Katsouyanni et al. (2001) for Claude). Although our prompt specified
that some decisions may require linking information across sentences and
paragraphs to identify the correct variable, this instruction doesn't
appear to be applied consistently.

\subsubsection{Text model}\label{sec-text-model}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-model-1.pdf}}

}

\caption{\label{fig-text-model}Distribution of decision similarity
(left) and multi-dimensional scaling (MDS) of the paper similarity
scores (right) computed for five different text models (BERT, BioBERT,
RoBERTa, SciBERT, and XLNet). The default language model, BERT, produces
the widest variation across the five models, while the similarity scores
form XLNet are all close to 1. The model BioBERT, RoBERTa, and SciBERT
yield decision similar scores mostly between 0.7 to 1. All the text
models shows a similar clustering structure based on the three main
smoothing methods (LOESS, natural spline and smoothing spline).}

\end{figure}%

We have conducted sensitivity analyses on the text model for calculating
the decision similarity score from the Gemini outputs. The tested
language models include 1) BERT (Devlin et al. 2019) by Google, 2)
RoBERTa (Liu et al., n.d.) by Facebook AI, trained on a larger dataset
(160GB v.s. BERT's 15GB), 3) XLNnet (Yang et al., n.d.) by Google Brain,
and two domain-trained BERT models: 4) sciBERT (Beltagy et al. 2019),
trained on scientific literature, and 5) bioBERT (Lee et al. 2020),
trained on PubMed and PMC data.

Figure~\ref{fig-text-model} shows the distribution of the decision
similarity and the corresponding multi-dimensional scaling
visualization, where distances are calculated from the paper similarity
for each text model. At the decision level, the BERT model produces the
widest variation across all five models, while the similarity scores
from XLNet are all close to 1. While the raw scores are not directly
comparable across models due to the difference in the underlying
transformer architecture, the visualizations from multi-dimensional
scaling (MDS) based on paper similarity scores all show a similar
clustering pattern corresponding to the three main smoothing methods
(LOESS, natural spline, and smoothing spline).

\section{Discussion}\label{sec-discussion}

\subsection{Large-language models for information
extraction}\label{large-language-models-for-information-extraction}

Numerous studies (Harrod et al. 2024; Katz et al. 2024; Farzi et al.
2024; Hu et al. 2024; Sciannameo et al. 2024; Gu et al. 2025;
Schilling-Wilhelmi et al. 2025; Gupta et al. 2024; Li et al. 2024;
Baddour et al. 2024; Polak and Morgan 2024) have demonstrated the
capability of LLMs for information extraction tasks. Our work applies
the LLMs to extract analytic decisions in scientific literature,
providing further evidence of their effectiveness. Our task requires
capturing more complex analytical decisions and their justifications,
which typically span more than just a few tokens, like in named entity
recognition. Our task also requires linking information across sentences
and sometimes sections to correctly identify the variables of a decision
(e.g., linking ``weather'' to ``temperature'' and ``humidity''). While
LLM has performed well on extracting decisions from the literature,
manual validations are still required to ensure the quality of the
extracted decisions for downstream analysis. Most existing applications
evaluate LLMs by comparing their outputs to human-annotated datasets,
reporting metrics such as precision, recall, and F1 score. Because this
approach depends on labeled data, and it is not yet clear how these
outputs should be validated for downstream analysis in practice. In our
work, we automate some of the manual validation with a secondary LLM
(Claude) to standardize the temporal lag choices in different
expressions into two categories.

With a default temperature of one and the prompt to instruct the model
to extract the original text rather than paraphrase, we find that
hallucination is not a major issue with Claude and Gemini in this
application. Since LLM outputs are inherently probabilistic, we also
conduct sensitivity analyses on reproducibility across runs and model
providers. The output is generally stable: repeated runs with the Gemini
produce consistent results, and different models extracted a similar
number of decisions.

While we optimize the prompt for decision extraction in this work, an
alternative approach is to fine-tune a local model to enhance LLM
performance. A catered local model could be useful for extraction
decisions for a comprehensive literature review on a larger scale, but
it would require greater model training efforts with labeled data.

\subsection{Extracting other types of
decisions}\label{extracting-other-types-of-decisions}

In this work, we focus on modeling decisions for the baseline model in
the air pollution epidemiology literature. Analyses in this field often
fit multiple models for different health outcomes and use secondary
models, such as distributed lag models and multi-pollutant models, to
estimate relative risks and multi-pollutant interactions. These increase
the complexity of decision extraction with LLMs because authors often
only describe the differences from the baseline specification,
implicitly assuming other decisions remain unchanged. Hence, LLMs will
need to link the decisions across different models and reconstruct the
complete set of decisions for each model.

Beyond modeling choices, decisions in data pre-processing are also
interesting to compare. For example, Braga et al. (2001) aggregated air
pollution measures from multiple PM10 monitors within the same location
into a single value. Pre-processing choices such as data source,
aggregation method, imputation also have an impact on the uncertainty of
the estimated effect size of particulate matter. However, these
decisions are often not properly and adequately described in the
manuscript, making it impossible to extract by LLMs. Proper
documentation and reporting standards in pre-processing decisions are
needed before our workflow could be applied to pre-processing decisions.

With growing advocacy for reproducibility, papers nowadays are expected
to share code and data, if applicable. Code availability provides a
useful supplementary source for identifying decision choices and
cross-checking them against descriptions in the manuscript. However,
while the script may reveal what choices were made, the rationales
behind these choices are often not documented under the current
practice.

\subsection{Generalizability of the
workflow}\label{generalizability-of-the-workflow}

In principle, our workflow is scalable and generalizable to a random set
of applied papers. However, insights about the data analysis practices
are more likely to be revealed when papers share certain similarities.
For example, literature on the same topic but different authors allows
for understanding of common practices within a field, literature using
the same methodology across different disciplines allows comparisons of
the same statistical method across fields; and literature that considers
the same variables can show how those variables are used in different
domains.

Our LLM prompt for extracting decisions will need to be customized for
each application of the workflow. The general prompt structure and the
data schema for recording decisions can be reused, while examples within
the prompt may be adapted to suit the specific application. The shiny
application for interactively validating and standardizing decisions can
be reused across applications. Calculating paper similarity requires
comparing decisions on the same variable and type across paper pairs.
For papers with limited similarities, the number of comparable decisions
may be limited. Diagnostic functions are available to display decisions
side by side or provide summary statistics on the number of comparable
decisions. Uncertainty visualization on the paper similarity score can
be used to highlight the confidence with respect to the number of
comparable decisions.

As a new method for collecting analytic decision data from literature,
our workflow can be connected to meta-analysis to assess how different
decisions influence results. More broadly, it can also be integrated
into literature search and recommender systems to suggest similar papers
based on the analytic decisions they employ.

\section{Conclusion}\label{sec-conclusion}

In this paper, we developed a scalable and generalizable pipeline for
automatically extracting analytical decisions using LLMs from scientific
literature to study how analysts make decisions in data analysis. We
also introduced a method for calculating paper similarity through
comparing the similarities among decision choices, and the similarity
metric can be used as a distance to cluster papers by their decision
choices and visualization with dimension reduction algorithms, such as
multidimensional scaling. We applied this pipeline to a set of air
pollution modeling literature that associates daily particulate matter
and daily mortality and hospital admissions. From the extracted modeling
decisions, we identify the most common decision choices in this type of
analysis, and the paper similarity score calculation revealed the three
clusters of paper corresponding to different smoothing methods.

Many work on studying decision-making in data analysis conduct
qualitative interviews with a small number of analysts to understand
their decision-making process. ``Many-analysts'' studies gather together
analysts in a controlled experiment to observe analysts conduct the
analysis. Our approach is also observational in nature, but we
``observe'' analysts in real-world problems with real data that have
policy implications, while being scalable and cost-effective for a
broader exploration of decision-making practices in different contexts
and disciplines. Compared to sensitivity analysis or multiverse
analysis, our approach offers a different perspective by pooling
together decisions made in analyses across the field to reveal the
options considered to highlight uncertainty in decisions that require
further sensitivity analyses to assess their impact (Peng et al. 2006;
Touloumi et al. 2006).

\section{Acknowledgement}\label{acknowledgement}

The article has been created using Quarto (Allaire et al. 2022) in R (R
Core Team 2025). The source code for reproducing the work reported in
this paper can be found at:
\url{https://github.com/huizezhang-sherry/paper-decisions}. The tools
developed to support this methodology are available as an R package,
\texttt{dossier} on GitHub at
\url{https://github.com/huizezhang-sherry/dossier}.

\section*{References}\label{references}
\addcontentsline{toc}{section}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{1}
\bibitem[\citeproctext]{ref-alexander2014}
Alexander, Eric, Joe Kohlmann, Robin Valenza, Michael Witmore, and
Michael Gleicher. 2014. {``2014 IEEE Conference on Visual Analytics
Science and Technology (VAST).''} October, 173--82.
\url{https://doi.org/10.1109/VAST.2014.7042493}.

\bibitem[\citeproctext]{ref-Allaire_Quarto_2022}
Allaire, J. J., C. Teague, C. Scheidegger, Y. Xie, and C. Dervieux.
2022. \emph{{Quarto}}. Version 1.2.
\url{https://doi.org/10.5281/zenodo.5960048}.

\bibitem[\citeproctext]{ref-alspaugh2019}
Alspaugh, Sara, Nava Zokaei, Andrea Liu, Cindy Jin, and Marti A. Hearst.
2019. {``Futzing and Moseying: Interviews with Professional Data
Analysts on Exploration Practices.''} \emph{IEEE Transactions on
Visualization and Computer Graphics} 25 (1): 22--31.
\url{https://doi.org/10.1109/TVCG.2018.2865040}.

\bibitem[\citeproctext]{ref-andersen2008}
Andersen, Z. J., P. Wahlin, O. Raaschou-Nielsen, M. Ketzel, T. Scheike,
and S. Loft. 2008. {``Size Distribution and Total Number Concentration
of Ultrafine and Accumulation Mode Particles and Hospital Admissions in
Children and the Elderly in Copenhagen, Denmark.''} \emph{Occupational
and Environmental Medicine} 65 (7): 458--66.
\url{https://doi.org/10.1136/oem.2007.033290}.

\bibitem[\citeproctext]{ref-atkinson_epidemiological_2014}
Atkinson, R. W., S. Kang, H. R. Anderson, I. C. Mills, and H. A. Walton.
2014. {``Epidemiological Time Series Studies of {PM2}.5 and Daily
Mortality and Hospital Admissions: A Systematic Review and
Meta-Analysis.''} \emph{Thorax} 69 (7): 660--65.
\url{https://doi.org/10.1136/thoraxjnl-2013-204492}.

\bibitem[\citeproctext]{ref-baddour2024}
Baddour, Moussa, Stéphane Paquelet, Paul Rollier, Marie De Tayrac,
Olivier Dameron, and Thomas Labbe. 2024. {``2024 IEEE 12th International
Conference on Intelligent Systems (IS).''} August, 1--8.
\url{https://doi.org/10.1109/IS61756.2024.10705235}.

\bibitem[\citeproctext]{ref-beltagy2019}
Beltagy, Iz, Kyle Lo, and Arman Cohan. 2019. {``Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing
(EMNLP-IJCNLP).''} (Hong Kong, China), 3613--18.
\url{https://doi.org/10.18653/v1/D19-1371}.

\bibitem[\citeproctext]{ref-bethard2010}
Bethard, Steven, and Dan Jurafsky. 2010. {``CIKM '10: International
Conference on Information and Knowledge Management.''} (Toronto ON
Canada), October 26, 609--18.
\url{https://doi.org/10.1145/1871437.1871517}.

\bibitem[\citeproctext]{ref-blair2019}
Blair, Graeme, Jasper Cooper, Alexander Coppock, and Macartan Humphreys.
2019. {``Declaring and Diagnosing Research Designs.''} \emph{American
Political Science Review} 113 (3): 838--59.
\url{https://doi.org/10.1017/S0003055419000194}.

\bibitem[\citeproctext]{ref-botvinik-nezer2020}
Botvinik-Nezer, Rotem, Felix Holzmeister, Colin F. Camerer, et al. 2020.
{``Variability in the Analysis of a Single Neuroimaging Dataset by Many
Teams.''} \emph{Nature} 582 (7810): 84--88.
\url{https://doi.org/10.1038/s41586-020-2314-9}.

\bibitem[\citeproctext]{ref-braga2001}
Braga, Alfésio Luís Ferreira, Antonella Zanobetti, and Joel Schwartz.
2001. {``The Lag Structure Between Particulate Air Pollution and
Respiratory and Cardiovascular Deaths in 10 US Cities.''} \emph{Journal
of Occupational and Environmental Medicine} 43 (11): 927.
\url{https://journals.lww.com/joem/fulltext/2001/11000/the_lag_structure_between_particulate_air.1.aspx}.

\bibitem[\citeproctext]{ref-burnett1998}
Burnett, Richard T., Sabit Cakmak, Mark E. Raizenne, et al. 1998. {``The
Association Between Ambient Carbon Monoxide Levels and Daily Mortality
in Toronto, Canada.''} \emph{Journal of the Air \& Waste Management
Association} 48 (8): 689--700.
\url{https://doi.org/10.1080/10473289.1998.10463718}.

\bibitem[\citeproctext]{ref-burnett2004}
Burnett, Richard T., Stieb ,Dave, Brook ,Jeffrey R., et al. 2004.
{``Associations Between Short-Term Changes in Nitrogen Dioxide and
Mortality in Canadian Cities.''} \emph{Archives of Environmental Health:
An International Journal} 59 (5): 228--36.
\url{https://doi.org/10.3200/AEOH.59.5.228-236}.

\bibitem[\citeproctext]{ref-castillejos2000}
Castillejos, Margarita, Borja-Aburto ,Victor H., Dockery ,Douglas W.,
Gold ,Diane R., and Dana. and Loomis. 2000. {``Airborne Coarse Particles
and Mortality.''} \emph{Inhalation Toxicology} 12 (sup1): 61--72.
\url{https://doi.org/10.1080/0895-8378.1987.11463182}.

\bibitem[\citeproctext]{ref-chen2006}
Chen, Chaomei. 2006. {``CiteSpace II: Detecting and Visualizing Emerging
Trends and Transient Patterns in Scientific Literature.''} \emph{Journal
of the American Society for Information Science and Technology} 57 (3):
359--77. \url{https://doi.org/10.1002/asi.20317}.

\bibitem[\citeproctext]{ref-chou2011}
Chou, J. -K., and C. -K. Yang. 2011. {``PaperVis: Literature Review Made
Easy.''} \emph{Computer Graphics Forum} 30 (3): 721--30.
\url{https://doi.org/10.1111/j.1467-8659.2011.01921.x}.

\bibitem[\citeproctext]{ref-devlin2019}
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
{``NAACL-HLT 2019.''} Edited by Jill Burstein, Christy Doran, and Thamar
Solorio. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/N19-1423}.

\bibitem[\citeproctext]{ref-dockery1992}
Dockery, Douglas W., Joel Schwartz, and John D. Spengler. 1992. {``Air
Pollution and Daily Mortality: Associations with Particulates and Acid
Aerosols.''} \emph{Environmental Research} 59 (2): 362--73.
\url{https://doi.org/10.1016/S0013-9351(05)80042-8}.

\bibitem[\citeproctext]{ref-duxf6rk2012}
Dörk, Marian, Nathalie Henry Riche, Gonzalo Ramos, and Susan Dumais.
2012. {``PivotPaths: Strolling Through Faceted Information Spaces.''}
\emph{IEEE Transactions on Visualization and Computer Graphics} 18 (12):
2709--18. \url{https://doi.org/10.1109/TVCG.2012.252}.

\bibitem[\citeproctext]{ref-farzi2024}
Farzi, Saeed, Soumitra Ghosh, Alberto Lavelli, and Bernardo Magnini.
2024. {``Get the Best Out of 1B LLMs: Insights from Information
Extraction on Clinical Documents.''} Edited by Dina Demner-Fushman,
Sophia Ananiadou, Makoto Miwa, Kirk Roberts, and Junichi Tsujii.
Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2024.bionlp-1.21}.

\bibitem[\citeproctext]{ref-gelman2014}
Gelman, Andrew, and Eric Loken. 2014. {``The Statistical Crisis in
Science.''} \emph{American Scientist} 102 (6): 460--65.
\url{https://www.proquest.com/docview/1616141998/abstract/5E050DCE82414037PQ/1}.

\bibitem[\citeproctext]{ref-guxf6tz2024}
Götz, Martin, Abhraneel Sarma, and Ernest H. O'Boyle. 2024. {``The
Multiverse of Universes: A Tutorial to Plan, Execute and Interpret
Multiverses Analyses Using the R Package Multiverse.''}
\emph{International Journal of Psychology} 59 (6): 1003--14.
\url{https://doi.org/10.1002/ijop.13229}.

\bibitem[\citeproctext]{ref-gould2025}
Gould, Elliot, Hannah S. Fraser, Timothy H. Parker, et al. 2025. {``Same
Data, Different Analysts: Variation in Effect Sizes Due to Analytical
Decisions in Ecology and Evolutionary Biology.''} \emph{BMC Biology} 23
(1): 35. \url{https://doi.org/10.1186/s12915-024-02101-x}.

\bibitem[\citeproctext]{ref-gu2025}
Gu, Bowen, Vivian Shao, Ziqian Liao, et al. 2025. {``Scalable
Information Extraction from Free Text Electronic Health Records Using
Large Language Models.''} \emph{BMC Medical Research Methodology} 25
(1): 23. \url{https://doi.org/10.1186/s12874-025-02470-z}.

\bibitem[\citeproctext]{ref-gu2023}
Gu, Ken, Eunice Jun, and Tim Althoff. 2023. {``Understanding and
Supporting Debugging Workflows in Multiverse Analysis.''} (New York, NY,
USA), CHI '23, April 19, 119.
\url{https://doi.org/10.1145/3544548.3581099}.

\bibitem[\citeproctext]{ref-gupta2024}
Gupta, Sonakshi, Akhlak Mahmood, Pranav Shetty, Aishat Adeboye, and
Rampi Ramprasad. 2024. {``Data Extraction from Polymer Literature Using
Large Language Models.''} \emph{Communications Materials} 5 (1): 269.
\url{https://doi.org/10.1038/s43246-024-00708-9}.

\bibitem[\citeproctext]{ref-harrod2024}
Harrod, Karlyn K., Prabin Bhandari, and Antonios Anastasopoulos. 2024.
{``From Text to Maps: LLM-Driven Extraction and Geotagging of
Epidemiological Data.''} Edited by Daryna Dementieva, Oana Ignat,
Zhijing Jin, et al. Association for Computational Linguistics.
\url{https://doi.org/10.18653/v1/2024.nlp4pi-1.24}.

\bibitem[\citeproctext]{ref-heimerl2016}
Heimerl, Florian, Qi Han, Steffen Koch, and Thomas Ertl. 2016.
{``CiteRivers: Visual Analytics of Citation Patterns.''} \emph{IEEE
Transactions on Visualization and Computer Graphics} 22 (1): 190--99.
\url{https://doi.org/10.1109/TVCG.2015.2467621}.

\bibitem[\citeproctext]{ref-hu2024}
Hu, Yan, Qingyu Chen, Jingcheng Du, et al. 2024. {``Improving Large
Language Models for Clinical Named Entity Recognition via Prompt
Engineering.''} \emph{Journal of the American Medical Informatics
Association} 31 (9): 1812--20.
\url{https://doi.org/10.1093/jamia/ocad259}.

\bibitem[\citeproctext]{ref-huntington-klein2021}
Huntington-Klein, Nick, Andreu Arenas, Emily Beam, et al. 2021. {``The
Influence of Hidden Researcher Decisions in Applied Microeconomics.''}
\emph{Economic Inquiry} 59 (3): 944--60.
\url{https://doi.org/10.1111/ecin.12992}.

\bibitem[\citeproctext]{ref-isenberg2017}
Isenberg, Petra, Tobias Isenberg, Michael Sedlmair, Jian Chen, and
Torsten Möller. 2017. {``Visualization as Seen Through Its Research
Paper Keywords.''} \emph{IEEE Transactions on Visualization and Computer
Graphics} 23 (1): 771--80.
\url{https://doi.org/10.1109/TVCG.2016.2598827}.

\bibitem[\citeproctext]{ref-ito2006}
Ito, Kazuhiko, William F. Christensen, Delbert J. Eatough, et al. 2006.
{``PM Source Apportionment and Health Effects: 2. An Investigation of
Intermethod Variability in Associations Between Source-Apportioned Fine
Particle Mass and Daily Mortality in Washington, DC.''} \emph{Journal of
Exposure Science \& Environmental Epidemiology} 16 (4): 300--310.
\url{https://doi.org/10.1038/sj.jea.7500464}.

\bibitem[\citeproctext]{ref-jun2022hypothesis}
Jun, Eunice, Melissa Birchfield, Nicole De Moura, Jeffrey Heer, and René
Just. 2022. {``Hypothesis Formalization: Empirical Findings, Software
Limitations, and Design Implications.''} \emph{ACM Transactions on
Computer-Human Interaction (TOCHI)} 29 (1): 1--28.

\bibitem[\citeproctext]{ref-jun2019}
Jun, Eunice, Maureen Daum, Jared Roesch, et al. 2019. {``Tea: A
High-Level Language and Runtime System for Automating Statistical
Analysis.''} (New York, NY, USA), UIST '19, October 17, 591603.
\url{https://doi.org/10.1145/3332165.3347940}.

\bibitem[\citeproctext]{ref-jun2022}
Jun, Eunice, Audrey Seo, Jeffrey Heer, and René Just. 2022. {``Tisane:
Authoring Statistical Models via Formal Reasoning from Conceptual and
Data Relationships.''} (New York, NY, USA), CHI '22, April 29, 116.
\url{https://doi.org/10.1145/3491102.3501888}.

\bibitem[\citeproctext]{ref-kale2019}
Kale, Alex, Matthew Kay, and Jessica Hullman. 2019. {``Decision-Making
Under Uncertainty in Research Synthesis: Designing for the Garden of
Forking Paths.''} (New York, NY, USA), CHI '19, May 2, 114.
\url{https://doi.org/10.1145/3290605.3300432}.

\bibitem[\citeproctext]{ref-kale2023}
Kale, Alex, Sarah Lee, Terrance Goan, Elizabeth Tipton, and Jessica
Hullman. 2023. {``MetaExplorer : Facilitating Reasoning with Epistemic
Uncertainty in Meta-Analysis.''} (New York, NY, USA), CHI '23, April 19,
114. \url{https://doi.org/10.1145/3544548.3580869}.

\bibitem[\citeproctext]{ref-kan2007}
Kan, Haidong, Stephanie J. London, Guohai Chen, et al. 2007.
{``Differentiating the Effects of Fine and Coarse Particles on Daily
Mortality in Shanghai, China.''} \emph{Environment International} 33
(3): 376--84. \url{https://doi.org/10.1016/j.envint.2006.12.001}.

\bibitem[\citeproctext]{ref-kandel2012}
Kandel, Sean, Andreas Paepcke, Joseph M. Hellerstein, and Jeffrey Heer.
2012. {``Enterprise Data Analysis and Visualization: An Interview
Study.''} \emph{IEEE Transactions on Visualization and Computer
Graphics} 18 (12): 2917--26.
\url{https://doi.org/10.1109/TVCG.2012.219}.

\bibitem[\citeproctext]{ref-APHENA}
Katsouyanni, Klea, Jonathan M. Samet, H. Ross Anderson, et al. 2009.
\emph{Air Pollution and Health: {A} European and North American Approach
(APHENA)}. Research Report. (Boston, MA), no. 142.

\bibitem[\citeproctext]{ref-katsouyanni1996}
Katsouyanni, Klea, Joel Schwartz, Claudia Spix, et al. 1996. {``Short
Term Effects of Air Pollution on Health: A European Approach Using
Epidemiologic Time Series Data: The APHEA Protocol.''} \emph{Journal of
Epidemiology \& Community Health} 50 (Suppl 1): S12--18.

\bibitem[\citeproctext]{ref-katsouyanni2001}
Katsouyanni, Klea, Giota Touloumi, Evangelia Samoli, et al. 2001.
{``Confounding and Effect Modification in the Short-Term Effects of
Ambient Particles on Total Mortality: Results from 29 European Cities
Within the APHEA2 Project.''} \emph{Epidemiology} 12 (5): 521.
\url{https://journals.lww.com/epidem/fulltext/2001/09000/confounding_and_effect_modification_in_the.11.aspx}.

\bibitem[\citeproctext]{ref-katz2024}
Katz, Uri, Mosh Levy, and Yoav Goldberg. 2024. {``Findings of the
Association for Computational Linguistics: EMNLP 2024.''} (Miami,
Florida, USA), 8838--55.
\url{https://doi.org/10.18653/v1/2024.findings-emnlp.516}.

\bibitem[\citeproctext]{ref-text}
Kjell, Oscar, Salvatore Giorgi, and H. Andrew Schwartz. 2023. {``The
Text-Package: An r-Package for Analyzing and Visualizing Human Language
Using Natural Language Processing and Deep Learning.''}
\emph{Psychological Methods}, ahead of print.
\url{https://doi.org/10.1037/met0000542}.

\bibitem[\citeproctext]{ref-ko2007}
Ko, F. W. S., W. Tam, T. W. Wong, et al. 2007. {``Effects of Air
Pollution on Asthma Hospitalization Rates in Different Age Groups in
Hong Kong.''} \emph{Clinical \& Experimental Allergy} 37 (9): 1312--19.
\url{https://doi.org/10.1111/j.1365-2222.2007.02791.x}.

\bibitem[\citeproctext]{ref-lafferty}
Lafferty, John, Andrew McCallum, and Fernando Pereira. n.d.
\emph{Conditional Random Fields: Probabilistic Models for Segmenting and
Labeling Sequence Data}.

\bibitem[\citeproctext]{ref-lee2020}
Lee, Jinhyuk, Wonjin Yoon, Sungdong Kim, et al. 2020. {``BioBERT: A
Pre-Trained Biomedical Language Representation Model for Biomedical Text
Mining.''} \emph{Bioinformatics} 36 (4): 1234--40.
\url{https://doi.org/10.1093/bioinformatics/btz682}.

\bibitem[\citeproctext]{ref-li2024}
Li, Ni, Shorouq Zahra, Mariana Brito, et al. 2024. {``Proceedings of the
1st Workshop on Natural Language Processing Meets Climate Change
(ClimateNLP 2024).''} (Bangkok, Thailand), 93--110.
\url{https://doi.org/10.18653/v1/2024.climatenlp-1.7}.

\bibitem[\citeproctext]{ref-liu_understanding_2020}
Liu, Jiali, Nadia Boukhelifa, and James R. Eagan. 2020. {``Understanding
the {Role} of {Alternatives} in {Data} {Analysis} {Practices}.''}
\emph{IEEE Transactions on Visualization and Computer Graphics} 26 (1):
66--76. \url{https://doi.org/10.1109/TVCG.2019.2934593}.

\bibitem[\citeproctext]{ref-liu2020}
Liu, Yang, Tim Althoff, and Jeffrey Heer. 2020. {``Paths Explored, Paths
Omitted, Paths Obscured: Decision Points \& Selective Reporting in
End-to-End Data Analysis.''} (New York, NY, USA), CHI '20, April 23,
114. \url{https://doi.org/10.1145/3313831.3376533}.

\bibitem[\citeproctext]{ref-liu2021}
Liu, Yang, Alex Kale, Tim Althoff, and Jeffrey Heer. 2021. {``Boba:
Authoring and Visualizing Multiverse Analyses.''} \emph{IEEE
Transactions on Visualization and Computer Graphics} 27 (2): 1753--63.
\url{https://doi.org/10.1109/TVCG.2020.3028985}.

\bibitem[\citeproctext]{ref-liu}
Liu, Yinhan, Myle Ott, Naman Goyal, et al. n.d. \emph{RoBERTa: A
Robustly Optimized BERT Pretraining Approach}.
\url{https://doi.org/10.48550/arXiv.1907.11692}.

\bibitem[\citeproctext]{ref-luxf3pez-villarrubia2010}
López-Villarrubia, Elena, Ferran Ballester, Carmen Iñiguez, and Nieves
Peral. 2010. {``Air Pollution and Mortality in the Canary Islands: A
Time-Series Analysis.''} \emph{Environmental Health} 9 (February): 8.
\url{https://doi.org/10.1186/1476-069X-9-8}.

\bibitem[\citeproctext]{ref-mar2000}
Mar, T F, G A Norris, J Q Koenig, and T V Larson. 2000. {``Associations
Between Air Pollution and Mortality in Phoenix, 1995-1997.''}
\emph{Environmental Health Perspectives} 108 (4): 347--53.
\url{https://doi.org/10.1289/ehp.00108347}.

\bibitem[\citeproctext]{ref-mikolov2013}
Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.
2013. {``Distributed Representations of Words and Phrases and Their
Compositionality.''} 26.
\url{https://papers.nips.cc/paper_files/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html}.

\bibitem[\citeproctext]{ref-moolgavkar2000}
Moolgavkar, Suresh H. 2000. {``Air Pollution and Hospital Admissions for
Diseases of the Circulatory System in Three u.s. Metropolitan Areas.''}
\emph{Journal of the Air \& Waste Management Association} 50 (7):
1199--206. \url{https://doi.org/10.1080/10473289.2000.10464162}.

\bibitem[\citeproctext]{ref-moolgavkar2003}
Moolgavkar, Suresh H. 2003. {``Air Pollution and Daily Mortality in Two
u.s. Counties: Season-Specific Analyses and Exposure-Response
Relationships.''} \emph{Inhalation Toxicology} 15 (9): 877--907.
\url{https://doi.org/10.1080/08958370390215767}.

\bibitem[\citeproctext]{ref-nadeau2007}
Nadeau, David, and Satoshi Sekine. 2007. {``A Survey of Named Entity
Recognition and Classification.''} \emph{Lingvisticæ Investigationes} 30
(1): 3--26. \url{https://doi.org/10.1075/li.30.1.03nad}.

\bibitem[\citeproctext]{ref-narechania2022}
Narechania, Arpit, Alireza Karduni, Ryan Wesslen, and Emily Wall. 2022.
{``VITALITY: Promoting Serendipitous Discovery of Academic Literature
with Transformers \& Visual Analytics.''} \emph{IEEE Transactions on
Visualization and Computer Graphics} 28 (1): 486--96.
\url{https://doi.org/10.1109/TVCG.2021.3114820}.

\bibitem[\citeproctext]{ref-peng2006}
Peng, Roger D., Francesca Dominici, and Thomas A. Louis. 2006. {``Model
Choice in Time Series Studies of Air Pollution and Mortality.''}
\emph{Journal of the Royal Statistical Society Series A: Statistics in
Society} 169 (2): 179--203.
\url{https://doi.org/10.1111/j.1467-985X.2006.00410.x}.

\bibitem[\citeproctext]{ref-polak2024}
Polak, Maciej P., and Dane Morgan. 2024. {``Extracting Accurate
Materials Data from Research Papers with Conversational Language Models
and Prompt Engineering.''} \emph{Nature Communications} 15 (1): 1569.
\url{https://doi.org/10.1038/s41467-024-45914-8}.

\bibitem[\citeproctext]{ref-R}
R Core Team. 2025. \emph{R: A Language and Environment for Statistical
Computing}. R Foundation for Statistical Computing.
\url{https://www.R-project.org/}.

\bibitem[\citeproctext]{ref-samet2000}
Samet, Jonathan M., Francesca Dominici, Frank C. Curriero, Ivan Coursac,
and Scott L. Zeger. 2000. {``Fine Particulate Air Pollution and
Mortality in 20 u.s. Cities, 1987{\textendash}1994.''} \emph{New England
Journal of Medicine} 343 (24): 1742--49.
\url{https://doi.org/10.1056/NEJM200012143432401}.

\bibitem[\citeproctext]{ref-multiverse}
Sarma, Abhraneel, Alex Kale, Michael Moon, et al. 2021. {``Multiverse:
Multiplexing Alternative Data Analyses in r Notebooks (Version
0.6.2).''} \emph{OSF Preprints}.
\url{https://github.com/MUCollective/multiverse}.

\bibitem[\citeproctext]{ref-sarstedt2024}
Sarstedt, Marko, Susanne J. Adler, Christian M. Ringle, et al. 2024.
{``Same Model, Same Data, but Different Outcomes: Evaluating the Impact
of Method Choices in Structural Equation Modeling.''} \emph{Journal of
Product Innovation Management} 41 (6): 1100--1117.
\url{https://doi.org/10.1111/jpim.12738}.

\bibitem[\citeproctext]{ref-schilling-wilhelmi2025}
Schilling-Wilhelmi, Mara, Martiño Ríos-García, Sherjeel Shabih, et al.
2025. {``From Text to Insight: Large Language Models for Chemical Data
Extraction.''} \emph{Chemical Society Reviews} 54 (3): 1125--50.
\url{https://doi.org/10.1039/D4CS00913D}.

\bibitem[\citeproctext]{ref-schwartz2000}
Schwartz, Joel. 2000. {``The Distributed Lag Between Air Pollution and
Daily Deaths.''} \emph{Epidemiology} 11 (3): 320--26.
\url{https://www.jstor.org/stable/3703220}.

\bibitem[\citeproctext]{ref-sciannameo2024}
Sciannameo, Veronica, Daniele Jahier Pagliari, Sara Urru, et al. 2024.
{``Information Extraction from Medical Case Reports Using OpenAI
InstructGPT.''} \emph{Computer Methods and Programs in Biomedicine} 255
(October): 108326. \url{https://doi.org/10.1016/j.cmpb.2024.108326}.

\bibitem[\citeproctext]{ref-silberzahn2018}
Silberzahn, R., E. L. Uhlmann, D. P. Martin, et al. 2018. {``Many
Analysts, One Data Set: Making Transparent How Variations in Analytic
Choices Affect Results.''} \emph{Advances in Methods and Practices in
Psychological Science} 1 (3): 337--56.
\url{https://doi.org/10.1177/2515245917747646}.

\bibitem[\citeproctext]{ref-simmons2011}
Simmons, Joseph P., Leif D. Nelson, and Uri Simonsohn. 2011.
{``False-Positive Psychology: Undisclosed Flexibility in Data Collection
and Analysis Allows Presenting Anything as Significant.''}
\emph{Psychological Science} 22 (11): 1359--66.
\url{https://doi.org/10.1177/0956797611417632}.

\bibitem[\citeproctext]{ref-simson2025}
Simson, Jan, Fiona Draxler, Samuel Mehr, and Christoph Kern. 2025.
{``Preventing Harmful Data Practices by Using Participatory Input to
Navigate the Machine Learning Multiverse.''} (New York, NY, USA), CHI
'25, April 25, 130. \url{https://doi.org/10.1145/3706598.3713482}.

\bibitem[\citeproctext]{ref-tbahriti2006}
Tbahriti, Imad, Christine Chichester, Frédérique Lisacek, and Patrick
Ruch. 2006. {``Using Argumentation to Retrieve Articles with Similar
Citations: An Inquiry into Improving Related Articles Search in the
MEDLINE Digital Library.''} \emph{International Journal of Medical
Informatics}, Recent advances in natural language processing for
biomedical applications special issue, vol. 75 (6): 488--95.
\url{https://doi.org/10.1016/j.ijmedinf.2005.06.007}.

\bibitem[\citeproctext]{ref-touloumi2006}
Touloumi, G., E. Samoli, M. Pipikou, A. Le Tertre, R. Atkinson, and K.
Katsouyanni. 2006. {``Seasonal Confounding in Air Pollution and Health
Time-Series Studies: Effect on Air Pollution Effect Estimates.''}
\emph{Statistics in Medicine} 25 (24): 4164--78.
\url{https://doi.org/10.1002/sim.2681}.

\bibitem[\citeproctext]{ref-ueda2009}
Ueda, Kayo, Nitta ,Hiroshi, Ono ,Masaji, and Ayano and Takeuchi. 2009.
{``Estimating Mortality Effects of Fine Particulate Matter in Japan: A
Comparison of Time-Series and Case-Crossover Analyses.''} \emph{Journal
of the Air \& Waste Management Association} 59 (10): 1212--18.
\url{https://doi.org/10.3155/1047-3289.59.10.1212}.

\bibitem[\citeproctext]{ref-wicherts2016}
Wicherts, Jelte M., Coosje L. S. Veldkamp, Hilde E. M. Augusteijn,
Marjan Bakker, Robbie C. M. van Aert, and Marcel A. L. M. van Assen.
2016. {``Degrees of Freedom in Planning, Running, Analyzing, and
Reporting Psychological Studies: A Checklist to Avoid p-Hacking.''}
\emph{Frontiers in Psychology} 7 (November).
\url{https://doi.org/10.3389/fpsyg.2016.01832}.

\bibitem[\citeproctext]{ref-wickham2014}
Wickham, Hadley. 2014. {``Tidy Data.''} \emph{Journal of Statistical
Software} 59 (September): 1--23.
\url{https://doi.org/10.18637/jss.v059.i10}.

\bibitem[\citeproctext]{ref-tidyverse}
Wickham, Hadley, Mara Averick, Jennifer Bryan, et al. 2019. {``Welcome
to the {tidyverse}.''} \emph{Journal of Open Source Software} 4 (43):
1686. \url{https://doi.org/10.21105/joss.01686}.

\bibitem[\citeproctext]{ref-ellmer}
Wickham, Hadley, Joe Cheng, and Aaron Jacobs. 2025. \emph{Ellmer: Chat
with Large Language Models}.
\url{https://CRAN.R-project.org/package=ellmer}.

\bibitem[\citeproctext]{ref-yang}
Yang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan
Salakhutdinov, and Quoc V. Le. n.d. \emph{XLNet: Generalized
Autoregressive Pretraining for Language Understanding}.
\url{https://doi.org/10.48550/arXiv.1906.08237}.

\end{CSLReferences}




\end{document}
