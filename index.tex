\documentclass[manuscript,screen,review,anonymous]{acmart}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}

%% PANDOC PREAMBLE BEGINS

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
%% PANDOC PREAMBLE ENDS

\setlength{\parindent}{10pt}
\setlength{\parskip}{0pt}

\hypersetup{
  pdftitle={Dossier: visualizing/ understanding decision choices in data analysis via decision similarity},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc, via quarto}}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI'26]{CHI Conference on Human Factors in Computing
Systems}{Apr 13--17, 2026}{Barcelona, Spain}
\acmPrice{}
\acmISBN{978-1-4503-XXXX-X/18/06}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Dossier: visualizing/ understanding decision choices in data
analysis via decision similarity}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


  \author{H. Sherry Zhang}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
        \author{Roger D. Peng}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
      

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}
%%  
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
In data analysis, analysts are expected to clearly communicate the
decisions they make, as these choices inform how results are interpreted
and compared across studies. Such decisions -- for example, selecting
the degree of freedom for a smoothing spline -- are often not
systematically studied, since onece an analysis is published, it is done
and seldomly revisited or replicated with alternative choices. In this
work, we focus on a body of data analysis studies on the effect of
particulate matter on mortality, conducted by researchers worldwide,
which naturally provide alteranative analyes of the same question. We
automatically extract analytic decisions from the published literature
into structured data using Large Language Models (Claude and Gemini). We
then proposed a pipeline to calculate paper similarity based on the
semantic similarity of these extracted decisions and their reasons, and
visualize the results through clustering algorithms. This approach
offers an efficient way to study decision-making practices than
traditional interviews. We also provide insights into the use of LLMs
for text extraction tasks and the communication of analytic choices in
data analysis practice.    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
    <concept_id>10010405.10010497.10010504.10010505</concept_id>
    <concept_desc>Applied computing~Document analysis</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
   <concept>
    <concept_id>10003120.10003121.10003126</concept_id>
    <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Applied computing~Document analysis}
\ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large language models}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\setlength{\parskip}{-0.1pt}

\begin{itemize}
\tightlist
\item
  Something about ``analysis review'' - Roger thinks it's a better to
  have a new word for this.
\item
  demonstrate - analytically homogeneous - the table won't look like
  that
\end{itemize}

\section{Introduction}\label{introduction}

Decisions are everywhere in data analysis, from the initial data
collection, data pre-processing to the modelling choices. These
decisions will impact the final output of the data analysis, which may
lead to different conclusions and policy recommendations. When such
flexibility can be misused---through practices such as p-hacking,
selective reporting, or unjustified analytical adjustments---it can
inflate effect sizes or produce misleading results that meet
conventional thresholds for statistical significance. They have been
demonstrated through many-analysts experiments, where independent teams
analyzing the same dataset to answer a pre-defined research question
often arrive at markedly different conclusions. These practices not only
compromise the validity of individual studies but also threaten the
broader credibility of statistical analysis and scientific research as a
whole.

Multiple recommendations have been proposed to improve data analysis
practices, such as pre-registration and multiverse analysis. Bayesian
methods also offer a different paradigm to p-value driven inference for
interpreting statistical evidence. Most empirical studies of data
analysis practices focus on specially designed and simplified analysis
scenarios. While informative, these setups may not adequately capture
the complexity of the data analysis with significant policy
implications. {[}In practice, studying the data analysis decisions with
actual applications is challenging.{]} Analysts may no longer be
available for interviews due to job changes, and even when they are,
recalling the full set of decisions and thinking process made during the
analysis is often infeasible. Moreover, only until the last decades,
analysis scripts and reproducible materials were not commonly required
by journals for publishing. {[}As a result, it remains challenging to
study how analytical decisions are made. {]}

In this work, we focus on a specific class of air pollution modelling
studies that estimate the effect size of particulate matter (PM2.5 or
PM10) on mortality, typically using Poisson regression or generalized
additive models (GAMs). While individual modelling choices vary, these
studies often share a common structure: they adjust for meteorological
covariates such as temperature and humidity, apply temporal or spatial
treatments, like including lagged variables and may estimate the effect
by city or region before combining results. Because these studies
investigate similar scientific questions using a shared modelling
framework, they form a natural many-analyst setting. This allows us to
examine, in a real-world context, the range of analytical decisions made
by different researchers addressing the same underlying question.

In this work, we develop a structured tabular format to record the
analytical decisions made by researchers in the air pollution modelling
literature. Using large language models (LLMs), we automate the
extraction of these decisions from published papers. This allows us to
treat decisions as data -- allowing us to track them over time, compare
methodology across papers, and query commonly used approaches. We
further introduce a workflow to cluster studies based on decision
similarity, revealing three distinct groups of papers that reflect the
modelling strategies differs in the European and U.S. studies, which
offers a new way to visualize the field in the air pollution mortality
modelling.

The contribution of this work includes:

\begin{itemize}
\item
  A new approach to study data analysis decision choices through
  automatic extraction of decisions from scientific literature using
  LLMs,
\item
  A dataset compiled from 62 papers to study decision-making in air
  pollution mortality modelling,
\item
  A pipeline to construct similarities between papers based on decision
  similarities, and
\item
  Issues we found from existing data analysis reporting
\end{itemize}

The rest of the paper is organized as follows. In
Section~\ref{sec-background}, we review the background on data analysis
decisions. Section~\ref{sec-extract-decisions} describes the data
structure for recording decisions, the use of large language models to
process research papers, and the validation of LLM outputs. In
Section~\ref{sec-paper-similarity}, we present the method for
calculating paper similarity based on decision similarities.
Section~\ref{sec-result} reports the finding of our analysis, including
the clustering of papers according to similarity scores and sensitivity
analyses related to LLM providers, prompt engineering, and LLM
parameters. Finally, Section~\ref{sec-discussion} discusses the
implications of our study.

\section{Related work}\label{sec-background}

\subsection{Decision-making in data
analysis}\label{decision-making-in-data-analysis}

A data analysis is a process of making choices at each step, from the
initial data collection to model specification, and post-processing.
Each decision represents a branching point where analysts choose a
specific path to follow, and the vast number of possible choices
analysts can take forms what \citet{gelman2014} describe as the ``garden
of forking paths''. While researchers may hope their inferential results
are robust to the specific path taken through the garden, in practice,
different choices can lead to substantially different conclusions. This
has been empirically demonstrated through ``many analyst experiments'',
where independent research groups analyze the same dataset to the same
answer using their chosen analytic approach. A classic example is
\citet{silberzahn2018}, where researchers reported an odds ratio from
0.89 to 2.93 for the effect of soccer players' skin tone on the number
of red cards awarded by referees. Similar variability has been observed
in structural equation modeling \citep{sarstedt2024}, applied
microeconomics \citep{huntington-klein2021}, neuroimaging
\citep{botvinik-nezer2020}, and ecology and evolutionary biology
\citep{gould2025}.

Examples above have rendered decision-making in data analysis as a
subject to study in data science. To collect data on how analysts making
decisions during data analysis, researchers have conducted interviews
with analysts and researchers on data analysis practices
\citep{kale2019, alspaugh2019, liu_understanding_2020}, visualization of
the decision process through the analytic decision graphics (ADG)
\citep{liu2020}. Recently, \citet{simson2025} describes a participatory
approach to decisions choices in fairness ML algorithms. Software tools
have also developed to incorporate potential alternatives in the
analysis workflow, including the \texttt{DeclareDesign} package
\citep{blair2019} and the \texttt{multiverse} package
\citep{multiverse}. The \texttt{DeclareDesign} package \citep{blair2019}
introduces the MIDA framework for researchers to declare, diagnose, and
redesign their analyses to produce a distribution of the statistic of
interest, which has been applied in the randomized controlled trial
study \citep{bishop2024}. The \texttt{multiverse} package
\citep{multiverse} provides a framework for researchers to
systematically explore how different choices affect results and to
report the range of plausible outcomes that arise from alternative
analytic paths. Other systems have been developed to visualize
\texttt{multiverse} analysis \citep{liu2021}.

\subsection{Visualization on scientific
literature}\label{visualization-on-scientific-literature}

Much of the work on IEEE visualizing scientific literature focuses on
helping researchers stay aware of relevant publications, given the
rapidly growing volume of scientific output and the difficulty of
navigating it. Systems have been developed to support the discovery of
relevant papers, where relevance is typically determined by keywords
\citep{isenberg2017}, citation information (e.g.~citation list,
co-citation) \citep{chen2006}, or combinations with other relevant paper
metadata (e.g.~author, title)
\citep{bethard2010, chou2011, dörk2012, heimerl2016}. More recent
approaches incorporate text-based information from the abstract or
sections of the paper to {[}obtain a better similar metric{]}. This
includes using topic modelling \citep{alexander2014},
argumentation-based information retrieval \citep{tbahriti2006}, and text
embedding \citep{narechania2022}. While these metadata information and
high level text-based information are valuable for discovering relevant
papers, for data analysis, researchers need tools that help them
\emph{make sense} of the literature rather than simply \emph{finding}
it. Capturing the decisions and reasoning expressed during analyses
within a similar theme can reveal common practices in the field and
guide decisions choices in new applications. With recent advances in
Large Language Models (LLMs), it has become possible to automatically
extract structured information from unstructured text through prompting.
This allows scientific literature to be clustered and visualized using
information about the underlying decisions and reasoning made during
analysis, providing a basis for studying analysts' decision choices.

\section{Methods}\label{sec-extract-decisions}

\subsection{Decisions in data analysis}\label{sec-decisions}

Decisions occur throughout the entire data analysis process -- from the
selection of variables and data source, to pre-processing steps to
prepare the data for modelling, to the model specification and variable
inclusion. In this work, we focus specifically on modelling decisions in
the air pollution mortality modelling literature. These include the
choice of modelling approach, covariate inclusion and smoothing, and
specifications of spatial and temporal structure. Consider the following
excerpt from \citet{ostro2006}:

\begin{quote}
Based on previous findings reported in the literature (e.g., Samet et
al.~2000), the basic model included a smoothing spline for time with 7
degrees of freedom (df) per year of data. This number of degrees of
freedom controls well for seasonal patterns in mortality and reduces and
often eliminates autocorrelation.
\end{quote}

This sentence encode the following components of a decision:

\begin{itemize}
\tightlist
\item
  \textbf{variable}: time
\item
  \textbf{method}: smoothing spline
\item
  \textbf{parameter}: degree of freedom (df)
\item
  \textbf{reason}: Based on previous findings reported in the literature
  (e.g., Samet et al.~2000); This number of degrees of freedom controls
  well for seasonal patterns in mortality and reduces and often
  eliminates autocorrelation.
\item
  \textbf{decision}: 7 degrees of freedom (df) per year of data
\end{itemize}

The decision above is regarding a certain parameter in the statistical
method, we categorize this as a ``parameter'' type decisions. Other
types of decisions - such as spatial modelling structure or the
inclusion of temporal lags - may not include an explicit method or
parameter, but still reference a variable and rationale, which we will
provide further examples below.

To record these decisions, we follow the tidy data principle
\citep{wickham2014}, where each variable should be in a column, each
observation in a row. In our context, each row represents a decision
made by the authors of a paper and an analysis often include multiple
decisions. To retain the original context of the decision, we extract
the original text in the paper, without paraphrase or summarization,
from the paper. Below we present an example of how to structure the
decisions made in a paper, using the paper by \citet{ostro2006}:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}
  >{\raggedright\arraybackslash}p{(\linewidth - 16\tabcolsep) * \real{0.1111}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Model
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ostro & 1 & Poisson regression & temperature & smoothing spline & degree
of freedom & parameter & NA & 3 degree of freedom \\
ostro & 2 & Poisson regression & temperature & smoothing spline & degree
of freedom & temporal & NA & 1-day lag \\
ostro & 3 & Poisson regression & relative humidity & LOESS & smoothing
parameter & parameter & to minimize Akaike's Information Criterion &
NA \\
ostro & 4 & Poisson regression & model & NA & NA & spatial & to account
for variation among cities & separate regression models fit in each
city \\
\end{longtable}

Most decisions in the published papers are not explicitly stated, this
could due to the coherence and conciseness of the writing or authors'
decision to include only necessary details. Here, we identify a few
common anomalies where decisions may be combined or omit certain fields:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Authors may combine multiple decisions into a single sentence}
  for coherence and conciseness of the writing. Consider the following
  excerpt from \citet{ostro2006}:
\end{enumerate}

\begin{quote}
Other covariates, such as day of the week and smoothing splines of 1-day
lags of average temperature and humidity (each with 3 df), were also
included in the model because they may be associated with daily
mortality and are likely to vary over time in concert with air pollution
levels.
\end{quote}

This sentence contains four decisions: two for temperature (the temporal
lag and the smoothing spline parameter) and two for humidity. These
decisions should be structured as separate entries.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  \textbf{The justification does not directly address the decision
  choice.} In the example above, the stated rationale (``and are likely
  to vary over time in concert with air pollution levels'') supports the
  general inclusion of temporal lags but does not justify the specific
  choice of 1-day lag over alternatives, such as 2-day average of lags 0
  and 1 (lag01) and single-day lag of 2 days (lag2). As such, the reason
  field should be recorded as NA.
\item
  \textbf{Some decisions may be omitted because they are data-driven}.
  For instance, \citet{katsouyanni2001} states:
\end{enumerate}

\begin{quote}
The inclusion of lagged weather variables and the choice of smoothing
parameters for all of the weather variables were done by minimizing
Akaike's information criterion.
\end{quote}

In this case, while the method of selection (minimizing AIC) is
specified, the actual degree of freedom used is not. Such data-driven
decisions may be recorded with ``NA'' in the decision field, but the
reason field should still be recorded as ``by minimizing Akaike's
information criterion''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  \textbf{Information required to interpret the decision may be
  distributed across multiple sections}. In the previous example,
  ``weather variables'' refers to mean temperature and relative
  humidity, as defined earlier in the text. This requires
  cross-referencing across sections to identify the correct variables
  associated with each modeling choice.
\end{enumerate}

\subsection{Automatic reading of literature with
LLMs}\label{automatic-reading-of-literature-with-llms}

\textbf{TODO}: Prompt engineering: these models may paraphrase or
hallucinate unless explicitly told not to since it is generative in
nature based on the predicted probability of the next word from the text
and the instruction

\textbf{TODO}: The Prompt Report: A Systematic Survey of Prompt
Engineering Techniques \url{https://arxiv.org/pdf/2406.06608}

While decisions can be extracted manually from the literature, this
process is labor-intensive and time-consuming. Recent advances in Large
Language Models (LLMs) have demonstrated potential for automating the
extraction of structured information from unstructured text {[}ref{]}.
In this work, we use LLMs to automatically identify decisions made by
authors during their data analysis processes.

Text recognition from PDF document relies on Optical Character
Recognition (OCR) to convert scanned images into machine-readable text
-- capability currently offered by Antropic Claude and Google Gemini. We
instruct the LLM to generate a markdown file containing a JSON block
that records extracted decisions, which can then be read into
statistical software for further analysis. The exact prompt feed to the
LLM is provided in the Appendix. The \texttt{ellmer} package
\citep{ellmer} in R is used to connect to the Gemini and Claude API,
providing the PDF attachment and the prompt in a markdown file as
inputs.

\subsection{Review the LLM output}\label{review-the-llm-output}

\begin{itemize}
\tightlist
\item
  \textbf{TODO} something about result validation of LLM output: We also
  observe data quality with the extraction: for example in
  \citet{lee2006}, the variable recorded is ``smoothing parameter''.
  Authors are unclear about the delivery Specify how much of validation
  and review has been done.
\end{itemize}

The shiny app is designed to provide users a visual interface to review
and edit the decisions extracted by the LLM from the literature. The app
allows three actions from the users: 1) \emph{overwrite} -- modify the
content of a particular cell, equivalently
\texttt{dplyr::mutate(xxx\ =\ ifelse(CONDITION,\ "yyy"\ ,\ xxx))}, 2)
\emph{delete} -- remove a particular cell,
\texttt{dplyr::filter(!(CONDITION))}, and 3) \emph{add} -- manually
enter a decision, \texttt{dplyr::bind\_rows()}. Figure~\ref{fig-shiny}
illustrates the \emph{overwrite} action in the Shiny application, where
users interactively filter the data and preview the rows affected by
their edits---in this case, changing the model entry from ``generalized
additive Poisson time series regression'' to the less verbose ``Poisson
regression''. Upon confirmation, the corresponding \texttt{tidyverse}
code is generated, and users can download the edited table and
incorporate the code into their R script.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface for editting
Large Language Model (LLM)-generated decisions (overwrite, delete, and
add). (1) the default interface after loading the input CSV file. (2)
The table view will update interactively upon the user-defined filter
condition -- expressed using \texttt{dplyr::filter()} syntax (e.g.,
\texttt{paper\ ==\ anderson2008size"}), (3) The user edits the
\texttt{model} column to ``Poisson regression'' and applies the change
by clicking the Apply changes button. The table view updates to reflect
the changes (4) After clicking the Confirm button, the corresponding
\texttt{tidyverse} code is generated, and the table view returns to its
original unfiltered view. The edited data can be downloaded by clicking
the Download CSV button.}

\end{figure}%

\subsection{Calculating paper similarity}\label{sec-paper-similarity}

Once the decisions have been extracted and validated, this opens up a
structured data for analyzing these information. For example, we can
compare whether author's choices at different times changes, or across
decisions varies at different regions. In this section, we present a
method to calculate paper similarity based on the decisions shared in
the paper pairs. The goal is to construct a distance metric based on
similarity of the decision choice among papers that could be further
used for clustering paper based on choices made by different authors in
the literature. An overview of the method is illustrated in
Figure~\ref{fig-similarity-diag}.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/similarity-diag.png}

}

\caption{\label{fig-similarity-diag}Workflow for calculating paper
similarity based on decision choices: (1) standardize variable names,
(2) identify most frequent variable-type decisions across all papers,
(3) identify papers with at least x identified decisions, (4) calculate
decisions similarity score on the \emph{decision} and \emph{reason}
fields using transformer language models, e.g.~BERT, (5) calculate paper
similarity score based on aggregating decision similarity scores.}

\end{figure}%

\begin{itemize}
\tightlist
\item
  \textbf{TODO} some discussion on what it means by for two papers to be
  similar based on decisions.
\end{itemize}

The calculation of paper similarity is based on the similarity of
decisions shared by each paper pair. A decision comparable in two papers
are the ones that share the same variable and type, e.g.~temperature and
parameter (a decisions on the choosing the statistical method
\emph{parameter} for the \emph{temperature} variable), or humidity and
temporal (any \emph{temporal} treatment, e.g.~choice of lag value for
the \emph{humidity} variable). While many decisions share a similar
variable, different authors may refer to them with slightly different
names, such as ``mean temperature'' and ``average temperature'', hence
variable names are first standardized to a common set of variable names.
For example, ``mean temperature'' and ``average temperature'' are both
standardized to ``temperature''. Notice that ``dewpoint temperature'' is
standardized into ``humidity'' since it is a proxy of temperature to
achieve a relative humidity (RH) of 100\%. For literature with a common
theme, there is usually a set of variables that shared by most papers
and additional variables are justified in individual research. For our
air pollution mortality modelling literature, we standardize the
following variable names:

\begin{itemize}
\tightlist
\item
  \textbf{temperature}: ``mean temperature'', ``average temperature'',
  ``temperature'', ``air temperature'', ``ambient temperature''
\item
  \textbf{humidity}: ``dewpoint temperature'' and its hyphenated
  variants, relative humidity'', ``humidity''
\item
  \textbf{PM}: ``pollutant'', ``pollution'', ``particulate matter'',
  ``particulate'', ``PM10'', ``PM2.5''
\item
  \textbf{time}: ``date'', ``time'', ``trends'', ``trend''
\end{itemize}

Depending on the specific pairs, papers have varied number of decisions
that can be compared and aggregated. While paper similarities can be
computed for all paper pairs, using the similarity of one or two pair of
decisions to represent paper similarity is less ideal. Hence, before
calculating the text similarity of decisions, we also include two
optional steps to identify and subset the most frequent decisions across
papers, and to retain only papers that report more than a certain number
of frequent decisions. Research questions in different fields may have
different levels of homogeneity, depending on the maturity of the field
and for air pollution mortality modelling, it is helpful to focus on
decisions and papers that share a substantial number of decisions.

To assign numerical value for the similarity of reason, we use a
transformer language model, such as BERT, to measure the semantic text
similarity between the decision itself and its justification. The
decision similarity is calculated by comparing the \emph{decision} and
\emph{reason} fields of the decisions in each paper pair. To obtain
paper similarity, we average the decision similarities across all
decisions in each paper pair and other method can be customized for
aggregation. The resulting paper similarity score can be used as a
distance matrix to cluster papers based on their decision choices to
understand the common practices in the investigated literature.

\section{Results}\label{sec-result}

From the 56 studies examining the effect of particulate matters
(\(\text{PM}_{10}\) and \(\text{PM}_{2.5}\)) on mortality, we focus on
the baseline model reported in each paper, excluding secondary models
(e.g.~lag-distributed models) and sensitivity analysis. We also exclude
decisions on other pollutants, such as nitrogen dioxide
(\(\text{NO}_2\)). This yields 242 decisions extracted using Gemini,
averaging approximately 4 decisions per paper. Table~\ref{tbl-review}
summarizes the number of edits made during the review process using the
Shiny app. {[}details{]}

Table~\ref{tbl-missing-decisions} summarizes the missingness of the
decisions and reason. While most papers report their decision choices
(e.g.~use of five degree of freedom), 55\% of decisions lack a stated
rationale for the choice. Table~\ref{tbl-most-common-decisions} lists
teh eight most frequently reported decision: parameter and temporal
choice for time, PM, temperature, and humidity.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.9368}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.0632}}@{}}

\caption{\label{tbl-review}tsdjflkajsldf.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Count
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Irrelevant decisions, e.g.~other pollutants, sensitivity analysis &
50 \\
Recode for secondary LLM processing for standardization & 45 \\
Decision captured not correct & 11 \\
Duplicates & 9 \\
General statements without specific decision, e.g.~minimum of 1 df per
year was required & 6 \\
Definition of variables, e.g.~season & 5 \\
Total & 126 \\

\end{longtable}

\begin{table}

\caption{\label{tbl-missing-decisions}Missingness of decision and reason
fields in the Gemini-extracted decisions. Most decisions report the
choice (35.5 + 57.1 = 92\%), but 57.1\% lacks a stated reason.}

\centering{

\begin{tabular}{lll}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision} \\
\cmidrule(l{3pt}r{3pt}){2-3}
Reason & Non-missing & Missing\\
\midrule
Non-missing & 90 (37.2\%) & 14 (5.8\%)\\
Missing & 134 (55.4\%) & 4 (1.7\%)\\
\bottomrule
\end{tabular}

}

\end{table}%

\begin{longtable}[]{@{}llr@{}}

\caption{\label{tbl-most-common-decisions}Count of variable-type
decisions in the Gemini-extracted decisions. The most commonly reported
decision are the parameter choices and temporal lags for for time, PM,
temperature, and humidity.}

\tabularnewline

\toprule\noalign{}
Variable & Type & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
time & parameter & 44 \\
PM & temporal & 39 \\
temperature & parameter & 35 \\
humidity & parameter & 25 \\
temperature & temporal & 23 \\
humidity & temporal & 19 \\
PM & parameter & 9 \\
time & temporal & 3 \\

\end{longtable}

Table~\ref{tbl-humidity-temperature-decisions} reports the
parameter-related decisions captured in the literature. They refer to
the number of knots or degree of freedom for spline methods (natural and
smoothing spline) applied to variable time, humidity and temperature.
For consistency, all values have been converted to a \emph{per year}
scale. The selection of knot for natural spline has less variation than
the degree of freedom choices for smoothing spline. Choices for
temperature and humidity tend to be close, given they are both weather
related variables, while the choices for time are more varied
inherently. This tabulation offers a reference set for potential options
for future studies and help to identify anomalies and special treatment
in practice. Notable example includes the use of 7.7 degree of freedom
in \citet{castillejos2000}, and highly flexible choices of 30 and 100 in
\citet{moolgavkar2000} and \citet{moolgavkar2003}, respectively. While
most papers choice to report the smoothing parameter as a constant
value, \citet{schwartz2000} specifies it as a proportion of the data
(``5\% of the data'' and ``5\% of the data'').

For temporal decisions, after an initial review, we observed that
decisions are still highly varied. The decisions can be divided into two
groups: multi-day lags include expressions such as ``6-day average'',
``3-d moving average'', ``mean of lags 0+1'', and ``cumulative lags,
mean 0+1+2'', and single-day lags include ``lagged exposure up to 6
days'', ``lag days from 0 to 5'' among others. To standardize these
entries, we applied a secondary LLM process (claude-3-7-sonnet-latest)
and converted them into a consistent format:
\texttt{multi-day:\ lag\ {[}start{]}-{[}end{]}} and
\texttt{single-day:\ lag\ {[}start{]},\ …\ lag\ {[}end{]}}.
Table~\ref{tbl-temporal-decisions} summarizes the temporal lag choices
for PM, temperature, and humidity. Both single and multiple day lags are
generally considered up to five days prior (lag 5). {[}TODO: check
multi-day starts from one{]}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2179}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6282}}@{}}

\caption{\label{tbl-humidity-temperature-decisions}Options captured for
parameter choices for time, humidity, and temperature variables in the
Gemini-extracted decisions. The choices for natural spline knots are
generally less varied than the degree of freedom choices for smoothing
spline. Choices for temperature and humidity tend to be close, given
they are both weather related variables, while the choices for time are
more varied inherently.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
natural spline & humidity & 3, 4 \\
natural spline & temperature & 3, 4, 6 \\
natural spline & time & 1, 1.5, 3, 4, 6, 7, 8, 12, 15, 30, NA \\
smoothing spline & humidity & 2, 3, 4, 6, 8, 50 \\
smoothing spline & temperature & 2, 3, 4, 6, 8, 50 \\
smoothing spline & time & 1, 3, 4, 5, 6, 7, 7.7, 8, 9, 10, 12, 30, 100,
NA \\

\end{longtable}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-cluster-paper-1-1.pdf}}

}

\caption{\label{fig-cluster-paper-1}The dendrogram (left) and
multi-dimensional scaling (MDS) (right) based on paper similarity
distance for 62 air pollution mortality modelling literature. The papers
are colored by the most common smoothing method used. The MDS reveals
the three distinct groups of papers. This grouping corresponds to the
modelling strategies differ in the European and U.S. studies, documented
in ALPHENA.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-cluster-paper-1.pdf}}

}

\caption{\label{fig-cluster-paper}The dendrogram (left) and
multi-dimensional scaling (MDS) (right) based on paper similarity
distance for 62 air pollution mortality modelling literature. The papers
are colored by the most common smoothing method used. The MDS reveals
the three distinct groups of papers. This grouping corresponds to the
modelling strategies differ in the European and U.S. studies, documented
in ALPHENA.}

\end{figure}%

For computing the decision similarity score, we include the first 6 most
common variable-type decisions as suggested in
Table~\ref{tbl-most-common-decisions}. Figure~\ref{fig-cluster-paper}
shows the clustering of the 48 papers based on the decision similarity
scores. The dendrogram is generated using hierarchical clustering, and
the labels are colored according to the most common smoothing method
used in each paper. The clustering reveals three distinct groups of
papers, which reflect the modelling strategies differ in the European
(LOESS) and U.S. (\ldots) studies {[}more on the APHENA{]}.

\section{Sensitivity analysis}\label{sec-sensitivity}

In this section, we examine the reproducibility for using LLMs for text
extraction tasks in Section~\ref{sec-llm-reproducibility}, discrepancies
between different LLM models: Gemini (\texttt{gemini-2.0-flash}) and
Claude (\texttt{claude-3-7-sonnet-latest}) in
Section~\ref{sec-llm-models}, and the sensitivity of our paper
similarity calculation pipeline to the choice of text model used for
computing decision similarity scores in Section~\ref{sec-text-model}.

\subsection{LLM reproducibility}\label{sec-llm-reproducibility}

For our text extraction task, we test the reproducibility of Gemini
(\texttt{gemini-2.0-flash}) by repeating the text extraction task 5
times for each of the 62 papers. For each of the 31 papers, five runs
yield \(5 \times 4 /2 = 10\) pairwise comparisons per field and
including both the ``reason'' and ``decision'' fields results in a total
of \(31 \times 10 \times 2 = 620\) pairs. We exclude the pairs that have
different number of decisions since it would require manually align the
decision to compare and this left us with 449 out of 620 (72\%) pairwise
comparisons. Table~\ref{tbl-gemini-1} shows an example of such
comparison in \citet{andersen2008}, where all the four reasons are
identical among the two runs, hence a zero number of difference.

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-gemini-1}An example of comparing the text extraction
in decisions in Andersen 2008.}

\tabularnewline

\toprule\noalign{}
Variable & Run1 & Run2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NCtot & 6day average (lag 05) & 6day average (lag 05) \\
calendar time & 3 4 or 5 dfyear & 3 4 or 5 dfyear \\
dew-point temperature & 4 or 5 df & 4 or 5 df \\
temperature & 4 or 5 df & 4 or 5 df \\

\end{longtable}

Table~\ref{tbl-gemini-2} summarizes the number of differences observed
in each pairwise comparison. Among all comparisons, 80\% produce the
identical text in reason and decision. The discrepancies come from the
following reasons:

\begin{itemize}
\tightlist
\item
  Gemini extracted different length for the same decision, e.g.~in
  \citet{kan2007}, some runs may extract ``singleday lag models
  underestimate the cumulative effect of pollutants on mortality 2day
  moving average \textbf{of current and previous day concentrations}
  (lag=01)'', while others extract ``singleday lag models underestimate
  the cumulative effect of pollutants on mortality 2day moving average
  (lag=01)''. Similarity, for decisions, some runs may yield ``10 df for
  total mortality'', while other runs yield ``10 df''. Similar
  extraction appears in \citet{breitner2009}.
\item
  Gemini fails to extract reasons in some runs but not others, e.g.~in
  \citet{burnett1998}, the first run generates NAs in the reasons, but
  the remaining four runs are identical. In \citet{ueda2009} and
  \citet{castillejos2000} , runs 1 and 5 fail to extract the reasons and
  produce the same incomplete version, whereas runs 2, 3, and 4 produce
  accurate versions with reasons populated.
\end{itemize}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-gemini-2}Number of differences in the reason and
decision fields across Gemini runs for papers with consistent number of
decisions across runs.}

\tabularnewline

\toprule\noalign{}
Num. of difference & Count & Proportion (\%) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 358 & 79.73 \\
1 & 12 & 2.67 \\
2 & 8 & 1.78 \\
3 & 0 & 0.00 \\
4 & 24 & 5.35 \\
5 & 12 & 2.67 \\
6 & 3 & 0.67 \\
7 & 0 & 0.00 \\
8 & 10 & 2.23 \\
9 & 6 & 1.34 \\
10 & 10 & 2.23 \\
11 & 6 & 1.34 \\
Total & 449 & 100.00 \\

\end{longtable}

\subsection{LLM models}\label{sec-llm-models}

Reading text from PDF document requires Optical Character Recognition
(OCR) to convert images into machine-readable text, which currently is
only supported by Antropic Claude (\texttt{claude-3-7-sonnet-latest})
and Google Gemini (\texttt{gemini-2.0-flash}).

We compare the number of decisions extracted by Claude and Gemini across
all 62 papers in \textbf{?@fig-claude-gemini}. Each point represents a
paper, with the x- and y-axes showing the number of decisions extracted
by Claude and Gemini, respectively. The dashed 1:1 line marks where both
models extract the same number of decisions. Most points fall below this
line, indicating that Claude extracts more decisions -- often from data
pre-processing or secondary data analysis steps requiring more manual
validation -- whereas Gemini focuses more on modelling choices relevant
to our analysis. Some of these decisions captured by Claude are

\begin{itemize}
\tightlist
\item
  the definition of ``cold day'' and ``hot day'' indicators in
  \citet{dockery1992} (``defined at the 5th/ 95th percentile''),
\item
  the choice to summarize \(\text{NO}_2\), \(\text{O}_3\), and
  \(\text{SO}_2\) using a ``24 hr average on variable'' in
  \citet{huang2009}, and
\item
  the definition of black smoke and in \citet{katsouyanni2001} for
  secondary analysis (``restrict to days with BS concentrations below
  150 \(\mu g/m^2\)'').
\end{itemize}

Gemini sometimes also include irrelevant decisions, such as in
\citet{mar2000}, where secondary analysis choices like ``0-4 lag days''
for air pollution exposure variables (CO, EC, \(\text{K}_S\),
\(\text{NO}_2\), \(\text{O}_3\), OC, Pb, S, \(\text{SO}_2\), TC, Zn) are
captured. However, these cases are less frequent, resulting in outputs
with less noise overall.

For both Claude and Gemini, we find they fail to link the general term
``weather variables'' to the specific weather variables. For example
Gemini misses this link in \citet{dockery1992} and \citet{burnett2004},
while Claude does so in \citet{dockery1992} and \citet{katsouyanni2001}.
Although our prompt specified that some decisions may require linking
information across sentences and paragraphs to identify the correct
variable, this instruction doesn't appear to be applied consistently.

\subsection{Text model}\label{sec-text-model}

We have conducted sensitivity analysis on the text model for obtaining
the decision similarity score from the Gemini outputs. The tested
language models tested include

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  BERT by Google \citep{devlin2019},
\item
  RoBERTa by Facebook AI \citep{liu}, trained on a larger dataset (160GB
  v.s. BERT's 15GB),
\item
  XLNnet by Google Brain \citep{yang}, and
\end{enumerate}

two domain-trained BERT models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  sciBERT \citep{beltagy2019}, trained on scientific literature, and
\item
  bioBERT \citep{lee2020}, trained on PubMed and PMC data.
\end{enumerate}

Figure~\ref{fig-text-density} presents the distribution of the decision
similarity (left) and paper similarity (right) for each text model. At
decision level, the BERT model produces the widest variation across all
five models, while the similarity scores from XLNet are all close to 1.
These scores are not comparable across models since the difference of
the underlying transformer architecture. However, the paper similarity
scores from each model are comparable and Figure~\ref{fig-text-mds}
shows the multi-dimensional scaling (MDS) of the paper similarity scores
from each text model: all showing a similar clustering pattern of the
three main smoothing methods.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-density-1.pdf}}

}

\caption{\label{fig-text-density}Distribution of decision similarity
(left) and paper similarity (right) scores for five different text
models (BERT, BioBERT, RoBERTa, SciBERT, and XLNet). The default
language model, BERT, produces the widest variation across the five
models, while the similarity scores form XLNet are all close to 1. The
model BioBERT, RoBERTa, and SciBERT yield decision similar scores mostly
between 0.7 to 1.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-mds-1.pdf}}

}

\caption{\label{fig-text-mds}The multi-dimensional scaling (MDS) of the
paper similarity scores from each text model: all showing a similar
clustering pattern of the three main smoothing methods. The points are
colored by the most common method used in the paper, and the hulls are
drawn around each method group.}

\end{figure}%

\section{Discussion}\label{sec-discussion}

There are other decisions in an analysis that are worth comparing and
documenting. For example data pre-processing decisions, e.g.~how
pollutant series are defined and collected, treatment on missing values,
etc. Again, for a complete review of the field, these decisions ideally
would be included, but for our demonstration of idea, we focus on the
modelling decisions. Spatial decisions are generally not well captured
because it often conducted uniformly as estimating the city individually
to accommodate city heterogeneity. Some papers only consider a handful
of cities, while in larger studies the individual city effects are then
pooled together using random effect.

The variation in the choice of parameters degree of freedom or knot for
smoothing can motivate separate investigation on the sensitivity
analysis. For instance, parameters that exhibit a wide range of choices
across studies may indicate areas of uncertainty or debate within the
field, suggesting that further investigation is needed to assess their
impact on study outcomes \citep{peng2006, touloumi2006}.

With LLMs, the extraction of decisions from literature could be largely
automated, but manual review is still needed to ensure the quality of
the extracted decisions. We also find secondary LLMs can be used to
standardize the extracted decisions, such as for temporal lag choices
from text expressing this decision in various ways. In this work, we use
prompt engineering to optimize the prompt for extracting decisions from
general LLMs (Claude and Gemini). Fine-tuning a local model is an
alternative approach for a locally-trained model. While it could
potentially yield more accurate extraction and hence less manual review,
for a systematic literature review, it would require substantially more
training efforts and a labelled decision dataset. We also find sometimes
the prompt is not fully followed throughout the extraction (example).
Claude and Gemini\ldots{}

Currently, only one model per paper - some have comparison of GLM and
GAM, compare different pollutants, stratify by \ldots.

With the advocacy for reproducibility in science, it is expected that
more papers will share their code and data. The availability of the code
could be a supplementary source for understanding the decisions made in
the analysis and cross comparison of the manuscript with the code.
However, given the lack of comments in the current practice, we are not
there to extract reasons for the decisions encoded in the script.

\section{Conclusion}\label{sec-conclusion}

In this paper, {[}we study how decisions are made in practical data
analysis{]}. We developed a pipeline for automatically extracting
decisions using LLMs (Claude and Gemini) and introduced a method for
calculating paper similarity through decision similarity. This enables
us to cluster papers by their decision choices and visualization through
hierarchical clustering and multidimensional scaling. We applied this
pipeline to mortality/ hospital admission -- PM modelling literature. We
extracted key modelling decisions, such as the choice of smoothing
methods and parameters for time, temperature, and humidity, and revealed
paper clusters that correspond to different modelling strategies, as
documented in the APHENA project.

While sensitivity analyses are commonly used to assess the robustness of
findings to different analytical choices, the set of choices tested is
often limited and selected subjectively by the authors. Our approach
offers a new perspective by pooling decisions made in analyses across
studies in the fields. This allows for a holistic account on the
alternatives in the field and identification of both consensus and
divergence within the field, providing insights for future research and
methodological development.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%% begin pandoc before-bib
%% end pandoc before-bib
%% begin pandoc biblio
%% end pandoc biblio
%% begin pandoc include-after
%% end pandoc include-after
%% begin pandoc after-body
%% end pandoc after-body

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
