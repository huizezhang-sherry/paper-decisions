\documentclass[manuscript,screen,review,anonymous]{acmart}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}

%% PANDOC PREAMBLE BEGINS

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
%% PANDOC PREAMBLE ENDS

\setlength{\parindent}{10pt}
\setlength{\parskip}{0pt}

\hypersetup{
  pdftitle={Dossier: visualizing/ understanding decision choices in data analysis via decision similarity},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc, via quarto}}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI'26]{CHI Conference on Human Factors in Computing
Systems}{Apr 13--17, 2026}{Barcelona, Spain}
\acmPrice{}
\acmISBN{978-1-4503-XXXX-X/18/06}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Dossier: visualizing/ understanding decision choices in data
analysis via decision similarity}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


  \author{H. Sherry Zhang}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
        \author{Roger D. Peng}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
      

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}
%%  
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Decision choices made during data analysis, along with the reasons
motivating them, are center to how results are interpreted and to
comparisons across similar studies. However, such decisions -- such as
selecting the degree of freedom for a smoothing spline and the
rationalebehind them -- are rarely studied, since it is impractical to
interview authors for all the alternatives and their motivations or to
rerun the analysis under different options. In this work, we propose a
workflow to automatically extract analytic decisions from the published
literature and organize them into structured data using Large Language
Models (Claude and Gemini). The pipeline then calculates paper
similarity based on the semantic similarity of these extracted decisions
and their reasons, and visualizes the results using clustering
algorithms. We apply this workflow to a set of studies on the effect of
particulate matter on mortality and hospital admission, conducted by
researchers worldwide, which naturally provide alteranative analyses of
the same question. Our approach offers an efficient way to study
decision-making practices and robustness in data analysis compared with
traditional interviews or author-focused sensitivity or multiverse
analyses.    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
    <concept_id>10010405.10010497.10010504.10010505</concept_id>
    <concept_desc>Applied computing~Document analysis</concept_desc>
    <concept_significance>300</concept_significance>
    </concept>
 <concept>
    <concept_id>10003120.10003121.10011748</concept_id>
    <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
    <concept_significance>500</concept_significance>
    </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[300]{Applied computing~Document analysis}
\ccsdesc[500]{Human-centered computing~Empirical studies in HCI}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large language models}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\setlength{\parskip}{-0.1pt}

\section{Introduction}\label{introduction}

Decisions are made at every stage of data analysis: from initial data
collection and pre-processing to modelling choices. Different decision
choices can have a direct impact to the final results, which can lead to
different interpretation and policy recommendations that follow. When
independent analysts analyzing the same dataset even to answer the same
research questions, through many-analysts experiments, they often arrive
at markedly different conclusions
\citep{silberzahn2018, botvinik-nezer2020, gould2025}. This variability
in results can be attributed to the flexibility analysts have in making
decisions throughout the data analysis process, which \citet{gelman2014}
describe as the ``garden of forking paths''. When such flexibility is
misused, data analysis can lead to p-hacking, selective reporting,
inflated effect sizes, and other issues, undermining the quality and
credibility of the findings.

{[}this is not okay --- Multiple recommendations have been proposed to
improve data analysis practices, such as pre-registration and multiverse
analysis. Bayesian methods also offer a different paradigm to p-value
driven inference for interpreting statistical evidence. Most empirical
studies of data analysis practices focus on specially designed and
simplified analysis scenarios. While informative, these setups may not
adequately capture the complexity of the data analysis with significant
policy implications. {[}In practice, studying the data analysis
decisions with actual applications is challenging.{]} Analysts may no
longer be available for interviews due to job changes, and even when
they are, recalling the full set of decisions and thinking process made
during the analysis is often infeasible. Moreover, only until the last
decades, analysis scripts and reproducible materials were not commonly
required by journals for publishing. --- up till here{]}

In this work, we develop a tabular format to record analytical decisions
in data analysis and automate the extraction of these decisions from
published papers using large language models (Gemini and Claude). The
workflow also include a component to calculate paper similarity based on
both the decisions and the semantic similarity of their rationales, and
use clustering methods to visualize papers according to distance based
on decision similarity. We apply this workflow to a set of 56 air
pollution modelling studies estimating the effect size of particulate
matter (PM2.5 or PM10) on mortality and hospital admissions, typically
modeled using Poisson generalised linear models (GLMs) or generalized
additive models (GAMs). Analysis of the extracted decisions reveals
common choices in this type of analysis (number of knots or degree of
freedom for smoothing methods for time, temperature and humidity) and
find three distinct clusters corresponding to different smoothing
methods (LOESS, natural spline, and smoothing spline) used in European
and U.S. studies, consistent with findings from the APHENA project.

In summary, the contribution of this work includes:

\begin{itemize}
\item
  A new approach to study data analysis decision choices through
  automatic extraction of decisions from scientific literature using
  LLMs,
\item
  A dataset of decisions and rationale, along with metadata, compiled
  from 62 studies in air pollution mortality modelling, and
\item
  A method to construct paper similarities based on the decisions and
  the semantic similarity of their rationale.
\end{itemize}

\section{Related work}\label{sec-background}

\subsection{Decision-making in data
analysis}\label{decision-making-in-data-analysis}

Data analysis involves making choices at every step, from initial data
collection, data pre-processing to model specification, and
post-processing. Each decision represents a branching point where
analysts choose a specific path to follow, and the vast number of
possible choices analysts can take forms what \citet{gelman2014}
describe as the ``garden of forking paths''. While researchers may hope
their inferential results are robust to the specific path taken through
the garden, in practice, different choices can lead to substantially
different conclusions. This has been empirically demonstrated through
``many analyst experiments'', where independent research groups analyze
the same dataset to address the same research questions with their own
chosen analytic approach. A classic example is \citet{silberzahn2018},
where researchers reported an odds ratio from 0.89 to 2.93 for the
effect of soccer players' skin tone on the number of red cards awarded
by referees. Similar variability has been observed in structural
equation modeling \citep{sarstedt2024}, applied microeconomics
\citep{huntington-klein2021}, neuroimaging \citep{botvinik-nezer2020},
and ecology and evolutionary biology \citep{gould2025}.

Examples like above have rendered decision-making in data analysis as a
subject to study in human computer interaction. To understand how
analysts making decisions during data analysis and navigating the garden
of forking path, researchers have conducted qualitative interviews with
analysts on data analysis practices
\citep{kale2019, alspaugh2019, liu_understanding_2020}. Visualization
tools have also been explored to communicate the decision process
through analytic decision graphics (ADG) \citep{liu2020}. In fairness
machine learning literature, \citet{simson2025} contributed a reusable
workflow that supports participatory input to democratize decisions in
machine learning algorithms related to fairness, privacy,
interpretability and performance. Conducting qualitative studies through
interviews to study how assumptions and decisions are made in data
analysis practices takes a significant amount of time and effort, and
the findings may not generalize to other contexts. While published
research papers may not provide a complete picture of the
decision-making process, they do contain valuable information about the
choices made by analysts and the rationale behind them. With recent
advances in Large Language Models (LLMs), it has become possible to
automatically extract structured information from unstructured text.
This could provide a scalable way to study decision-making practices in
data analysis.

On top of qualitative studies, software tools have also developed to
incorporate potential alternatives in the analysis workflow. The
\texttt{DeclareDesign} package \citep{blair2019} introduces the MIDA
framework for researchers to declare, diagnose, and redesign their
analyses to produce a distribution of the statistic of interest, which
has been applied in the randomized controlled trial study
\citep{bishop2024}. The \texttt{multiverse} package
\citep{multiverse, liu2021} provides a framework for researchers to
conduct multiverse analysis to systematically explore how different
choices affect results and to report the range of plausible outcomes
that arise from alternative analytic paths.

\subsection{Visualization on scientific
literature}\label{visualization-on-scientific-literature}

With the growing volume of scientific publications and the difficulty of
navigating the literature to stay informed, there is increasing interest
in developing tools to visualize and recommend scientific papers. These
systems link papers based on their similarity and relevance, typically
determined by keywords \citep{isenberg2017}, citation information
(e.g.~citation list, co-citation) \citep{chen2006}, or combinations with
other relevant paper metadata (e.g.~author, title)
\citep{bethard2010, chou2011, dörk2012, heimerl2016}. Recent approaches
incorporate text-based information using topic modelling
\citep{alexander2014}, argumentation-based information retrieval
\citep{tbahriti2006}, and text embedding \citep{narechania2022}. While
metadata and high-level text-based information are useful for finding
relevant papers, researchers also need tools that help them \emph{make
sense} of the literature rather than simply \emph{locating} it. In
applied data analysis, one interest is to understand how studies differ
or align in their analytical approaches. Capturing the decisions and
reasoning expressed in analyses on a shared theme enables the
calculation of similarity metrics based on these choice and their
underlying rationale, which supports clustering and visualizing paper to
identify common practices in the field.

\section{Methods}\label{sec-extract-decisions}

TODO: a generic summary of the workflow, maybe an illustration

\subsection{Record decisions in data analysis}\label{sec-decisions}

Consider the following excerpt from \citet{ostro2006} that describes the
modelling approach to provide evidence of an association between daily
counts of mortality and ambient particulate matter (PM10):

\begin{quote}
Based on previous findings reported in the literature (e.g., Samet et
al.~2000), the basic model included a smoothing spline for time with 7
degrees of freedom (df) per year of data. This number of degrees of
freedom controls well for seasonal patterns in mortality and reduces and
often eliminates autocorrelation.
\end{quote}

This sentence encode the following components of a decision:

\begin{itemize}
\tightlist
\item
  \textbf{variable}: time
\item
  \textbf{method}: smoothing spline
\item
  \textbf{parameter}: degree of freedom (df)
\item
  \textbf{reason}: Based on previous findings reported in the literature
  (e.g., Samet et al.~2000); This number of degrees of freedom controls
  well for seasonal patterns in mortality and reduces and often
  eliminates autocorrelation.
\item
  \textbf{decision}: 7 degrees of freedom (df) per year of data
\end{itemize}

To record these decisions in a tabular format, we follow the tidy data
principle \citep{wickham2014}, which states each variable should be in a
column and each observation in a row. For our purpose, each row
represents a decision made by the authors in a paper and an analysis
often include multiple decisions. To retain the original context of the
decision, we extract the original text in the paper, without paraphrase
or summarization. The decision choice above is a parameter choice of a
statistical method applied to the variable. Analyses also include other
types of decisions, such as temporal and spatial treatments, for
example, the choice of lagged exposure for certain variables or whether
the model is estimated collectively or separated for individual
locations. These decisions don't have a specific method or parameter,
but should still be recorded with the variable, type (spatial or
temporal), reason, and decision fields.

Given the writing style and the quality of the analysis itself, multiple
decisions may be combined in one sentence and certain fields,
e.g.~decision and reason, may be omitted. Consider the following excerpt
from \citet{ostro2006}:

\begin{quote}
Other covariates, such as day of the week and smoothing splines of 1-day
lags of average temperature and humidity (each with 3 df), were also
included in the model because they may be associated with daily
mortality and are likely to vary over time in concert with air pollution
levels.
\end{quote}

This sentence contains four decisions: two for temperature (the temporal
lag and the smoothing spline parameter) and two for humidity and should
be structured as separate entries:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.0508}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.0339}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1525}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1525}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1525}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1525}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1525}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1525}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ostro & 1 & temperature & smoothing spline & degree of freedom &
parameter & 3 degree of freedom & NA \\
ostro & 2 & relative humidity & smoothing spline & degree of freedom &
parameter & 3 degree of freedom & NA \\
ostro & 3 & temperature & NA & NA & temporal & 1-day lags & NA \\
ostro & 4 & relative humidity & NA & NA & temporal & 1-day lags & NA \\
\end{longtable}

Notice in the example above, the reason field are recorded as NA. This
is because the stated rationale (``and are likely to vary over time in
concert with air pollution levels'') only supports the general inclusion
of temporal lags but does not justify the specific choice of 1-day lag
over other alternatives, for example, 2-day average of lags 0 and 1 and
single-day lag of 2 days. Similar scenario can happen when a direct
decision is missing while a reason is provided (``done by minimizing
Akaike's information criterion''), as in \citet{katsouyanni2001}:

\begin{quote}
The inclusion of lagged weather variables and the choice of smoothing
parameters for all of the weather variables were done by minimizing
Akaike's information criterion.
\end{quote}

\subsection{Extract decisions automatically from literature with
LLMs}\label{extract-decisions-automatically-from-literature-with-llms}

Manually extracting decisions from published papers is labor-intensive
and time-consuming. With Large Language Models (LLMs), it has become
possible to automatically extract structured information from
unstructured text by supplying a set of PDF documents and a prompt for
instruction. Text recognition from PDF document relies on Optical
Character Recognition (OCR) to convert scanned images into
machine-readable text -- capability currently offered by Antropic Claude
and Google Gemini. In the prompt, we assign the LLM a role as an applied
statistician and instruct it to generate a markdown file containing a
JSON block that extract decisions from the PDF in the format described
in Section~\ref{sec-decisions}. We also provide a set of instructions
and examples on the potential missing of reason and decision fields.
Prompt engineering techniques \citep{chen2025, xu} are used to optimize
the prompt script. The full prompt feed to the LLM is provided in the
Appendix. We use the \texttt{chat\_PROVIDER()} functions from the
\texttt{ellmer} package \citep{ellmer} in R to obtain the output with
Gemini and Claude API.

\subsection{Validate and standardize LLM
outputs}\label{validate-and-standardize-llm-outputs}

The LLM outputs need to be validated and standardized before further
analysis. Validation focuses on ensuring the correctness of the
extracted decisions by LLMs, while standardization aims to ensure
consistency in variable and model names across papers, given authors may
express the same concept in different ways. For example, ``mean
temperature'', ``average temperature'', and ``temperature'' all refer to
the same variable, which can be all standardized to ``temperature'' for
consistency. To help with the validation and standardization process, we
developed a Shiny application that provides an interactive interface for
users to review and edit the LLM outputs. A Shiny application takes a
CSV of extracted decisions as input and allows three types of edits: 1)
\emph{overwrite} -- modify the content of a particular cell, 2)
\emph{delete} -- remove a particular irrelevant decision, and 3)
\emph{add} -- manually enter a missing decision. Figure~\ref{fig-shiny}
illustrates the \emph{overwrite} action for standardizing the variable
NCtot (The number concentration of urban background particles
\textless100 nm in diameter) to ``pollution'': the user enters a
predicate function in the filter condition box on the left panel, and
the filtered data will appear interactively in the right panel. The user
can then specify the variable to overwrite and the new value and the
corresponding cells in the right panel will be updated. This change need
to be confirmed by pressing the ``Apply changes'' button to update the
full dataset. The corresponding \texttt{tidyverse} \citep{tidyverse}
code will then be generated in the left panel to be included in an R
script, and the edited table can be downloaded for future analysis.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=0.8\textheight]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface to validate
and standardize Large Language Model (LLM)-generated output. (1) the
default interface after loading the input CSV file. (2) The table view
will update interactively to reflect the edit: for paper with handle
``andersen2008size'' and id in 4, 5, 6, replace the variable NCtot with
``pollutant''. (3) After clicking the Confirm button, the corresponding
\texttt{tidyverse} code is generated, and the table view returns to its
original unfiltered view with the edits applied. The edited data can be
downloaded by clicking the Download CSV button.}

\end{figure}%

\subsection{Calculate paper similarity and
visualization}\label{sec-paper-similarity}

Once the output has been extracted and validated, the decisions can be
treated as data for further analysis. In this section, we construct a
distance metric between pairs of papers based on the similarity of their
decision choices. This metric can then be used as a distance matrix
among papers for clustering, dimension reduction, and visualization.

For each paper pair, a decision is considered comparable if the papers
share the same variable and decision type, for example, a parameter
decision on temperature or the temporal decision on humidity. For two
decisions to be considered similar, both the decision choice and the
rationale are taken into account. A similar choice indicates a similar
final decisions are made in the analysis, whereas a similar reason
reflects a shared rationale or justification for the choice, even when
the choices themselves differ, potentially due to differences in the
underlying data. To assign numerical value for measuring the similarity,
we use the semantic similarity from text model (default to BERT). For
parameter type decisions, the statistical method used also contributes
to the similarity of the decision. Since semantic similarity cannot
fully capture the difference between statistical methods (the difference
between smoothing spline and natural spline is not well represented by
the textual difference of ``smoothing'' and ``natural''), method
similarity is encoded as binary: 1 if the two papers used the same
method, and 0 otherwise. The paper similarity is then computed as the
average similarity across all the matched methods, decisions, and
reasons. The resulting paper similarity metric can be interpreted as a
distance measure to cluster and visualize papers based on their decision
choices.

Because analyses vary in the decisions they report, the number of
matched decisions differs across paper pairs. In practice, some studies
may not fully report the decision and reason for every choice made,
leading to missing data for the matched decisions. Although paper
similarity can be calculated based on all available matched decisions,
cares should be taken for pairs with only a small number of matches, as
the paper similarity may be overly influenced by one or two decisions.
To address this, users may focus on a set of decisions shared across
papers and on papers that report a minimal number of these decisions
when calculating paper similarity.

\section{Results}\label{sec-result}

This class of studies has significant impact to provide scientific
evidence for to guide public policy on setting the National Ambient Air
Quality Standards (NAAQS) for air pollutants in the U.S. While
individual modelling choices vary, these studies often share a common
structure: they adjust for meteorological covariates such as temperature
and humidity, apply temporal or spatial treatments, like including
lagged variables and may estimate the effect by city or region before
combining results. Because these studies investigate similar scientific
questions using a shared modelling framework, they form a natural
many-analyst setting. This allows us to examine, in a real-world
context, the range of analytical decisions made by different researchers
addressing the same underlying question.

While decisions occur throughout the entire data analysis process --
from the selection of variables and data source, to pre-processing steps
to prepare the data for modelling, to the model specification and
variable inclusion. From the 56 studies examining the effect of
particulate matters (\(\text{PM}_{10}\) and \(\text{PM}_{2.5}\)) on
mortality, we focus on the baseline model reported in each paper,
excluding secondary models (e.g.~lag-distributed models) and sensitivity
analysis. We also exclude decisions on other pollutants, such as
nitrogen dioxide (\(\text{NO}_2\)). This yields 242 decisions extracted
using Gemini, averaging approximately 4 decisions per paper.
Table~\ref{tbl-review} summarizes the number of edits made during the
review process using the Shiny app. {[}details{]}

\begin{itemize}
\tightlist
\item
  \textbf{TODO} something about result validation of LLM output: We also
  observe data quality with the extraction: for example in
  \citet{lee2006}, the variable recorded is ``smoothing parameter''.
  Authors are unclear about the delivery Specify how much of validation
  and review has been done.
\end{itemize}

While many decisions share a similar variable, different authors may
refer to them with slightly different names (e.g.~``mean temperature''
vs.~``average temperature''). For our air pollution mortality modelling
literature, we standardize the following variable names:

\begin{itemize}
\tightlist
\item
  \textbf{temperature}: ``mean temperature'', ``average temperature'',
  ``temperature'', ``air temperature'', ``ambient temperature''
\item
  \textbf{humidity}: ``dewpoint temperature'' and its hyphenated
  variants, relative humidity'', ``humidity''
\item
  \textbf{PM}: ``pollutant'', ``pollution'', ``particulate matter'',
  ``particulate'', ``PM10'', ``PM2.5''
\item
  \textbf{time}: ``date'', ``time'', ``trends'', ``trend''
\end{itemize}

Notice that ``dewpoint temperature'' is standardized into ``humidity''
since it is a proxy of temperature to achieve a relative humidity (RH)
of 100\%.

Table~\ref{tbl-missing-decisions} summarizes the missingness of the
decisions and reason. While most papers report their decision choices
(e.g.~use of five degree of freedom), 55\% of decisions lack a stated
rationale for the choice. Table~\ref{tbl-most-common-decisions} lists
teh eight most frequently reported decision: parameter and temporal
choice for time, PM, temperature, and humidity.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.9368}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.0632}}@{}}

\caption{\label{tbl-review}tsdjflkajsldf.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Count
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Irrelevant decisions, e.g.~other pollutants, sensitivity analysis &
50 \\
Recode for secondary LLM processing for standardization & 45 \\
Decision captured not correct & 11 \\
Duplicates & 9 \\
General statements without specific decision, e.g.~minimum of 1 df per
year was required & 6 \\
Definition of variables, e.g.~season & 5 \\
Total & 126 \\

\end{longtable}

\begin{table}

\caption{\label{tbl-missing-decisions}Missingness of decision and reason
fields in the Gemini-extracted decisions. Most decisions report the
choice (35.5 + 57.1 = 92\%), but 57.1\% lacks a stated reason.}

\centering{

\begin{tabular}{lll}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision} \\
\cmidrule(l{3pt}r{3pt}){2-3}
Reason & Non-missing & Missing\\
\midrule
Non-missing & 90 (37.2\%) & 14 (5.8\%)\\
Missing & 134 (55.4\%) & 4 (1.7\%)\\
\bottomrule
\end{tabular}

}

\end{table}%

\begin{longtable}[]{@{}llr@{}}

\caption{\label{tbl-most-common-decisions}Count of variable-type
decisions in the Gemini-extracted decisions. The most commonly reported
decision are the parameter choices and temporal lags for for time, PM,
temperature, and humidity.}

\tabularnewline

\toprule\noalign{}
Variable & Type & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
time & parameter & 44 \\
PM & temporal & 39 \\
temperature & parameter & 35 \\
humidity & parameter & 25 \\
temperature & temporal & 23 \\
humidity & temporal & 19 \\
PM & parameter & 9 \\
time & temporal & 3 \\

\end{longtable}

Table~\ref{tbl-humidity-temperature-decisions} reports the
parameter-related decisions captured in the literature. They refer to
the number of knots or degree of freedom for spline methods (natural and
smoothing spline) applied to variable time, humidity and temperature.
For consistency, all values have been converted to a \emph{per year}
scale. The selection of knot for natural spline has less variation than
the degree of freedom choices for smoothing spline. Choices for
temperature and humidity tend to be close, given they are both weather
related variables, while the choices for time are more varied
inherently. This tabulation offers a reference set for potential options
for future studies and help to identify anomalies and special treatment
in practice. Notable example includes the use of 7.7 degree of freedom
in \citet{castillejos2000}, and highly flexible choices of 30 and 100 in
\citet{moolgavkar2000} and \citet{moolgavkar2003}, respectively. While
most papers choice to report the smoothing parameter as a constant
value, \citet{schwartz2000} specifies it as a proportion of the data
(``5\% of the data'' and ``5\% of the data'').

For temporal decisions, after an initial review, we observed that
decisions are still highly varied. The decisions can be divided into two
groups: multi-day lags include expressions such as ``6-day average'',
``3-d moving average'', ``mean of lags 0+1'', and ``cumulative lags,
mean 0+1+2'', and single-day lags include ``lagged exposure up to 6
days'', ``lag days from 0 to 5'' among others. To standardize these
entries, we applied a secondary LLM process (claude-3-7-sonnet-latest)
and converted them into a consistent format:
\texttt{multi-day:\ lag\ {[}start{]}-{[}end{]}} and
\texttt{single-day:\ lag\ {[}start{]},\ …\ lag\ {[}end{]}}.
Table~\ref{tbl-temporal-decisions} summarizes the temporal lag choices
for PM, temperature, and humidity. Both single and multiple day lags are
generally considered up to five days prior (lag 5). {[}TODO: check
multi-day starts from one{]}.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2179}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1538}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6282}}@{}}

\caption{\label{tbl-humidity-temperature-decisions}Options captured for
parameter choices for time, humidity, and temperature variables in the
Gemini-extracted decisions. The choices for natural spline knots are
generally less varied than the degree of freedom choices for smoothing
spline. Choices for temperature and humidity tend to be close, given
they are both weather related variables, while the choices for time are
more varied inherently.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
natural spline & humidity & 3, 4 \\
natural spline & temperature & 3, 4, 6 \\
natural spline & time & 1, 1.5, 3, 4, 6, 7, 8, 12, 15, 30, NA \\
smoothing spline & humidity & 2, 3, 4, 6, 8, 50 \\
smoothing spline & temperature & 2, 3, 4, 6, 8, 50 \\
smoothing spline & time & 1, 3, 4, 5, 6, 7, 7.7, 8, 9, 10, 12, 30, 100,
NA \\

\end{longtable}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-cluster-paper-1-1.pdf}}

}

\caption{\label{fig-cluster-paper-1}The dendrogram (left) and
multi-dimensional scaling (MDS) (right) based on paper similarity
distance for 62 air pollution mortality modelling literature. The papers
are colored by the most common smoothing method used. The MDS reveals
the three distinct groups of papers. This grouping corresponds to the
modelling strategies differ in the European and U.S. studies, documented
in ALPHENA.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-cluster-paper-1.pdf}}

}

\caption{\label{fig-cluster-paper}The dendrogram (left) and
multi-dimensional scaling (MDS) (right) based on paper similarity
distance for 62 air pollution mortality modelling literature. The papers
are colored by the most common smoothing method used. The MDS reveals
the three distinct groups of papers. This grouping corresponds to the
modelling strategies differ in the European and U.S. studies, documented
in ALPHENA.}

\end{figure}%

For computing the decision similarity score, we include the first 6 most
common variable-type decisions as suggested in
Table~\ref{tbl-most-common-decisions}. Figure~\ref{fig-cluster-paper}
shows the clustering of the 48 papers based on the decision similarity
scores. The dendrogram is generated using hierarchical clustering, and
the labels are colored according to the most common smoothing method
used in each paper. The clustering reveals three distinct groups of
papers, which reflect the modelling strategies differ in the European
(LOESS) and U.S. (\ldots) studies {[}more on the APHENA{]}.

\section{Discussion}\label{sec-discussion}

Prompt engineering: these models may paraphrase or hallucinate unless
explicitly told not to since it is generative in nature based on the
predicted probability of the next word from the text and the
instruction.

In this section, we examine the reproducibility for using LLMs for text
extraction tasks in Section~\ref{sec-llm-reproducibility}, discrepancies
between different LLM models: Gemini (\texttt{gemini-2.0-flash}) and
Claude (\texttt{claude-3-7-sonnet-latest}) in
Section~\ref{sec-llm-models}, and the sensitivity of our paper
similarity calculation pipeline to the choice of text model used for
computing decision similarity scores in Section~\ref{sec-text-model}.

\subsection{LLM reproducibility}\label{sec-llm-reproducibility}

For our text extraction task, we test the reproducibility of Gemini
(\texttt{gemini-2.0-flash}) by repeating the text extraction task 5
times for each of the 56 papers. For each of the 31 papers, five runs
yield \(5 \times 4 /2 = 10\) pairwise comparisons per field and
including both the ``reason'' and ``decision'' fields results in a total
of \(31 \times 10 \times 2 = 620\) pairs. We exclude the pairs that have
different number of decisions since it would require manually align the
decision to compare and this left us with 449 out of 620 (72\%) pairwise
comparisons. Table~\ref{tbl-gemini-1} shows an example of such
comparison in \citet{andersen2008}, where all the four reasons are
identical among the two runs, hence a zero number of difference.

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-gemini-1}An example of comparing the text extraction
in decisions in Andersen 2008.}

\tabularnewline

\toprule\noalign{}
Variable & Run1 & Run2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NCtot & 6day average (lag 05) & 6day average (lag 05) \\
calendar time & 3 4 or 5 dfyear & 3 4 or 5 dfyear \\
dew-point temperature & 4 or 5 df & 4 or 5 df \\
temperature & 4 or 5 df & 4 or 5 df \\

\end{longtable}

Table~\ref{tbl-gemini-2} summarizes the number of differences observed
in each pairwise comparison. Among all comparisons, 80\% produce the
identical text in reason and decision. The discrepancies come from the
following reasons:

\begin{itemize}
\tightlist
\item
  Gemini extracted different length for the same decision, e.g.~in
  \citet{kan2007}, some runs may extract ``singleday lag models
  underestimate the cumulative effect of pollutants on mortality 2day
  moving average \textbf{of current and previous day concentrations}
  (lag=01)'', while others extract ``singleday lag models underestimate
  the cumulative effect of pollutants on mortality 2day moving average
  (lag=01)''. Similarity, for decisions, some runs may yield ``10 df for
  total mortality'', while other runs yield ``10 df''. Similar
  extraction appears in \citet{breitner2009}.
\item
  Gemini fails to extract reasons in some runs but not others, e.g.~in
  \citet{burnett1998}, the first run generates NAs in the reasons, but
  the remaining four runs are identical. In \citet{ueda2009} and
  \citet{castillejos2000} , runs 1 and 5 fail to extract the reasons and
  produce the same incomplete version, whereas runs 2, 3, and 4 produce
  accurate versions with reasons populated.
\end{itemize}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-gemini-2}Number of differences in the reason and
decision fields across Gemini runs for papers with consistent number of
decisions across runs.}

\tabularnewline

\toprule\noalign{}
Num. of difference & Count & Proportion (\%) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 358 & 79.73 \\
1 & 12 & 2.67 \\
2 & 8 & 1.78 \\
3 & 0 & 0.00 \\
4 & 24 & 5.35 \\
5 & 12 & 2.67 \\
6 & 3 & 0.67 \\
7 & 0 & 0.00 \\
8 & 10 & 2.23 \\
9 & 6 & 1.34 \\
10 & 10 & 2.23 \\
11 & 6 & 1.34 \\
Total & 449 & 100.00 \\

\end{longtable}

\subsection{LLM models}\label{sec-llm-models}

Reading text from PDF document requires Optical Character Recognition
(OCR) to convert images into machine-readable text, which currently is
only supported by Antropic Claude (\texttt{claude-3-7-sonnet-latest})
and Google Gemini (\texttt{gemini-2.0-flash}).

We compare the number of decisions extracted by Claude and Gemini across
all 56 papers in \textbf{?@fig-claude-gemini}. Each point represents a
paper, with the x- and y-axes showing the number of decisions extracted
by Claude and Gemini, respectively. The dashed 1:1 line marks where both
models extract the same number of decisions. Most points fall below this
line, indicating that Claude extracts more decisions -- often from data
pre-processing or secondary data analysis steps requiring more manual
validation -- whereas Gemini focuses more on modelling choices relevant
to our analysis. Some of these decisions captured by Claude are

\begin{itemize}
\tightlist
\item
  the definition of ``cold day'' and ``hot day'' indicators in
  \citet{dockery1992} (``defined at the 5th/ 95th percentile''),
\item
  the choice to summarize \(\text{NO}_2\), \(\text{O}_3\), and
  \(\text{SO}_2\) using a ``24 hr average on variable'' in
  \citet{huang2009}, and
\item
  the definition of black smoke and in \citet{katsouyanni2001} for
  secondary analysis (``restrict to days with BS concentrations below
  150 \(\mu g/m^2\)'').
\end{itemize}

Gemini sometimes also include irrelevant decisions, such as in
\citet{mar2000}, where secondary analysis choices like ``0-4 lag days''
for air pollution exposure variables (CO, EC, \(\text{K}_S\),
\(\text{NO}_2\), \(\text{O}_3\), OC, Pb, S, \(\text{SO}_2\), TC, Zn) are
captured. However, these cases are less frequent, resulting in outputs
with less noise overall.

For both Claude and Gemini, we find they fail to link the general term
``weather variables'' to the specific weather variables. For example
Gemini misses this link in \citet{dockery1992} and \citet{burnett2004},
while Claude does so in \citet{dockery1992} and \citet{katsouyanni2001}.
Although our prompt specified that some decisions may require linking
information across sentences and paragraphs to identify the correct
variable, this instruction doesn't appear to be applied consistently.

\subsection{Text model}\label{sec-text-model}

We have conducted sensitivity analysis on the text model for obtaining
the decision similarity score from the Gemini outputs. The tested
language models tested include

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\tightlist
\item
  BERT by Google \citep{devlin2019},
\item
  RoBERTa by Facebook AI \citep{liu}, trained on a larger dataset (160GB
  v.s. BERT's 15GB),
\item
  XLNnet by Google Brain \citep{yang}, and
\end{enumerate}

two domain-trained BERT models:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\setcounter{enumi}{3}
\tightlist
\item
  sciBERT \citep{beltagy2019}, trained on scientific literature, and
\item
  bioBERT \citep{lee2020}, trained on PubMed and PMC data.
\end{enumerate}

Figure~\ref{fig-text-density} presents the distribution of the decision
similarity (left) and paper similarity (right) for each text model. At
decision level, the BERT model produces the widest variation across all
five models, while the similarity scores from XLNet are all close to 1.
These scores are not comparable across models since the difference of
the underlying transformer architecture. However, the paper similarity
scores from each model are comparable and Figure~\ref{fig-text-mds}
shows the multi-dimensional scaling (MDS) of the paper similarity scores
from each text model: all showing a similar clustering pattern of the
three main smoothing methods.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-density-1.pdf}}

}

\caption{\label{fig-text-density}Distribution of decision similarity
(left) and paper similarity (right) scores for five different text
models (BERT, BioBERT, RoBERTa, SciBERT, and XLNet). The default
language model, BERT, produces the widest variation across the five
models, while the similarity scores form XLNet are all close to 1. The
model BioBERT, RoBERTa, and SciBERT yield decision similar scores mostly
between 0.7 to 1.}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-mds-1.pdf}}

}

\caption{\label{fig-text-mds}The multi-dimensional scaling (MDS) of the
paper similarity scores from each text model: all showing a similar
clustering pattern of the three main smoothing methods. The points are
colored by the most common method used in the paper, and the hulls are
drawn around each method group.}

\end{figure}%

\subsection{Others}\label{others}

There are other decisions in an analysis that are worth comparing and
documenting. For example data pre-processing decisions, e.g.~how
pollutant series are defined and collected, treatment on missing values,
etc. Again, for a complete review of the field, these decisions ideally
would be included, but for our demonstration of idea, we focus on the
modelling decisions. Spatial decisions are generally not well captured
because it often conducted uniformly as estimating the city individually
to accommodate city heterogeneity. Some papers only consider a handful
of cities, while in larger studies the individual city effects are then
pooled together using random effect.

The variation in the choice of parameters degree of freedom or knot for
smoothing can motivate separate investigation on the sensitivity
analysis. For instance, parameters that exhibit a wide range of choices
across studies may indicate areas of uncertainty or debate within the
field, suggesting that further investigation is needed to assess their
impact on study outcomes \citep{peng2006, touloumi2006}.

With LLMs, the extraction of decisions from literature could be largely
automated, but manual review is still needed to ensure the quality of
the extracted decisions. We also find secondary LLMs can be used to
standardize the extracted decisions, such as for temporal lag choices
from text expressing this decision in various ways. In this work, we use
prompt engineering to optimize the prompt for extracting decisions from
general LLMs (Claude and Gemini). Fine-tuning a local model is an
alternative approach for a locally-trained model. While it could
potentially yield more accurate extraction and hence less manual review,
for a systematic literature review, it would require substantially more
training efforts and a labelled decision dataset. We also find sometimes
the prompt is not fully followed throughout the extraction (example).
Claude and Gemini\ldots{}

Currently, only one model per paper - some have comparison of GLM and
GAM, compare different pollutants, stratify by \ldots.

With the advocacy for reproducibility in science, it is expected that
more papers will share their code and data. The availability of the code
could be a supplementary source for understanding the decisions made in
the analysis and cross comparison of the manuscript with the code.
However, given the lack of comments in the current practice, we are not
there to extract reasons for the decisions encoded in the script.

\section{Conclusion}\label{sec-conclusion}

In this paper, {[}we study how decisions are made in practical data
analysis{]}. We developed a pipeline for automatically extracting
decisions using LLMs (Claude and Gemini) and introduced a method for
calculating paper similarity through decision similarity. This
similarity metric enables us to cluster papers by their decision choices
and visualization through hierarchical clustering and multidimensional
scaling. We applied this pipeline to mortality/ hospital admission -- PM
modelling literature and extracted key modelling decisions, such as the
choice of smoothing methods and parameters for time, temperature, and
humidity, and revealed paper clusters that correspond to different
modelling strategies, as documented in the APHENA project.

While sensitivity analyses are commonly used to assess the robustness of
findings to different analytical choices, the set of choices tested is
often limited and selected subjectively by the authors. Our approach
offers a new perspective by pooling decisions made in analyses across
studies in the fields. This allows for a holistic account on the
alternatives in the field and identification of both consensus and
divergence within the field, providing insights for future research and
methodological development.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%% begin pandoc before-bib
%% end pandoc before-bib
%% begin pandoc biblio
%% end pandoc biblio
%% begin pandoc include-after
%% end pandoc include-after
%% begin pandoc after-body
%% end pandoc after-body

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
