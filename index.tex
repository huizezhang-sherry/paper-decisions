\documentclass[manuscript,screen,review,anonymous]{acmart}


\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother

%%
%% This is file `sample-manuscript.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `manuscript')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-manuscript.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.


% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}

\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}

%% PANDOC PREAMBLE BEGINS

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother





\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 
\usepackage[]{natbib}
\bibliographystyle{plainnat}


\definecolor{mypink}{RGB}{219, 48, 122}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
%% PANDOC PREAMBLE ENDS

\setlength{\parindent}{10pt}
\setlength{\parskip}{0pt}

\hypersetup{
  pdftitle={An LLM-based pipeline for understanding decision choices in data analysis from published literature},
  pdfauthor={H. Sherry Zhang; Roger D. Peng},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={red},
  pdfcreator={LaTeX via pandoc, via quarto}}

%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[CHI'26]{CHI Conference on Human Factors in Computing
Systems}{Apr 13--17, 2026}{Barcelona, Spain}
\acmPrice{}
\acmISBN{978-1-4503-XXXX-X/18/06}

%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%% end of the preamble, start of the body of the document source.
\begin{document}


%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{An LLM-based pipeline for understanding decision choices in data
analysis from published literature}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.


  \author{H. Sherry Zhang}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
        \author{Roger D. Peng}
  
            \affiliation{%
                  \institution{University of Texas at Austin}
                                  \city{Austin}
                                  \country{USA}
                      }
      

%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
%\renewcommand{\shortauthors}{Trovato et al.}
%%  
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Decision choices, such as those made when building regression models,
and their rationale are essential for interpreting results and
understanding uncertainty in an analysis. However, these decisions are
rarely studied because tracing every alternatives considered by authors
is often impractical, and reworking a completed analysis is generally of
limited interest. Consequently, researchers must manually review large
bodies of published analyses to identify common choices and understand
how choices are made. In this work, we propose a workflow to
automatically extract analytic decisions and their reasons from
published literature using Large Language Models. Our method also
introduces a paper similarity measure based on decision similarity and
visualization methods using clustering algorithms. As an example, this
workflow is applied to analyses studying the effect of particulate
matter on mortality. This approach enables scalable and automated
studies of decision choices in applied data analysis, providing an
alternative to existing qualitative and interview-based studies.    
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <ceval: <concept>
   <ceval: <concept_id>10003120.10003121</concept_id>
   <ceval: <concept_desc>Human-centered computing~Human computer interaction (HCI)</concept_desc>
   <ceval: <concept_significance>500</concept_significance>
 </eval: </concept>
 <concept>
   <concept_id>10002951.10003317</concept_id>
   <concept_desc>Information systems~Information retrieval</concept_desc>
   <concept_significance>500</concept_significance>
   </concept>
 </eval: </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Human-centered computing~Human computer interaction (HCI)}
\ccsdesc[500]{Information systems~Information retrieval}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{large language models, analytic decision making in data
analysis, document similarity}


%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\setlength{\parskip}{-0.1pt}

\section{Introduction}\label{introduction}

TODO: need references

Decisions are made at every stage of data analysis, from initial data
collection, pre-processing to modeling. One might expect well-trained
researchers to make similar choices when faced with the same analytical
task, yet evidence suggests otherwise. Many-analyst experiments show
that independent analysts often arrive at markedly different
conclusions, even when analyzing the same dataset to answer the same
research question \citep{silberzahn2018, botvinik-nezer2020, gould2025}.
This variation in analytical decision-making, described by
\citet{gelman2014} as the ``garden of forking paths,'' can undermine the
quality and credibility of reported results and hinder comparability
across studies. For junior researchers who lack guidance, this
variability may lead to over reliance on default statistical software
settings or arbitrary choices made without clear justification.

A common approach to investigate uncertainty in analytical decisions is
sensitivity analysis, where researchers systematically vary key
decisions in their analysis to assess the robustness of their findings.
Multiverse analysis extends this idea by evaluating \emph{all} plausible
combinations of decision choices to examine how results vary across the
full decision space \citep{multiverse, blair2019}. However, what an
analyst consider reasonable may not reflect the full range of options
used in practice. Even when a reasonable set of alternatives is tested,
the sensitivity analysis may be of limited interest to other researchers
facing a similar problem, who are seeking evidence to inform comparable
decision. Ideally, decision-making in applied research would be studied
by following experienced analysts throughout the entire analysis process
to capture their reasoning. In reality, this is rarely feasible and not
scalable. While individual studies may not capture the full range of
decision choices used in practice, crowdsourcing decisions from a
collection of studies on a shared theme creates a ``many-analyst''
setting that reveals how analysts make choices and justify them in
practice. Classic research training typically involves reading through
the literature to learn the common choices and to understand how
decisions are made. This process now has the possibility to be automated
at scale given recent advance in information extraction with Large
Language Models' (LLMs)
\citep{harrod2024, katz2024, farzi2024, hu2024, sciannameo2024, gu2025, schilling-wilhelmi2025, gupta2024, li2024, baddour2024, polak2024}.

In this work, we propose a new approach for studying data analysis
decisions by automatically extracting decisions from scientific
literature using LLMs. We develop a tabular schema to record decisions,
automate the extraction process with LLMs, and introduce a new paper
similarity measure based on decision similarity. This similarity measure
can serves as a distance metric in dimension reduction methods to
visualize papers according to their decisions. We apply this workflow to
a set of 56 air pollution modeling studies that estimate the effect of
particulate matter (PM2.5 or PM10) on mortality and hospital admissions.
This type of studies is typically analyzed using Poisson generalized
linear models (GLMs) or generalized additive models (GAMs). Analysis of
the extracted decisions reveals common choices in this class of studies,
such as the number of knots or degree of freedom for smoothing methods
and the temporal lags for time and weather variables. Multi-dimensional
scaling on the paper similarity distance finds three distinct clusters
corresponding to different smoothing methods: LOESS, natural spline, and
smoothing spline. These findings align with the APHENA project
\citep{APHENA}, which synthesizes research from multiple studies in
Europe and North America led by expert investigators.

In this workflow, we also provide detailed documentation on the
validation and standardization of LLM outputs. We outline the validation
and standardization process, including the use of a developed Shiny
application in R for reviewing decisions and the types of edits made
through validation. We also use a secondary LLM to standardize reported
choices of temporal lag decisions. Additionally, we conduct sensitivity
analysis on reproducibility across runs and model providers. future
studies for information extraction task with LLMs.

In summary, the contribution of this work includes:

\begin{itemize}
\item
  A scalable and automated approach to study data analysis decisions
  through extracting of decisions from published scientific literature
  using LLMs,
\item
  A new method to construct paper similarities based on decision choices
  and the semantic similarity of their rationales,
\item
  Practices for validating and standardizing LLM outputs, including a
  shiny GUI tool for editing outputs, the use of secondary LLM for
  standardizing unstructured response, and sensitivity analysis on
  reproducibility across runs and model providers,
\item
  A data schema for recording decisions in data analysis in a tidy
  format, and
\item
  A dataset of decisions, along with metadata, compiled from 56 studies
  in air pollution mortality modeling literature.
\end{itemize}

\section{Related work}\label{sec-background}

\subsection{Analytic decision making in data
analysis}\label{analytic-decision-making-in-data-analysis}

Data analysis is a complex and iterative process
\citep{jun2022, jun2022hypothesis, jun2019} that involves multiple
stages, including data collection, data cleaning, visualization,
modeling, and communication. At each stage, analysts make decisions
informed by domain practices, statistical knowledge, and the data. These
decisions, such as which variables to include in a model, how to handle
missing data, and how hyper-parameters are chosen, act as branching
points in the analysis workflow. {[}TODO{]}The full set of possible
paths through these branching points form what \citet{gelman2014}
describe as the ``garden of forking paths''. While one might expect
well-trained researchers to make similar choices when facing similar
decisions, empirical evidence suggests otherwise. ``Many analyst
experiments'' show that independent research groups analyzing the same
dataset to address the same research questions can arrive at widely
different conclusions. For example, \citet{silberzahn2018} asks 29
groups of analysts to conduct an analysis to address the same research
questions \emph{whether soccer players with dark skin tone are more
likely than those with light skin tone to receive red cards from
referees}. Researchers reported an estimated effect size from 0.89 to
2.93 in odds ratio with 21 unique combinations of covariates are used
among all 29 analyses. 70\% of the teams found a statistically
significant positive effect while others don't. This great discrepancy
among researchers when performing data analysis task is also observed in
other domains, for example, structural equation modeling
\citep{sarstedt2024}, applied microeconomics
\citep{huntington-klein2021}, neuroimaging \citep{botvinik-nezer2020},
and ecology and evolutionary biology \citep{gould2025}.

Examples like the above illustrate how analytical decisions introduce
uncertainty into data analysis. These uncertainties have been widely
discussed in the literature given their impact for policy recommendation
\citep{APHENA} and {[}TODO{]} applications in health, finance, fairness
machine learning \citep{simson2025}. Through experiments, research has
shown that analysts' decisions can lead to p-hacking and inflated effect
size, when not properly used \citep{wicherts2016, simmons2011}. Hence,
guidelines and checklists have been developed to recommend the best
practice to guide statistical analysis. In medicine and biostatistics,
pre-registration is a common practice to regulate analysts making
decisions after seeing the data \citep{van2015}. Given the nuanced
nature of data analysis, more work have examined how analysts make
decisions in practice through interviews in both academia and industry.
These studies include qualitative analysis of the decisions made
\citep{kale2019, liu2020}, interviews with data analysts about
exploratory data analysis practice in industry
\citep{alspaugh2019, kandel2012} and about how they consider
alternatives in data analysis \citep{liu_understanding_2020}.

In addition to qualitative studies, software tools have developed to
help researchers account for alternatives and uncertainties and make
informed decisions in data analysis. Examples include \texttt{Tea}
\citep{jun2019}, which support general statistical analysis;
\texttt{Tisane} \citep{jun2022}, which guides choices in generalized
linear mixed-effects models (GLMMs); and \texttt{MetaExplore}
\citep{kale2023}, which account for epistemic uncertainty (decision
uncertainty) in meta-analysis. The \texttt{DeclareDesign} package
\citep{blair2019} proposes the MIDA framework for researchers to
declare, diagnose, and redesign their analyses with account for
uncertainties of reporting the statistic of interest. Multiverse
analysis proposes a different method to allow researchers to evaluate
\emph{all} plausible combinations of decision choices to examine how
results vary in the full decision space. Work has been done on the
software tools to support multiverse analysis
\citep{multiverse, götz2024} and visualization of multiverse results
\citep{liu2021}, and debugging tools \citep{gu2023}.

\subsection{Automatic information extraction with
LLMs}\label{automatic-information-extraction-with-llms}

In natural language processing, information extraction is a task focus
on extracting structured information from unstructured text. Earlier
approaches in information extraction tasks relied on rule-based systems
and regular expressions. More recent advances, including conditional
random fields \citep{lafferty}, word embeddings such as word2vec
\citep{mikolov2013}, and transformer-based architectures like BERT
\citep{devlin2019}, have led to the current use of LLM to extract
information with prompts. Using LLMs to extract unstructured text offers
the advantage of automating the process at scale. Applications have been
seen in epidemiology data \citep{harrod2024}, scientific literature
\citep{katz2024}, clinical data
\citep{farzi2024, hu2024, sciannameo2024, gu2025}, chemistry knowledge
\citep{schilling-wilhelmi2025}, and polymer science \citep{gupta2024},
climate extreme impact \citep{li2024}, phenotypes \citep{baddour2024},
and material properties \citep{polak2024}. An easier task in information
extraction is called Named Entity Recognition (NER) to identify short
span information (1-4 tokens) like person names and locations from
unstructured text \citep{nadeau2007}. An example of this is extracting
patient's information and vitals in clinical data. Extracting decisions
from published literature is a more general task than NER, since
justification of a decision typically spans more than just a few words.
Our task also requires linking information across sentences, sometimes
sections, to correctly identify the variables a decision refers to.

\subsection{Visualization on scientific
literature}\label{visualization-on-scientific-literature}

With the growing volume of scientific publications and the difficulty of
navigating the literature, there is an increasing interest in developing
systems to visualize and recommend scientific papers. These systems link
papers based on their similarity and relevance, typically determined by
keywords \citep{isenberg2017}, citation information \citep{chen2006},
e.g.~citation list and co-citation, or combinations with other relevant
paper metadata \citep{bethard2010, chou2011, dörk2012, heimerl2016},
e.g.~author and title. Recent approaches incorporate text-based
information using topic modeling \citep{alexander2014},
argumentation-based information retrieval \citep{tbahriti2006}, and text
embedding \citep{narechania2022}. While metadata and high-level
text-based information are useful for finding relevant papers,
researchers also need tools that help them \emph{make sense} of the
literature rather than simply \emph{locating} it. In applied data
analysis, one interest is to understand how studies differ or align in
their decision choices. Capturing the decision choices and reasons that
justify the choices from analyses enables the calculation of similarity
among papers and can pipe into dimension reduction methods and
visualization for a global view of analysis practice in the field or
recommend similar papers based on decision similarities.

\section{Methods}\label{sec-extract-decisions}

In this section, we present the workflow for extracting decisions from
published literature using LLMs. We first describe the data structure
for recording decisions, followed by the four main steps in the
workflow: 1) automatic extraction of decisions from literature with
LLMs, 2) validation and standardization of LLM outputs, 3) calculation
of paper similarity, and 4) visualization paper similarity using
clustering or dimension reduction methods. The section concludes with an
illustration summarizing the workflow.

\subsection{Record decisions in data analysis}\label{sec-decisions}

In the study of the health effects of outdoor air pollution, one area of
interest is the association between short-term, day-to-day changes in
particulate matter air pollution and daily mortality counts. This
question has been studied extensively by researchers across the globe
and it serves to provide scientific evidence in the US to guide public
policy on setting the National Ambient Air Quality Standards (NAAQS) for
air pollutants. While individual modeling choices vary, these studies
often share a common structure: they adjust for meteorological
covariates, such as temperature and humidity, include lagged variables
to account for temporal correlations, and estimate the effect size by
city or region before pooling the results with random effect. This
naturally forms a ``many-analyst'' experiment setting to analyze
decisions in air pollution mortality modelling.

Consider the following excerpt from \citet{ostro2006} modeling the
association between daily counts of mortality and ambient particulate
matter (PM10):

\begin{quote}
Based on previous findings reported in the literature (e.g., Samet et
al.~2000), the basic model included a smoothing spline for time with 7
degrees of freedom (df) per year of data. This number of degrees of
freedom controls well for seasonal patterns in mortality and reduces and
often eliminates autocorrelation.
\end{quote}

This sentence encode the following components of a decision:

\begin{itemize}
\tightlist
\item
  \textbf{variable}: time
\item
  \textbf{method}: smoothing spline
\item
  \textbf{parameter}: degree of freedom (df)
\item
  \textbf{reason}: Based on previous findings reported in the literature
  (e.g., Samet et al.~2000); This number of degrees of freedom controls
  well for seasonal patterns in mortality and reduces and often
  eliminates autocorrelation.
\item
  \textbf{decision}: 7 degrees of freedom (df) per year of data
\end{itemize}

This decision can be recorded in a tabular format following the tidy
data principle \citep{wickham2014}, which states that each variable
forms a column and each observation forms in a row. For our purpose,
each row represents a decision made in a paper and an analysis often
include multiple decisions. We extract the original text in the paper,
without paraphrase or summarization. The decision above is a parameter
choice of a statistical method applied to the variable \emph{time}. A
data analysis may also include other types of decisions, such as
temporal or spatial ones, for example, the choice of lagged exposure for
certain variables or whether the model is estimated collectively or
separated for individual locations. These decisions don't have a
specific method or parameter fields, but should still include variable,
type (spatial or temporal), reason, and decision fields.

Given the writing style of authors, multiple decisions may be combined
in one sentence and certain fields may be omitted. Consider a different
excerpt from \citet{ostro2006}:

\begin{quote}
Other covariates, such as day of the week and smoothing splines of 1-day
lags of average temperature and humidity (each with 3 df), were also
included in the model because they may be associated with daily
mortality and are likely to vary over time in concert with air pollution
levels.
\end{quote}

This sentence contains four decisions: two for temperature (the temporal
lag and the smoothing spline parameter) and two for humidity, and should
be structured as separate entries:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}
  >{\raggedright\arraybackslash}p{(\linewidth - 14\tabcolsep) * \real{0.1250}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Paper
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
parameter
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ostro & 1 & temperature & smoothing spline & degree of freedom &
parameter & 3 degree of freedom & NA \\
ostro & 2 & relative humidity & smoothing spline & degree of freedom &
parameter & 3 degree of freedom & NA \\
ostro & 3 & temperature & NA & NA & temporal & 1-day lags & NA \\
ostro & 4 & relative humidity & NA & NA & temporal & 1-day lags & NA \\
\end{longtable}

Notice in the example above, the reason field is recorded as NA. This is
because the stated reason (``and are likely to vary over time in concert
with air pollution levels'') only supports the general inclusion of
temporal lags but does not justify the specific choice of 1-day lag over
other alternatives, e.g.~2-day average of lags 0 and 1 or single-day lag
of 2 days. Similar scenario can happen when a direct decision choice is
missing but a reason is provided, as in \citet{katsouyanni2001}:

\begin{quote}
The inclusion of lagged weather variables and the choice of smoothing
parameters for all of the weather variables were done by minimizing
Akaike's information criterion.
\end{quote}

\subsection{Extract decisions automatically from literature with
LLMs}\label{extract-decisions-automatically-from-literature-with-llms}

Manually extracting decisions from published papers is labor-intensive
and time-consuming. With LLMs, it is now possible to automatically
extract this type of information by supplying a set of PDF documents and
a prompt for instruction. Text recognition from PDF document relies on
Optical Character Recognition (OCR) to convert scanned images into
machine-readable text -- a capability currently offered by Antropic
Claude and Google Gemini. In the prompt, we assign the LLM a role as an
applied statistician and instruct it extract decisions from the PDF in
the format, described in Section~\ref{sec-decisions} and write the
output in a JSON block in a markdown file. We also provide a set of
instructions and examples on the possibility of missing of reason and
decision fields as discussed in Section~\ref{sec-decisions}. Prompt
engineering techniques \citep{chen2025, xu} are used to optimize the
prompt and the full prompt used in this work is provided in the
Appendix. We use the \texttt{chat\_PROVIDER()} functions from the
\texttt{ellmer} package \citep{ellmer} in R to obtain the output.

\subsection{Validate and standardize LLM
outputs}\label{validate-and-standardize-llm-outputs}

The LLM outputs need to be validated and standardized before further
analysis. Validation focuses on ensuring the extracted decisions are
correct, while standardization ensure different expressions of the same
variable are standardized into the same expression. For example, the
expression \emph{mean temperature}, \emph{average temperature}, and
\emph{temperature} all refer to the same variable and are standardized
to \emph{temperature}. To help with the validation and standardization
process, we developed a Shiny application, which provides an interactive
interface for users to review and edit the LLM outputs. The Shiny
application takes an input of a CSV file that contains the extracted
decisions and allows users to perform three types of edits: 1)
\emph{overwrite} -- modify the content of a particular cell, 2)
\emph{delete} -- remove an irrelevant decision, and 3) \emph{add} --
manually enter a missing decision. Figure~\ref{fig-shiny} illustrates
the \emph{overwrite} action for standardizing the variable \emph{NCtot}
(number concentration of particles \textless100 nm in diameter) to
\emph{pollution}. The user enters a predicate function in the filter
condition box on the left panel, and the filtered data will appear
interactively on the right panel. The user can then specify the variable
to overwrite and the new value. The corresponding cells on the right
panel will be updated. This change need to be confirmed by pressing the
``Apply changes'' button to update to the full dataset. The
corresponding \texttt{tidyverse} \citep{tidyverse} code will then be
generated on the left panel to be included in an R script, and the
edited table can be downloaded for future analysis.

\begin{figure}

\centering{

\includegraphics[width=0.8\linewidth,height=0.8\textheight]{figures/shiny.png}

}

\caption{\label{fig-shiny}The Shiny application interface to validate
and standardize Large Language Model (LLM)-generated output. (1) The
default interface after loading the input CSV file. (2) The table view
will update interactively to reflect the edit: for paper with handle
``andersen2008size'' and id in 4, 5, 6, modify the variable name
\emph{NCtot} to \emph{pollutant}. (3) After clicking the Confirm button,
the corresponding \texttt{tidyverse} code for the modification is
generated, and the table view returns to its original unfiltered view
with the edit applied. The edited data can be downloaded by clicking the
Download CSV button.}

\end{figure}%

\subsection{Calculate paper similarity and
visualization}\label{sec-paper-similarity}

Once the output has been extracted and validated, these decisions can be
treated as data for further analysis. Apart from exploratory data
analysis, we propose a paper similarity measure to compare how similar
decisions are between paper pairs. A decision is considered comparable
between a paper pair if the two papers share the same variable and
decision type, e.g.~a parameter decision on temperature. Three factors
are considered in calculating the similarity between two matched
decisions: 1) whether the two decisions are similar, 2) whether the
reasons for the decisions are similar, and 3) for parameter type
decisions, whether the statistical methods used are the same. Method and
choice similarity indicate the same decision being made in the analysis,
whereas a similar reason reflects a shared principle for making the
choice, even when the choices themselves may differ due to differences
in the underlying data. For reasons and choices, we first obtain the
text embedding for all the choices and reasons, and calculate the cosine
similarity between the matched reason and decisions from the language
model \texttt{BERT} using the \texttt{text} package \citep{text} in
\texttt{R}. For methods, we encode them as a binary variable: 1 if the
two papers used the same method, and 0 otherwise because semantic
similarity cannot fully capture the difference between statistical
methods, e.g., the difference between smoothing spline and natural
spline is not well represented by the textual difference of
``smoothing'' and ``natural''. The paper similarity is then computed as
the average decision similarities across all the matched methods,
decisions, and reasons.

Although paper similarity can be calculated based on all available
matched decisions, cares should be taken for pairs with only a small
number of matches. This can happen because two papers focus on different
variables or some decisions have missing choices or reasons (discussed
in Section~\ref{sec-decisions}). In practice, users may decide to focus
on a set of decisions shared among papers or on papers that report a
minimal number of shared decisions when calculating paper similarity.

\subsection{Summary}\label{summary}

Figure~\ref{fig-workflow} summarises the whole workflow proposed for
extracting and analyzing decisions from published literature using LLMs.
Once researchers have identified a set of literature of interest, a
prompt is needed to instruct LLMs to extract decisions from these
literature. The outputs from LLM need to be validated and standardized
before further analysis, due to authors' varied writing styles. The
validated data can then be used for exploratory data analysis of
decisions and one analysis we propose is to calculate paper similarity.
This paper similarity metric can be seen as a distance metric among
papers, which can be used for clustering and dimension reduction to
visualize the decision patterns among papers.

\begin{figure}

\centering{

\includegraphics[width=1\linewidth,height=\textheight,keepaspectratio]{figures/workflow.png}

}

\caption{\label{fig-workflow}The workflow for extracting decisions from
published literature using Large Language Models (LLMs) and analyzing
the extracted decisions. The workflow consists of four main steps: (1)
Extract decisions automatically from literature with LLMs, (2) Validate
and standardize LLM outputs, (3) Calculate paper similarity and
visualization, and (4) visualization with clustering or dimension
reduction methods.}

\end{figure}%

\section{Results}\label{sec-result}

We apply the workflow to extract the decisions in 56 studies that
estimate the effect of particulate matters (\(\text{PM}_{10}\) and
\(\text{PM}_{2.5}\)) on mortality and hospital admission using Gemini
(\texttt{gemini-2.0-flash}). We focus on the baseline model reported in
each paper, excluding secondary models (e.g.~lag-distributed models),
multi-pollutant models, and alternatives tested in the sensitivity
analysis, which are discussed in \textbf{?@sec-discussions}. This yields
242 decisions extracted, averaging 4 decisions per paper.

\subsection{Validation and standardization of LLM
outputs}\label{sec-res-validation}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.9355}}
  >{\raggedleft\arraybackslash}p{(\linewidth - 2\tabcolsep) * \real{0.0645}}@{}}

\caption{\label{tbl-review}Summary of validation and standardization
edits made during the review process.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Count
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Remove decisions out of scope: other pollutants and sensitivity analysis
& 50 \\
Edit made to recode smoothing parametser unit to per year & 45 \\
Duplicates & 9 \\
Fix incorrect capture & 9 \\
Edit made due to decisions are too general, e.g.~minimum of 1 df per
year was required & 6 \\
Remove decisions related to definition of variables, e.g.~season & 5 \\
Total & 124 \\

\end{longtable}

Table~\ref{tbl-review} summarizes the number of edits made during the
review process using the Shiny application. Validation includes fixing
incorrect captures, removing non-decision (e.g.~definition of
variables), removing duplication, excluding irrelevant decisions
(e.g.~sensitivity analyses), and excluding decisions whose stated
reasons reflect general guidelines rather than actual choices
(e.g.~``minimum of 1 degree of freedom per year is required'').

Standardization is performed on the variable name of decisions and
choices. The variable name in the decisions are standardized into four
main categories:

\begin{itemize}
\tightlist
\item
  \textbf{temperature}: ``mean temperature'', ``average temperature'',
  ``temperature'', ``air temperature'', ``ambient temperature''
\item
  \textbf{humidity}: ``dewpoint temperature'' and its hyphenated
  variants, relative humidity'', ``humidity''
\item
  \textbf{PM}: ``pollutant'', ``pollution'', ``particulate matter'',
  ``particulate'', ``PM10'', ``PM2.5''
\item
  \textbf{time}: ``date'', ``time'', ``trends'', ``trend''
\end{itemize}

Notice that ``dewpoint temperature'' is standardized under humidity
because it serves as a proxy for temperature in achieving a 100\%
relative humidity.

Decisions themselves also require standardization. For example, the
smoothing parameter (number of knots and degree of freedom) may be
expressed as \emph{per year} or \emph{in total}, and temporal lag
decision may be expressed in different formats (e.g.~``6-day average'',
``mean of lags 0+1'', ``lagged exposure up to 6 days''). Decision
choices on the smoothing parameter are manually recoded to a \emph{per
year} basis, as in Table~\ref{tbl-review}. Temporal decisions show a
wider variety, which makes manual standardization impractical. However,
we observe that they generally falling into two categories:

\begin{itemize}
\tightlist
\item
  \textbf{multi-day average lags}: ``6-day average'', ``3-d moving
  average'', ``mean of lags 0+1'', ``cumulative lags, mean 0+1+2'', etc
  and
\item
  \textbf{single-day lags}: ``lagged exposure up to 6 days'', ``lag days
  from 0 to 5'', etc
\end{itemize}

Hence we apply a secondary LLM (claude-3-7-sonnet-latest) to convert
temporal decisions into a consistent format:
\texttt{multi-day:\ lag\ {[}start{]}-{[}end{]}} and
\texttt{single-day:\ lag\ {[}start{]},\ …\ ,lag\ {[}end{]}}. This
converts ``6-day average'' into ``multi-day: lag 0-5'' and ``lagged
exposure up to 6 days'' into ``single-day: lag 0, lag 1, lag 2, lag 3,
lag 4, lag 5''.

\subsection{Exploratory analysis of decision
choices}\label{exploratory-analysis-of-decision-choices}

\begin{table}

\caption{\label{tbl-missing-decisions}Missingness of decision and reason
fields in the Gemini-extracted decisions. Most decisions report the
choice (35.5 + 57.1 = 92\%), but 57.1\% lacks a stated reason.}

\centering{

\begin{tabular}{lll}
\toprule
\multicolumn{1}{c}{} & \multicolumn{2}{c}{Decision} \\
\cmidrule(l{3pt}r{3pt}){2-3}
Reason & Non-missing & Missing\\
\midrule
Non-missing & 90 (37.2\%) & 14 (5.8\%)\\
Missing & 134 (55.4\%) & 4 (1.7\%)\\
\bottomrule
\end{tabular}

}

\end{table}%

As raised in Section~\ref{sec-decisions}, not all decisions reported in
the literature include both the decision choice and the rationale. Some
decisions may only report the choice without a stated reason, while
others may provide a reason without specifying the exact choice made.
Table~\ref{tbl-missing-decisions} summarizes the missingness of the
decisions and reason. While 37\% of decisions are complete in both
decision choices and reasons, 55\% of decisions lack a stated rationale
for the choice. This reflects a common reporting practice in the field,
where authors often report the decision choice used without an explicit
reason.

\begin{longtable}[]{@{}llr@{}}

\caption{\label{tbl-most-common-decisions}Count of variable-type
decisions in the Gemini-extracted decisions. The most commonly reported
decision are the parameter choices and temporal lags for for time, PM,
temperature, and humidity.}

\tabularnewline

\toprule\noalign{}
Variable & Type & Count \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
time & parameter & 44 \\
PM & temporal & 39 \\
temperature & parameter & 35 \\
humidity & parameter & 25 \\
temperature & temporal & 23 \\
humidity & temporal & 19 \\
PM & parameter & 9 \\
time & temporal & 3 \\

\end{longtable}

Table~\ref{tbl-most-common-decisions} lists the eight most frequently
reported decision: parameter and temporal choice for \texttt{time},
\texttt{PM}, \texttt{temperature}, and \texttt{humidity.} While a wider
list of variables have been used in the analysis, these four variables
are most commonly included in baseline models. Parameter choices for
time, temperature, and humidity are typically made on the use of
smoothing parameter for the smoothing method (natural spline and
smoothing spline), whereas temporal choices are commonly reported for
PM, temperature, and humidity for the number of lag to consider in the
model.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1889}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6778}}@{}}

\caption{\label{tbl-humidity-temperature-decisions}Options captured for
parameter choices for time, humidity, and temperature variables in the
Gemini-extracted decisions. The choices for natural spline knots are
generally less varied than the degree of freedom choices for smoothing
spline. Choices for temperature and humidity tend to be close, given
they are both weather related variables, while the choices for time are
more varied inherently.}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Method
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
natural spline & humidity & 3, 4 \\
natural spline & temperature & 3, 4, 6 \\
natural spline & time & 1, 1.5, 3, 4, 6, 7, 8, 12, 15, 30 \\
smoothing spline & humidity & 2, 3, 4, 6, 8, 50\% of the data \\
smoothing spline & temperature & 2, 3, 4, 6, 8, 50\% of the data \\
smoothing spline & time & 1, 3, 4, 5, 6, 7, 7.7, 8, 9, 10, 12, 30, 100,
5\% of the data \\

\end{longtable}

Table~\ref{tbl-humidity-temperature-decisions} presents the number of
knots or degree of freedom used in two spline methods (natural and
smoothing spline) applied to variable \texttt{time}, \texttt{humidity}
and \texttt{temperature}, with all values standardized to a \emph{per
year} scale. The choices of knot for natural spline has less variation
than the degree of freedom choices for smoothing spline. Choices for
temperature and humidity are generally similar, given they are both
weather related variables, whereas choices for time are more varied.
This tabulation provides a reference set for common parameter choices
for future studies and help to identify anomalies and special treatment
in practice. For example, the choice of 7.7 degree of freedom reported
in \citet{castillejos2000} may prompt analysts to seek further
justification for its use. By cross-comparing with other reporting, some
decisions appear ambiguous. For example, in \citet{moolgavkar2000} and
\citet{moolgavkar2003}, the reported value of 30 and 100 degrees of
freedom for time may be understandable for experienced domain
researchers, it could be unclear for junior analysts as to whether they
refer to the parameter used for the full study period or on a per-year
basis, which is often clear in other paper. We also observe a different
report style from \citet{schwartz2000}, where smoothing spline
parameters are expressed as a proportion of the data (``5\% of the
data'' and ``5\% of the data''), rather than fixed numerical value.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.2278}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1519}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.6203}}@{}}

\caption{\label{tbl-temporal-decisions}Options captured for temporal lag
choices for PM, temperature, and humidity variables in the
Gemini-extracted decisions. Both single-day lags and multi-day average
lags are commonly used, generally considering up to five days prior (lag
5).}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Lag type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Variable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Decision
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
multi-day average & PM & lag 0-1, 0-2, 0-3, 0-4, 0-5, 0-6 \\
multi-day average & humidity & lag 0-1, 0-2, 0-3, 0-5, 1-5, 2-4 \\
multi-day average & temperature & lag 0-1, 0-2, 0-3, 0-5, 2-4 \\
single-day lag & PM & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
13 \\
single-day lag & humidity & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13 \\
single-day lag & temperature & lag 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11,
12, 13 \\

\end{longtable}

Similarly, Table~\ref{tbl-temporal-decisions} summarizes the temporal
lag choices for \texttt{PM}, \texttt{temperature}, and
\texttt{humidity.} For single-day lags, the lags are considered up to 13
days (approximately two weeks) while for multi-day averages, 3-day and
5-day averages are the most common, although other choices such as 2-4
day average are also observed \citep{lópez-villarrubia2010}.

\subsection{Paper similarity calculation, clustering analysis, and
visualization}\label{paper-similarity-calculation-clustering-analysis-and-visualization}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-mds-1.pdf}}

}

\caption{\label{fig-mds}The multi-dimensional scaling (MDS) based on
paper similarity distance for \texttt{length(good\_pp)} air pollution
mortality modeling papers, colored by the smoothing method used. The MDS
reveals the three distinct groups of papers, corresponds to LOESS,
natural spline, and smoothing spline. These groups corresponds to the
different modeling strategies debated in the European and U.S. studies,
as documented in the APHENA project \citep{APHENA}.}

\end{figure}%

Given the number of decisions reported in
Table~\ref{tbl-most-common-decisions}, we focus on the six most common
variable-type decisions for calculating paper similarity: parameter
choices for time, temperature, and humidity, and temporal lag choices
for PM, temperature, and humidity. We also restrict our analysis to
papers that report at least three of these six decisions, resulting in
48 papers for the paper similarity calculation. This ensures that the
paper similarity metric is based on a sufficient number of comparable
decisions. We use the default text embedding model (BERT) in the
\texttt{text} package and cosine similarity to compute the similarity
score. Sensitivity analysis on different text embedding model is checked
in Section~\ref{sec-text-model}. Paper similarity is then calculated as
the average of decision similarity for each paper pair. The resulting
similarity score is then used as the distance matrix in
multi-dimensional scaling (MDS) and plotted in Figure~\ref{fig-mds}. The
two MDS dimension axes reveal three clusters correspond to the three
smoothing methods used in these analyses: LOESS, natural spline, and
smoothing spline, where natural spline is commonly used in U.S. based
studies suggested in the NMMAPS study \citep{samet2000}, while LOESS and
smoothing spline are more often used in the European studies, as
suggested in the APHEA \citep{katsouyanni1996} and APHEA2
\citep{katsouyanni2001} project.

\subsection{Sensitivity analysis}\label{sensitivity-analysis}

A series of sensitivity analysis have been conducted to explore the
reproducibility across runs (Section~\ref{sec-llm-reproducibility}),
model providers (Section~\ref{sec-llm-models}), and the sensitivity of
text model for computing the semantic decision similarity
(Section~\ref{sec-text-model}).

\subsubsection{LLM reproducibility}\label{sec-llm-reproducibility}

\begin{longtable}[]{@{}lll@{}}

\caption{\label{tbl-gemini-1}Example comparing Gemini's text extraction
for \citet{andersen2008} across two runs. The extracted decisions are
identical in both runs.}

\tabularnewline

\toprule\noalign{}
Variable & Run1 & Run2 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
NCtot & 6day average (lag 05) & 6day average (lag 05) \\
calendar time & 3 4 or 5 dfyear & 3 4 or 5 dfyear \\
dew-point temperature & 4 or 5 df & 4 or 5 df \\
temperature & 4 or 5 df & 4 or 5 df \\

\end{longtable}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-gemini-2}Number of differences in the reason and
decision fields across Gemini runs for papers with consistent number of
decisions across runs.}

\tabularnewline

\toprule\noalign{}
Num. of difference & Count & Proportion (\%) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & 358 & 79.73 \\
1 & 12 & 2.67 \\
2 & 8 & 1.78 \\
3 & 0 & 0.00 \\
4 & 24 & 5.35 \\
5 & 12 & 2.67 \\
6 & 3 & 0.67 \\
7 & 0 & 0.00 \\
8 & 10 & 2.23 \\
9 & 6 & 1.34 \\
10 & 10 & 2.23 \\
11 & 6 & 1.34 \\
Total & 449 & 100.00 \\

\end{longtable}

We assess the reproducibility across runs of Gemini
(\texttt{gemini-2.0-flash}) by repeating the text extract task five
times for each of the 62 papers and perform pairwise comparison between
runs. This generates \(5 \times 4 /2 \times 62 = 620\) possible
comparisons for both ``reason'' and ``decisions'' fields. Comparisons
are excluded when two runs produced a different number of decisions
since this would require manual alignment. This leaves 449 out of 620
(72\%) extractions to compare. Table~\ref{tbl-gemini-1} prints an
comparison of decisions in \citet{andersen2008} across two runs and all
the four decisions are identical with no difference.
Table~\ref{tbl-gemini-2} summarizes the number of differences observed
in each pairwise comparison. Among all comparisons, 80\% produce the
identical text in reason and decision. The discrepancies mainly come
from the following two reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi})}
\item
  Gemini extracted the same decision in different length. For example,
  in \citet{kan2007}, some runs may extract ``singleday lag models
  underestimate the cumulative effect of pollutants on mortality 2day
  moving average \textbf{of current and previous day concentrations}
  (lag=01)'', while others extract ``singleday lag models underestimate
  the cumulative effect of pollutants on mortality 2day moving average
  (lag=01)''.
\item
  Gemini fails to extract reasons in some runs but not others. For
  example, in \citet{burnett1998}, the first run generates \texttt{NA}
  in the reason, but the remaining four runs are identical with the
  reason populated. In \citet{ueda2009} and \citet{castillejos2000} ,
  runs 1 and 5 fail to extract the reason and produce the same
  incomplete version, whereas runs 2, 3, and 4 produce accurate versions
  with reason populated.
\end{enumerate}

\subsubsection{LLM models}\label{sec-llm-models}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-claude-gemini-1.pdf}}

}

\caption{\label{fig-claude-gemini}Comparison of decisions extracted by
Claude and Gemini. Each point represents a paper, with the x- and y-axis
showing the number of decisions extracted by Claude and Gemini,
respectively. The dashed 1:1 line marks where both models extract the
same number of decisions. More points fall below this line, suggesting
Claude extracts more decisions -- often including noise from data
pre-processing or secondary data analysis steps -- which requires
additional manual validation.}

\end{figure}%

Reading text from PDF document requires Optical Character Recognition
(OCR) to convert images into machine-readable text, which currently is
only supported by Antropic Claude and Google Gemini. We compare the
number of decisions extracted by Gemini (\texttt{gemini-2.0-flash}) and
Claude (\texttt{claude-3-7-sonnet-latest}) across all 62 papers. In
Figure~\ref{fig-claude-gemini}, each point represents a paper, with the
x- and y-axis showing the number of decisions extracted by Claude and
Gemini, respectively. The dashed 1:1 line marks where both models
extract the same number of decisions. In general the two models produce
similar number of decision. However, more points fall below this line,
suggesting Claude extracts more decisions, often including noise from
data pre-processing or secondary data analysis steps. Examples of papers
with large discrepancies include \citet{mar2000} (Claude: 10 vs.~Gemini:
28), \citet{ito2006} (Claude: 25 vs.~Gemini: 19), \citet{ko2007}
(Claude: 8 vs.~Gemini: 16), among others. For both Claude and Gemini, we
find they sometimes fail to link the general term ``weather variables''
to the specific weather variables (e.g. \citet{dockery1992} and
\citet{burnett2004} for Gemini and \citet{dockery1992} and
\citet{katsouyanni2001} for Claude). Although our prompt specified that
some decisions may require linking information across sentences and
paragraphs to identify the correct variable, this instruction doesn't
appear to be applied consistently.

\subsubsection{Text model}\label{sec-text-model}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{index_files/figure-pdf/fig-text-model-1.pdf}}

}

\caption{\label{fig-text-model}Distribution of decision similarity
(left) and multi-dimensional scaling (MDS) of the paper similarity
scores (right) computed for five different text models (BERT, BioBERT,
RoBERTa, SciBERT, and XLNet). The default language model, BERT, produces
the widest variation across the five models, while the similarity scores
form XLNet are all close to 1. The model BioBERT, RoBERTa, and SciBERT
yield decision similar scores mostly between 0.7 to 1. All the text
models shows a similar clustering structure based on the three main
smoothing methods (LOESS, natural spline and smoothing spline).}

\end{figure}%

We have conducted sensitivity analysis on the text model for calculating
decision similarity score from the Gemini outputs. The tested language
models tested include 1) BERT \citep{devlin2019} by Google, 2) RoBERTa
\citep{liu} by Facebook AI, trained on a larger dataset (160GB v.s.
BERT's 15GB), 3) XLNnet \citep{yang} by Google Brain, and two
domain-trained BERT models: 4) sciBERT \citep{beltagy2019}, trained on
scientific literature, and 5) bioBERT \citep{lee2020}, trained on PubMed
and PMC data.

Figure~\ref{fig-text-model} shows the distribution of the decision
similarity and the corresponding multi-dimensional scaling
visualization, where distance are calculated from the paper similarity
for each text model. At decision level, the BERT model produces the
widest variation across all five models, while the similarity scores
from XLNet are all close to 1. While the raw scores are not directly
comparable across models due to the difference in the underlying
transformer architecture, the visualizations from multi-dimensional
scaling (MDS) based on paper similarity scores all show a similar
clustering pattern corresponding to the three main smoothing methods
(LOESS, natural spline, and smoothing spline).

\section{Discussion}\label{sec-discussion}

\subsection{Large-language models for information
extraction}\label{large-language-models-for-information-extraction}

Numerous studies
\citep{harrod2024, katz2024, farzi2024, hu2024, sciannameo2024, gu2025, schilling-wilhelmi2025, gupta2024, li2024, baddour2024, polak2024}
have demonstrated the capability of LLMs for information extraction
task. Our work applies the LLMs to extract analytic decisions in
scientific literature, providing further evidence of their effectiveness
for information extraction task. Our task requires capturing more
complex analytical decisions and their justifications, which typically
span more than just a few tokens like in named entity recognition. Our
task also requires linking information across sentences and sometimes
sections to correctly identify the variables of a decision (e.g.,
linking ``weather'' to ``temperature'' and ``humidity''). While LLM has
performed well on extracting decisions from the literature, manual
validations are still required to ensure the quality of the extracted
decisions for downstream analysis. Most existing applications evaluate
LLMs by comparing their outputs to human-annotated datasets, reporting
metrics such as precision, recall, and F1 score. Because this approach
depends on labeled data and it is not yet clear how these outputs should
be validated for downstream analysis in practice. In our work, we
automate some of the manual validation with a secondary LLM (Claude) to
standardize the temporal lag choices in different expression into two
categories.

With a default temperature of one and the prompt to instruct the model
to extract the original text rather than paraphrase, we find that
hallucination is not a major issue with Claude and Gemini in this
application. Since LLM outputs are inherently probabilistic, we also
conduct sensitivity analyses on reproducibility across runs and model
providers. The output is generally stable: repeated runs with the Gemini
produce consistent results, and different models extracted a similar
number of decisions.

While we optimize the prompt for decision extraction in this work, an
alternative approach is to fine-tune a local model to enhance LLM
performance. A catered local model could be useful for extraction
decisions for a comprehensive literature reviews on a larger scale, but
it would require greater model training efforts with labeled data.

\subsection{Extracting other types of
decisions}\label{extracting-other-types-of-decisions}

In this work, we focus on modeling decisions for the baseline model in
the air pollution epidemiology literature. Analyses in this field often
fit multiple models for different health outcomes and use secondary
models, such as distributed lag models and multi-pollutant models, to
estimate relative risks and multi-pollutants interactions. These
increase the complexity of decision extraction with LLMs because authors
often only describe the differences from the baseline specification,
implicitly assuming other decisions remain unchanged. Hence, LLMs will
need to link the decisions across different models and reconstruct the
complete set of decisions for each model.

Beyond modeling choices, decisions in data pre-processing are also
interesting to compare. For example, \citet{braga2001} aggregated air
pollution measures from multiple PM10 monitors within the same location
into a single value. Pre-processing choices such as data source,
aggregation method, imputation, also have impact on uncertainty of the
estimated effect size of particulate matters. However, these decisions
are often not properly and adequately described in the manuscript,
making it impossible to extract by LLMs. Proper documentation and
reporting standard of in pre-processing decisions are needed before our
workflow could be applied to pre-processing decisions.

With growing advocacy for reproducibility, papers nowadays are expected
to share code and data, if applicable. Code availability provides a
useful supplementary source for identifying decision choices and
cross-checking them against descriptions in the manuscript. However,
while script may reveal what choices were made, the rationale behind
these choices are often not documented under the current practice.

\subsection{Generalizability of the
workflow}\label{generalizability-of-the-workflow}

In principle, our workflow is scalable and generalizable to a random set
of applied papers. However, insights about the data analysis practices
are more likely to be reveals when papers share certain similarities.
For example literature on the same topic but different authors allows
for understanding of common practices within a field, literature using
the same methodology across different disciplines allow comparisons of
the same statistical method across fields; and literature that considers
the same variables can show how those variables are used in different
domains.

Our LLM prompt for extracting decisions will need to be customized for
each application of the workflow. The general prompt structure and the
data schema for recording decisions can be reused, while examples within
the prompt may be adapted to suit the specific application. The shiny
application for interactively validating and standardizing decisions can
be reused across applications. Calculating paper similarity requires
comparing decisions on the same variable and type across paper pairs.
For papers with limited similarities, the number of comparable decisions
may be limited. Diagnostic functions are available to display decisions
side by side or provide summary statistics on the number of comparable
decisions. Uncertainty visualization on the paper similarity score can
be used to highlight the confidence with respect to the number of
comparable decisions.

As a new method for collecting analytic decision data from literature,
our workflow can be connected to meta-analysis to assess how different
decisions influence results. More broadly, it can also be integrated
into literature search and recommender systems to suggest similar papers
based on the analytic decisions they employ.

\section{Conclusion}\label{sec-conclusion}

In this paper, we developed a scalable and generalizable pipeline for
automatically extracting analytical decisions using LLMs from scientific
literature to study how analysts make decisions in data analysis. We
also introduced a method for calculating paper similarity through
comparing the similarities among decision choices and the similarity
metric can be used as a distance to cluster papers by their decision
choices and visualization with dimension reduction algorithms, such as
multidimensional scaling. We applied this pipeline to a set of air
pollution modeling literature that associates daily particulate matter
and daily mortality and hospital admission. From the extracted modeling
decisions, we identify the most common decision choices in this type of
analysis and the paper similarity score calculation revealed the three
clusters of paper corresponding to different smoothing methods.

Many work on studying decision-making in data analysis conduct
qualitative interviews with a small number of analysts to understand
their decision-making process. ``many-analysts'' studies gather together
analysts in a controlled experiment to observe analysts conduct the
analysis. Our approach is also observational in nature, but we
``observe'' analysts in real world problems with real data that have
policy implication, while being scalable and cost-effective to a broader
exploration of decision-making practices in different contexts and
disciplines. Compared to sensitivity analysis or multiverse analysis,
our approach offers a different perspective by pooling together
decisions made in analyses across the field to reveal the options
considered to highlight uncertainty in decisions that require further
sensitivity analyses to assess their impact
\citep{peng2006, touloumi2006}.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

%% begin pandoc before-bib
%% end pandoc before-bib
%% begin pandoc biblio
%% end pandoc biblio
%% begin pandoc include-after
%% end pandoc include-after
%% begin pandoc after-body
%% end pandoc after-body

\end{document}
\endinput
%%
%% End of file `sample-manuscript.tex'.
