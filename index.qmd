---
format: acm-pdf
keep-tex: true
pdf-engine: pdflatex
bibliography: references.bib
title: The Name of the Title Is Hope
short-title: Hope
abstract: bla blabla
notebook-links: false
author:
  - name: H. Sherry Zhang
    email: hsherryzhang@utexas.edu
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
  - name: Roger D. Peng
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  anonymous: true
  # comment this out to build a draft version
  #final: true
  # comment this out to specify detailed document options
  # acmart-options: sigconf, review
  # acm preamble information
  copyright-year: 2025
  acm-year: 2025
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "CHI'26"
  conference-name: CHI Conference on Human Factors in Computing Systems
  conference-date: Apr 13--17, 2026
  conference-location: Barcelona, Spain
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  #shortauthors: Trovato et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
   \begin{CCSXML}
   <ccs2012>
    <concept>
       <concept_id>10010405.10010497.10010504.10010505</concept_id>
       <concept_desc>Applied computing~Document analysis</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
     <concept>
       <concept_id>10010405.10010432.10010437.10010438</concept_id>
       <concept_desc>Applied computing~Environmental sciences</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
      <concept>
       <concept_id>10003120.10003121.10003126</concept_id>
       <concept_desc>Human-centered computing~HCI theory, concepts and models</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   </ccs2012>
   \end{CCSXML}

   \ccsdesc[300]{Applied computing~Document analysis}
   \ccsdesc[500]{Applied computing~Environmental sciences}
   \ccsdesc[500]{Human-centered computing~HCI theory, concepts and models}

  keywords:
    - Large language models
    
  # if uncommented, this produces a teaser figure
  #
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, fig.align = "center", fig.height = 3)
library(tidyverse)
library(dossier)
library(ggdendro)
library(patchwork)
load(here::here("data/text_sensitivity_decision_df.rda"))
load(here::here("data/text_sensitivity_df.rda"))
```

-   Something about "analysis review" - Roger thinks it's a better to have a new word for this.
-   provide a baseline understand - place to start
-   demonstrate - analytically homogeneous - the table won't look like that

# Introduction

Decisions are everywhere in data analysis, from the initial data collection, data pre-processing to the modelling choices. These decisions will impact the final output of the data analysis, which may lead to different conclusions and policy recommendations. When such flexibility can be misused—through practices such as p-hacking, selective reporting, or unjustified analytical adjustments—it can inflate effect sizes or produce misleading results that meet conventional thresholds for statistical significance. They have been demonstrated through many-analysts experiments, where independent teams analyzing the same dataset to answer a pre-defined research question often arrive at markedly different conclusions. These practices not only compromise the validity of individual studies but also threaten the broader credibility of statistical analysis and scientific research as a whole.

Multiple recommendations have been proposed to improve data analysis practices, such as pre-registration and multiverse analysis. Bayesian methods also offer a different paradigm to p-value driven inference for interpreting statistical evidence. Most empirical studies of data analysis practices focus on specially designed and simplified analysis scenarios. While informative, these setups may not adequately capture the complexity of the data analysis with significant policy implications. \[In practice, studying the data analysis decisions with actual applications is challenging.\] Analysts may no longer be available for interviews due to job changes, and even when they are, recalling the full set of decisions and thinking process made during the analysis is often infeasible. Moreover, only until the last decades, analysis scripts and reproducible materials were not commonly required by journals for publishing. \[As a result, it remains challenging to study how analytical decisions are made. \]

In this work, we focus on a specific class of air pollution modelling studies that estimate the effect size of particulate matter (PM2.5 or PM10) on mortality, typically using Poisson regression or generalized additive models (GAMs). While individual modelling choices vary, these studies often share a common structure: they adjust for meteorological covariates such as temperature and humidity, apply temporal or spatial treatments, like including lagged variables and may estimate the effect by city or region before combining results. Because these studies investigate similar scientific questions using a shared modelling framework, they form a natural many-analyst setting. This allows us to examine, in a real-world context, the range of analytical decisions made by different researchers addressing the same underlying question.

<!-- This class of studies has significant impact to provide scientific evidence for to guide public policy on setting the National Ambient Air Quality Standards (NAAQS) for air pollutants in the U.S.  -->

In this work, we develop a structured tabular format to record the analytical decisions made by researchers in the air pollution modelling literature. Using large language models (LLMs), we automate the extraction of these decisions from published papers. This allows us to treat decisions as data – allowing us to track them over time, compare methodology across papers, and query commonly used approaches. We further introduce a workflow to cluster studies based on decision similarity, revealing three distinct groups of papers that reflect the modelling strategies differs in the European and U.S. studies, which offers a new way to visualize the field in the air pollution mortality modelling.

The rest of the paper is organized as follows. In @sec-background, we review the background on data analysis decisions. @sec-extract-decisions describes the data structure for recording decisions, the use of large language models to process research papers, and the validation of LLM outputs. In @sec-paper-similarity, we present the method for calculating paper similarity based on decision similarities. @sec-result reports the finding of our analysis, including the clustering of papers according to similarity scores and sensitivity analyses related to LLM providers, prompt engineering, and LLM parameters. Finally, @sec-discussion discusses the implications of our study.

# Background {#sec-background}

## Decisions in data analysis

**Question** Is "decision" going to be confusing with "decision-making" in decision theory

A data analysis is a process of making choices at each step, from the initial data collection to model specification, and post-processing. Each decision represents a branching point where analysts choose a specific path to follow, and the vast number of possible choices analysts can take forms what @gelman2014 describe as the "garden of forking paths". While researchers may hope their inferential results are robust to the specific path taken through the garden, in practice, different choices can lead to substantially different conclusions. This has been empirically demonstrated through "many analyst experiments", where independent research groups analyze the same dataset to the same answer using their chosen analytic approach. A classic example is @silberzahn2018, where researchers reported an odds ratio from 0.89 to 2.93 for the effect of soccer players’ skin tone on the number of red cards awarded by referees. Similar variability has been observed in structural equation modeling [@sarstedt2024], applied microeconomics [@huntington-klein2021], neuroimaging [@botvinik-nezer2020], and ecology and evolutionary biology [@gould2025]. Many studies have been conducted on a relatively smaller scale to interviews of analysts and researchers on data analysis practices [@kale2019; @alspaugh2019; @liu_understanding_2020], visualization of the decision process through the analytic decision graphics (ADG) [@liu2020]. Recently, @simson2025 describes a participatory approach to decisions choices in fairness ML algorithms.

Software tools have also developed to incorporate potential alternatives in the analysis workflow, including the `DeclareDesign` package [@blair2019] and the `multiverse` package [@multiverse]. The `DeclareDesign` package [@blair2019] introduces the MIDA framework for researchers to declare, diagnose, and redesign their analyses to produce a distribution of the statistic of interest, which has been applied in the randomized controlled trial study [@bishop2024]. The `multiverse` package [@multiverse] provides a framework for researchers to systematically explore how different choices affect results and to report the range of plausible outcomes that arise from alternative analytic paths.

**TODO** Something about the context on air pollution mortality modelling \@ Roger

# Extracting decisions from data analysis {#sec-extract-decisions}

## Decisions in data analysis {#sec-decisions}

Decisions occur throughout the entire data analysis process -- from the selection of variables and data source, to pre-processing steps to prepare the data for modelling, to the model specification and variable inclusion. In this work, we focus specifically on modelling decisions in the air pollution mortality modelling literature. These include the choice of modelling approach, covariate inclusion and smoothing, and specifications of spatial and temporal structure. Consider the following excerpt from @ostro2006:

> Based on previous findings reported in the literature (e.g., Samet et al. 2000), the basic model included a smoothing spline for time with 7 degrees of freedom (df) per year of data. This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation.

This sentence encode the following components of a decision:

-   **variable**: time
-   **method**: smoothing spline
-   **parameter**: degree of freedom (df)
-   **reason**: Based on previous findings reported in the literature (e.g., Samet et al. 2000); This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation.
-   **decision**: 7 degrees of freedom (df) per year of data

The decision above is regarding a certain parameter in the statistical method, we categorize this as a "parameter" type decisions. Other types of decisions - such as spatial modelling structure or the inclusion of temporal lags - may not include an explicit method or parameter, but still reference a variable and rationale, which we will provide further examples below.

To record these decisions, we follow the tidy data principle [@wickham2014], where each variable should be in a column, each observation in a row. In our context, each row represents a decision made by the authors of a paper and an analysis often include multiple decisions. To retain the original context of the decision, we extract the original text in the paper, without paraphrase or summarization, from the paper. Below we present an example of how to structure the decisions made in a paper, using the paper by @ostro2006:

| Paper | ID | Model | variable | method | parameter | type | reason | decision |
|--------|--------|--------|--------|--------|--------|--------|--------|--------|
| ostro | 1 | Poisson regression | temperature | smoothing spline | degree of freedom | parameter | NA | 3 degree of freedom |
| ostro | 2 | Poisson regression | temperature | smoothing spline | degree of freedom | temporal | NA | 1-day lag |
| ostro | 3 | Poisson regression | relative humidity | LOESS | smoothing parameter | parameter | to minimize Akaike's Information Criterion | NA |
| ostro | 4 | Poisson regression | model | NA | NA | spatial | to account for variation among cities | separate regression models fit in each city |

Most decisions in the published papers are not explicitly stated, this could due to the coherence and conciseness of the writing or authors' decision to include only necessary details. Here, we identify a few common anomalies where decisions may be combined or omit certain fields:

1.  **Authors may combine multiple decisions into a single sentence** for coherence and conciseness of the writing. Consider the following excerpt from @ostro2006:

> Other covariates, such as day of the week and smoothing splines of 1-day lags of average temperature and humidity (each with 3 df), were also included in the model because they may be associated with daily mortality and are likely to vary over time in concert with air pollution levels.

This sentence contains four decisions: two for temperature (the temporal lag and the smoothing spline parameter) and two for humidity. These decisions should be structured as separate entries.

2.  **The justification does not directly address the decision choice.** In the example above, the stated rationale ("and are likely to vary over time in concert with air pollution levels") supports the general inclusion of temporal lags but does not justify the specific choice of 1-day lag over alternatives, such as 2-day average of lags 0 and 1 (lag01) and single-day lag of 2 days (lag2). As such, the reason field should be recorded as NA.

3.  **Some decisions may be omitted because they are data-driven**. For instance, @katsouyanni2001 states:

> The inclusion of lagged weather variables and the choice of smoothing parameters for all of the weather variables were done by minimizing Akaike’s information criterion.

In this case, while the method of selection (minimizing AIC) is specified, the actual degree of freedom used is not. Such data-driven decisions may be recorded with "NA" in the decision field, but the reason field should still be recorded as "by minimizing Akaike’s information criterion"

4.  **Information required to interpret the decision may be distributed across multiple sections**. In the previous example, "weather variables" refers to mean temperature and relative humidity, as defined earlier in the text. This requires cross-referencing across sections to identify the correct variables associated with each modeling choice.

<!-- | Paper | ID | Model | variable | method | parameter | type | reason | decision | -->

<!-- |-----|--|--------|--------|--------|--------|--------|--------|--------| -->

<!-- | ostro | xx | Poisson regression | temperature | smoothing spline | degree of freedom | parameter | NA | 3 df | -->

<!-- | ostro | xx | Poisson regression | temperature | NA | NA | temporal | NA | 1-day lag | -->

<!-- | ostro | xx | Poisson regression | relative humidity | smoothing spline | degree of freedom | parameter | NA | 4 df | -->

<!-- | ostro | xx | Poisson regression | relative humidity | NA | NA | temporal | NA | 1-day lag | -->

<!-- | katsouyanni2001 | xx | generalized additive models (GAM) Poisson regression | mean temperature | smoothing spline | degree of freedom | parameter | by minimizing Akaike’s information criterion | NA | -->

<!-- | katsouyanni2001 | xx | generalized additive models (GAM)  Poisson regression | relative humidity | smoothing spline | degree of freedom | parameter | by minimizing Akaike’s information criterion | NA | -->

## Automatic reading of literature with LLMs

**TODO**: Prompt engineering: these models may paraphrase or hallucinate unless explicitly told not to since it is generative in nature based on the predicted probability of the next word from the text and the instruction

While decisions can be extracted manually from the literature, this process is labor-intensive and time-consuming. Recent advances in Large Language Models (LLMs) have demonstrated potential for automating the extraction of structured information from unstructured text \[ref\]. In this work, we use LLMs to automatically identify decisions made by authors during their data analysis processes.

Text recognition from PDF document relies on Optical Character Recognition (OCR) to convert scanned images into machine-readable text -- capability currently offered by Antropic Claude and Google Gemini. We instruct the LLM to generate a markdown file containing a JSON block that records extracted decisions, which can then be read into statistical software for further analysis. The exact prompt feed to the LLM is provided in the Appendix. The `ellmer` package [@ellmer] in R is used to connect to the Gemini and Claude API, providing the PDF attachment and the prompt in a markdown file as inputs.

## Review the LLM output

-   **TODO** something about result validation of LLM output

The shiny app is designed to provide users a visual interface to review and edit the decisions extracted by the LLM from the literature. The app allows three actions from the users: 1) *overwrite* -- modify the content of a particular cell, equivalently `dplyr::mutate(xxx = ifelse(CONDITION, "yyy" , xxx))`, 2) *delete* -- remove a particular cell, `dplyr::filter(!(CONDITION))`, and 3) *add* -- manually enter a decision, `dplyr::bind_rows()`. @fig-shiny illustrates the *overwrite* action in the Shiny application, where users interactively filter the data and preview the rows affected by their edits—in this case, changing the model entry from "generalized additive Poisson time series regression" to the less verbose "Poisson regression". Upon confirmation, the corresponding `tidyverse` code is generated, and users can download the edited table and incorporate the code into their R script.

```{r}
#| echo: false
#| label: fig-shiny
#| out.width: 100%
#| fig-cap: 'The Shiny application interface for editting Large Language Model (LLM)-generated decisions (overwrite, delete, and add). (1) the default interface after loading the input CSV file. (2) The table view will update interactively upon the user-defined filter condition -- expressed using `dplyr::filter()` syntax (e.g., `paper == anderson2008size"`), (3) The user edits the `model` column to "Poisson regression" and applies the change by clicking the Apply changes button. The table view updates to reflect the changes (4) After clicking the Confirm button, the corresponding `tidyverse` code is generated, and the table view returns to its original unfiltered view. The edited data can be downloaded by clicking the Download CSV button.'
knitr::include_graphics("figures/shiny.png")
```

# Calculating paper similarity {#sec-paper-similarity}

Once the decisions have been extracted and validated, this opens up a structured data for analyzing these information. For example, we can compare whether author's choices at different times changes, or across decisions varies at different regions. In this section, we present a method to calculate paper similarity based on the decisions shared in the paper pairs. The goal is to construct a distance metric based on similarity of the decision choice among papers that could be further used for clustering paper based on choices made by different authors in the literature. An overview of the method is illustrated in @fig-similarity-diag.

```{r}
#| fig-cap: "Workflow for calculating paper similarity based on decision choices: (1) standardize variable names, (2) identify most frequent variable-type decisions across all papers, (3) identify papers with at least x identified decisions, (4) calculate decisions similarity score on the *decision* and *reason* fields using transformer language models, e.g. BERT, (5) calculate paper similarity score based on aggregating decision similarity scores."
#| out-width: "100%"
#| label: fig-similarity-diag
knitr::include_graphics("figures/similarity-diag.png")
```

-   **TODO** some discussion on what it means by for two papers to be similar based on decisions.

The calculation of paper similarity is based on the similarity of decisions shared by each paper pair. A decision comparable in two papers are the ones that share the same variable and type, e.g. temperature and parameter (a decisions on the choosing the statistical method *parameter* for the *temperature* variable), or humidity and temporal (any *temporal* treatment, e.g. choice of lag value for the *humidity* variable). While many decisions share a similar variable, different authors may refer to them with slightly different names, such as "mean temperature" and "average temperature", hence variable names are first standardized to a common set of variable names. For example, "mean temperature" and "average temperature" are both standardized to "temperature". Notice that "dewpoint temperature" is standardized into "humidity" since it is a proxy of temperature to achieve a relative humidity (RH) of 100%. For literature with a common theme, there is usually a set of variables that shared by most papers and additional variables are justified in individual research. For our air pollution mortality modelling literature, we standardize the following variable names:

-   **temperature**: "mean temperature", "average temperature", "temperature", "air temperature", "ambient temperature"
-   **humidity**: "dewpoint temperature" and its hyphenated variants, relative humidity", "humidity"
-   **PM**: "pollutant", "pollution", "particulate matter", "particulate", "PM10", "PM2.5"
-   **time**: "date", "time", "trends", "trend"

Depending on the specific pairs, papers have varied number of decisions that can be compared and aggregated. While paper similarities can be computed for all paper pairs, using the similarity of one or two pair of decisions to represent paper similarity is less ideal. Hence, before calculating the text similarity of decisions, we also include two optional steps to identify and subset the most frequent decisions across papers, and to retain only papers that report more than a certain number of frequent decisions. Research questions in different fields may have different levels of homogeneity, depending on the maturity of the field and for air pollution mortality modelling, it is helpful to focus on decisions and papers that share a substantial number of decisions.

To assign numerical value for the similarity of reason, we use a transformer language model, such as BERT, to measure the semantic text similarity between the decision itself and its justification. The decision similarity is calculated by comparing the *decision* and *reason* fields of the decisions in each paper pair. To obtain paper similarity, we average the decision similarities across all decisions in each paper pair and other method can be customized for aggregation. The resulting paper similarity score can be used as a distance matrix to cluster papers based on their decision choices to understand the common practices in the investigated literature.

# Results {#sec-result}

## Air pollution mortality modelling

The results follows examines \[x\] papers for modelling the effect of particulate matters on mortality based on Gemini for parsing the decision choices. The results from Anthropic Claude is reported in @sec-sensitivity.

Specify how much of validation and review has been done

Decision quality summary

-   missingness of the reason and decisions for the paper - how often papers report decisions
-   look at for one type of decision (time) - what are the choices made by different papers
-   look at whether decisions changes across time (cluster diagram with year)
-   Visualize the decision database: apply clustering algorithm and visualize the clusters
-   a characterization of the field, what are the common variables included, what smoothing methods are used, what are the options for temporal lags often considered, how are models generally estimated spatially.

## Sensitivity analysis {#sec-sensitivity}


sensitivity of the pipeline: 1) LLM, 2) text model, 3) LLM parameters

-   A section on reproducibility of LLM outputs: prompt experiment (see if there are papers discussing this: https://arxiv.org/pdf/2406.06608)

### LLM provider

Input of PDF, require OCR, which is only supported by Antropic Claude and Google Gemini. Comparison of the specification from the their sites
Claude is decoder only, Gemini is an encoder–decoder model

Identify the proportion of the common decision extracted by Claude and Gemini

Compare the clustering by Claude and Gemini

* Claude tends to give more robust output while Gemini has a higher accuracy.

* Given examples of the failure of LLM models for parsing and examples where authors are unclear about the delivery

### Text model

The tested language models include 1) standard `BERT` [@devlin2019], 2) `Roberta` [@liu], which is trained on a much larger dataset (160GB v.s. BERT's 15GB), 3) `transformer-xl` [@dai], 4) `xlnet` by Google Brain [@yang], and two domain-trained BERT models: 5) `sciBert` [@beltagy2019], trained on scientific literature, and 6) `bioBert`[@lee2020], trained on PubMed and PMC data. @fig-text-density presents the distribution of the decision similarity score for each language models and the distribution of paper similarity scores. 

```{r}
#| fig.height: 6
#| fig.width: 10
#| label: fig-text-density
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
p1 <- text_sensitivity_df |> 
  filter(!str_detect(decision, "method")) |> 
  ggplot(aes(x = dist, fill = models)) + 
  geom_density(alpha = 0.1) + 
  facet_wrap(vars(models), ncol = 1)

text_distance_df <- text_sensitivity_decision_df |> 
  nest(data = -id) |> 
  mutate(id = as.numeric(id)) |> 
  rowwise() |> 
  mutate(score = list(calc_paper_similarity(data))) |> 
  unnest(score) |> 
  left_join(tibble(id = 1:5, models = models))

p2 <- text_distance_df |> 
  ggplot(aes(x = similarity, fill = models)) + 
  geom_density(alpha = 0.1)

(p1 | p2) + plot_layout(guides='collect') &
  theme_bw() + 
  theme(legend.position='bottom') 
```

### LLM parameters

The text generation process of the LLMs produce a sequence of tokens based on the input text and the prompt. Model parameters such as temperature, top-p, and top-k control the randomness of the output. A higher temperature value yield more diverse outputs, while a lower value produces more conservative ones. For each Gemini extraction, we set the temperature to 1 and performed five repetitions run to assess the output stability. Among the 62 papers investigated, 31 (50%) yields a consistent number of extracted decisions across all five runs as in @tbl-gemini-1. In 29 papers, two different counts of decisions are observed. For the remaining two papers with three versions, we investigate them separately. In the case of @ito2006, variation arose because Gemini sometimes includes a collection of weather models investigated for sensitivity analysis. For @huang2009, differences come from how the term "smoothing function" was sometimes extracted as the method instead of the actual method, "penalized spline" for temperature and humidity. Depending on the run, Gemini would extract "smoothing function", "penalized spline", or both.

We further investigate the text similarity in the "reason" and "decision" fields for the 31 papers with the same number of decisions across runs and @tbl-gemini-2 tabulates the number of differences found in each pairwise comparison. Fifteen (48.4%) produce the identical text in both fields across all ten cross comparisons (5 runs yield 10 comparisons). Other showed differences in up to 11 comparisons. The primary source of variation were differing extraction amounts and failure to capture the "reason" field. For example, @kan2007 has the most different (11) and the main differences are

> singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average **of current and previous day concentrations** (lag=01)

vs.

> singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average (lag=01)

and

> a minimum of 1 df per year was required 9 df for cardiovascular mortality

vs. NA

```{r}
#| label: tbl-gemini-1
#| tbl-cap: "Number of decisions extracted from each paper by Gemini"
all_geminis <- list.files(here::here("data/gemini"), full.names = TRUE) |>
  purrr::map_dfr( ~ .x |> as_decision_tbl() |> arrange(variable)) |>
  mutate(id = str_remove(paper, "-gemini-[0-9]+"),
         reason = ifelse(reason == "NA", NA, reason ),
         decision = ifelse(decision == "NA", NA, decision ),
         ) |>
  mutate(
    reason = str_replace_all(reason, "/|,|-", ""),
    decision = str_replace_all(decision, "/|,|-", ""),
    reason = ifelse(!is.na(reason), 
                    paste0(tolower(substr(reason, 1, 1)), 
                           substr(reason, 2, nchar(reason))),
                    NA)
  )

paper_id <- all_geminis |> group_split(id) |> map_chr(~.x |> pull(id) |> unique())
n_vec <- all_geminis |> 
  group_split(id) |> 
  map_dbl(~.x |> group_by(paper) |> summarize(n = n()) |> distinct(n) |> nrow())

tibble(paper = paper_id, n = n_vec) |> 
  count(n) |> 
  knitr::kable(col.names = c("Num. of different row number", "Count"))
```

```{r eval = FALSE}
diff_df_raw <- paper_id |> map_dfr(function(paper_id) {
  res <- all_geminis |> filter(id == paper_id) |> group_split(paper)
  expand.grid(seq(1, 5, 1), seq(1, 5, 1)) |>
    filter(Var1 < Var2) |>
    rowwise() |>
    mutate(
      cmpr_obj = list(waldo::compare(
        res[[Var1]] |> select(reason, decision),
        res[[Var2]] |> select(reason, decision))), 
      ll = length(cmpr_obj),
      paper = paper_id, 
      same_n = nrow(res[[Var1]]) == nrow(res[[Var2]]))
})

llm_temp_df <- diff_df_raw |> 
  mutate(n_diff = NA) |> 
  mutate(n_diff = ifelse(ll == 2, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 3, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2 + 
                           str_count(cmpr_obj[[3]], "\033\\[33m") + 
                           str_count(cmpr_obj[[3]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 0, 0, n_diff)) |> 
  ungroup() |> 
  select(-cmpr_obj)
#save(llm_temp_df, file = here::here("data/llm_temp_df.rda"))
```

```{r}
#| label: tbl-gemini-2
#| tbl-cap: "Number of differences in the reason and decision fields across Gemini runs for papers with consistent number of decisions across runs"
load(here::here("data/llm_temp_df.rda"))
all_same_length_papers <- llm_temp_df |>
  group_by(paper) |> 
  reframe(a = unique(same_n)) |> 
  count(paper) |> 
  filter(n == 1) |> 
  pull(paper)

llm_temp_df |> 
  filter(paper %in% all_same_length_papers) |> 
  group_by(paper) |> 
  arrange(-n_diff) |> 
  filter(row_number() == 1) |> 
  ungroup() |>  
  count(n_diff) |> 
  mutate(`Proportion (%)` = n / sum(n) * 100) |> 
  rename(`Num. of difference` = n_diff, Count = n) |> 
  knitr::kable(digits = 2)
```


# Discussion {#sec-discussion}

- Address how sensitivity analysis is/ is not relevant

-   Only prompting engineering is used to extract decisions from the literature. We expect that fine-tuning the model on statistical or domain-specific literature to yield more robust performance on the same document, though it would require substantially more training effort.
-   people from the NYU-LMU workshop are interested to have code script attached as well because people can do one thing in the script but report another in the paper - it would be interesting to compare the paper and the script with some syntax extraction.
-   Validation of the output:

the nature of the task: Our task involve a reasoning component in that it requires casual reasoning to identify the decisions made by the authors, and its justification/ rationale, rather than purely summarizing the text through pattern-matching.

# References {.unnumbered}

::: {#refs}
:::
