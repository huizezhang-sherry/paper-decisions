---
format: acm-pdf
keep-tex: true
pdf-engine: pdflatex
bibliography: references.bib
title: "An LLM-based pipeline for understanding decision choices in data analysis from published literature"
abstract: "Decision choices, such as those made when building regression models, and their rationale are essential for interpreting results and understanding uncertainty in an analysis. However, these decisions are rarely studied because tracing every alternatives considered by authors is often impractical, and reworking a completed analysis is generally of limited interest. Consequently, researchers must manually review large bodies of published analyses to identify common choices and understand how choices are made. In this work, we propose a workflow to automatically extract analytic decisions and their reasons from published literature using Large Language Models. Our method also introduces a paper similarity measure based on decision similarity and visualization methods using clustering algorithms. As an example, this workflow is applied to analyses studying the effect of particulate matter on mortality. This approach enables scalable and automated studies of decision choices in applied data analysis, providing an alternative to existing qualitative and interview-based studies. "
notebook-links: false
author:
  - name: H. Sherry Zhang
    email: hsherryzhang@utexas.edu
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
  - name: Roger D. Peng
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  anonymous: true
  # comment this out to build a draft version
  #final: true
  # comment this out to specify detailed document options
  # acmart-options: sigconf, review
  # acm preamble information
  copyright-year: 2025
  acm-year: 2025
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "CHI'26"
  conference-name: CHI Conference on Human Factors in Computing Systems
  conference-date: Apr 13--17, 2026
  conference-location: Barcelona, Spain
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  #shortauthors: Trovato et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
  
   \begin{CCSXML}
   <ccs2012>
    <ceval: <concept>
      <ceval: <concept_id>10003120.10003121</concept_id>
      <ceval: <concept_desc>Human-centered computing~Human computer interaction (HCI)</concept_desc>
      <ceval: <concept_significance>500</concept_significance>
    </eval: </concept>
    <concept>
      <concept_id>10002951.10003317</concept_id>
      <concept_desc>Information systems~Information retrieval</concept_desc>
      <concept_significance>500</concept_significance>
      </concept>
    </eval: </ccs2012>
   \end{CCSXML}

   \ccsdesc[500]{Human-centered computing~Human computer interaction (HCI)}
   \ccsdesc[500]{Information systems~Information retrieval}

  keywords:
    - large language models
    - analytic decision making in data analysis 
    - document similarity
    
  # if uncommented, this produces a teaser figure
  #
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, fig.align = "center", fig.height = 3)
library(tidyverse)
library(dossier)
library(ggdendro)
library(patchwork)
library(text)
load(here::here("data/text_sensitivity_decision_df.rda"))
load(here::here("data/llm_temp_df.rda"))
load(here::here("data/all_geminis.rda"))
load(here::here("data/all_claudes.rda"))
gemini_df <- read_csv(here::here("data/gemini_df.csv")) |> as_decision_tbl()
```

<!-- This approach treats decisions in data analysis as data to analyse, allowing us to track them over time, compare methodology across papers, and query commonly used approaches. -->

<!-- As a new approach to study decision-making practices in data analysis at scale, what we propose can be applied to other domain of interest to understand the choices in the other applied field. This complements existing qualitative and interview-based studies and many analyst experiments that puts together a set of analysts to analyze the same dataset (maybe not impact in applied research). -->

<!-- There are also meta-analysis to synthesize results from multiple studies, but these focus on the results and not consider the decisions made in the analysis. -->

<!-- Conducting qualitative studies through interviews to study how assumptions and decisions are made in data analysis practices takes a significant amount of time and effort, and the findings may not generalize to other contexts. While published research papers may not provide a complete picture of the decision-making process, they do contain valuable information about the choices made by analysts and the rationale behind them. -->

# Introduction

TODO: need references

Decisions are made at every stage of data analysis, from initial data collection, pre-processing to modeling. One might expect well-trained researchers to make similar choices when faced with the same analytical task, yet evidence suggests otherwise. Many-analyst experiments show that independent analysts often arrive at markedly different conclusions, even when analyzing the same dataset to answer the same research question [@silberzahn2018; @botvinik-nezer2020; @gould2025]. This variation in analytical decision-making, described by @gelman2014 as the "garden of forking paths," can undermine the quality and credibility of reported results and hinder comparability across studies. For junior researchers who lack guidance, this variability may lead to over reliance on default statistical software settings or arbitrary choices made without clear justification.

A common approach to investigate uncertainty in analytical decisions is sensitivity analysis, where researchers systematically vary key decisions in their analysis to assess the robustness of their findings. Multiverse analysis extends this idea by evaluating *all* plausible combinations of decision choices to examine how results vary across the full decision space [@multiverse; @blair2019]. However, what an analyst consider reasonable may not reflect the full range of options used in practice. Even when a reasonable set of alternatives is tested, the sensitivity analysis may be of limited interest to other researchers facing a similar problem, who are seeking evidence to inform comparable decision. Ideally, decision-making in applied research would be studied by following experienced analysts throughout the entire analysis process to capture their reasoning. In reality, this is rarely feasible and not scalable. While individual studies may not capture the full range of decision choices used in practice, crowdsourcing decisions from a collection of studies on a shared theme creates a "many-analyst" setting that reveals how analysts make choices and justify them in practice. Classic research training typically involves reading through the literature to learn the common choices and to understand how decisions are made. This process now has the possibility to be automated at scale given recent advance in information extraction with Large Language Models' (LLMs) [@harrod2024; @katz2024; @farzi2024; @hu2024; @sciannameo2024; @gu2025; @schilling-wilhelmi2025; @gupta2024; @li2024; @baddour2024; @polak2024]. 

In this work, we propose a new approach for studying data analysis decisions by automatically extracting decisions from scientific literature using LLMs. We develop a tabular schema to record decisions, automate the extraction process with LLMs, and introduce a new paper similarity measure based on decision similarity. This similarity measure can serves as a distance metric in dimension reduction methods to visualize papers according to their decisions. We apply this workflow to a set of `r length(unique(gemini_df$paper))` air pollution modeling studies that estimate the effect of particulate matter (PM2.5 or PM10) on mortality and hospital admissions. This type of studies is typically analyzed using Poisson generalized linear models (GLMs) or generalized additive models (GAMs). Analysis of the extracted decisions reveals common choices in this class of studies, such as the number of knots or degree of freedom for smoothing methods and the temporal lags for time and weather variables. Multi-dimensional scaling on the paper similarity distance finds three distinct clusters corresponding to different smoothing methods: LOESS, natural spline, and smoothing spline. These findings align with the APHENA project [@APHENA], which synthesizes research from multiple studies in Europe and North America led by expert investigators.

In this workflow, we also provide detailed documentation on the validation and standardization of LLM outputs. We outline the validation and standardization process, including the use of a developed Shiny application in R for reviewing decisions and the types of edits made through validation. We also use a secondary LLM to standardize reported choices of temporal lag decisions. Additionally, we conduct sensitivity analysis on reproducibility across runs and model providers. future studies for information extraction task with LLMs.

In summary, the contribution of this work includes:

-   A scalable and automated approach to study data analysis decisions through extracting of decisions from published scientific literature using LLMs,

-   A new method to construct paper similarities based on decision choices and the semantic similarity of their rationales,

-   Practices for validating and standardizing LLM outputs, including a shiny GUI tool for editing outputs, the use of secondary LLM for standardizing unstructured response, and sensitivity analysis on reproducibility across runs and model providers, 

-   A data schema for recording decisions in data analysis in a tidy format, and

-   A dataset of decisions, along with metadata, compiled from `r length(unique(gemini_df$paper))` studies in air pollution mortality modeling literature.

<!-- The rest of the paper is organized as follows. In @sec-background, we review the background on data analysis decisions. @sec-extract-decisions describes the data structure for recording decisions, the use of large language models to process research papers, and the validation of LLM outputs. In @sec-paper-similarity, we present the method for calculating paper similarity based on decision similarities. @sec-result reports the finding of our analysis, including the clustering of papers according to similarity scores and sensitivity analyses related to LLM providers, prompt engineering, and LLM parameters. Finally, @sec-discussion discusses the implications of our study. -->

# Related work {#sec-background}

## Analytic decision making in data analysis

Data analysis is a complex and iterative process [@jun2022; @jun2022hypothesis; @jun2019] that involves multiple stages, including data collection, data cleaning, visualization, modeling, and communication. At each stage, analysts make decisions informed by domain practices, statistical knowledge, and the data. These decisions, such as which variables to include in a model, how to handle missing data, and how hyper-parameters are chosen, act as branching points in the analysis workflow. [TODO]The full set of possible paths through these branching points form what @gelman2014 describe as the "garden of forking paths". While one might expect well-trained researchers to make similar choices when facing similar decisions, empirical evidence suggests otherwise. "Many analyst experiments" show that independent research groups analyzing the same dataset to address the same research questions can arrive at widely different conclusions. For example, @silberzahn2018 asks 29 groups of analysts to conduct an analysis to address the same research questions *whether soccer players with dark skin tone are more likely than those with light skin tone to receive red cards from referees*. Researchers reported an estimated effect size from 0.89 to 2.93 in odds ratio with 21 unique combinations of covariates are used among all 29 analyses. 70% of the teams found a statistically significant positive effect while others don't. This great discrepancy among researchers when performing data analysis task is also observed in other domains, for example, structural equation modeling [@sarstedt2024], applied microeconomics [@huntington-klein2021], neuroimaging [@botvinik-nezer2020], and ecology and evolutionary biology [@gould2025].

Examples like the above illustrate how analytical decisions introduce uncertainty into data analysis. These uncertainties have been widely discussed in the literature given their impact for policy recommendation [@APHENA] and [TODO] applications in health, finance, fairness machine learning [@simson2025]. Through experiments, research has shown that analysts' decisions can lead to p-hacking and inflated effect size, when not properly used [@wicherts2016; @simmons2011]. Hence, guidelines and checklists have been developed to recommend the best practice to guide statistical analysis. In medicine and biostatistics, pre-registration is a common practice to regulate analysts making decisions after seeing the data [@van2015]. Given the nuanced nature of data analysis, more work have examined how analysts make decisions in practice through interviews in both academia and industry. These studies include qualitative analysis of the decisions made [@kale2019; @liu2020], interviews with data analysts about exploratory data analysis practice in industry [@alspaugh2019; @kandel2012] and about how they consider alternatives in data analysis [@liu_understanding_2020]. 

In addition to qualitative studies, software tools have developed to help researchers account for alternatives and uncertainties  and make informed decisions in data analysis. Examples include `Tea` [@jun2019], which support general statistical analysis; `Tisane` [@jun2022], which guides choices in generalized linear mixed-effects models (GLMMs); and `MetaExplore` [@kale2023], which account for epistemic uncertainty (decision uncertainty) in meta-analysis. The `DeclareDesign` package [@blair2019] proposes the MIDA framework for researchers to declare, diagnose, and redesign their analyses with account for uncertainties of reporting the statistic of interest. Multiverse analysis proposes a different method to allow researchers to evaluate *all* plausible combinations of decision choices to examine how results vary in the full decision space. Work has been done on the software tools to support multiverse analysis [@multiverse; @götz2024] and visualization of multiverse results [@liu2021], and debugging tools [@gu2023].


## Automatic information extraction with LLMs

In natural language processing, information extraction is a task focus on extracting structured information from unstructured text. Earlier approaches in information extraction tasks relied on rule-based systems and regular expressions. More recent advances, including conditional random fields [@lafferty], word embeddings such as word2vec [@mikolov2013], and transformer-based architectures like BERT [@devlin2019], have led to the current use of LLM to extract information with prompts. Using LLMs to extract unstructured text offers the advantage of automating the process at scale. Applications have been seen in epidemiology data [@harrod2024], scientific literature [@katz2024], clinical data [@farzi2024; @hu2024; @sciannameo2024; @gu2025], chemistry knowledge [@schilling-wilhelmi2025], and polymer science [@gupta2024], climate extreme impact [@li2024], phenotypes [@baddour2024], and material properties [@polak2024]. An easier task in information extraction is called Named Entity Recognition (NER) to identify short span information (1-4 tokens) like person names and locations from unstructured text [@nadeau2007]. An example of this is extracting patient's information and vitals  in clinical data. Extracting decisions from published literature is a more general task than NER, since justification of a decision typically spans more than just a few words. Our task also requires linking information across sentences, sometimes sections, to correctly identify the variables a decision refers to. 


## Visualization on scientific literature

With the growing volume of scientific publications and the difficulty of navigating the literature, there is an increasing interest in developing systems to visualize and recommend scientific papers. These systems link papers based on their similarity and relevance, typically determined by keywords [@isenberg2017], citation information [@chen2006], e.g. citation list and co-citation, or combinations with other relevant paper metadata [@bethard2010; @chou2011; @dörk2012; @heimerl2016], e.g. author and title. Recent approaches incorporate text-based information using topic modeling [@alexander2014], argumentation-based information retrieval [@tbahriti2006], and text embedding [@narechania2022]. While metadata and high-level text-based information are useful for finding relevant papers, researchers also need tools that help them *make sense* of the literature rather than simply *locating* it. In applied data analysis, one interest is to understand how studies differ or align in their decision choices. Capturing the decision choices and reasons that justify the choices from analyses enables the calculation of similarity among papers and can pipe into dimension reduction methods and visualization for a global view of analysis practice in the field or recommend similar papers based on decision similarities.

# Methods {#sec-extract-decisions}

In this section, we present the workflow for extracting decisions from published literature using LLMs. We first describe the data structure for recording decisions, followed by the four main steps in the workflow: 1) automatic extraction of decisions from literature with LLMs, 2) validation and standardization of LLM outputs, 3) calculation of paper similarity, and 4) visualization paper similarity using clustering or dimension reduction methods. The section concludes with an illustration summarizing the workflow.

## Record decisions in data analysis {#sec-decisions}

In the study of the health effects of outdoor air pollution, one area of interest is the association between short-term, day-to-day changes in particulate matter air pollution and daily mortality counts. This question has been studied extensively by researchers across the globe and it serves to provide scientific evidence in the US to guide public policy on setting the National Ambient Air Quality Standards (NAAQS) for air pollutants. While individual modeling choices vary, these studies often share a common structure: they adjust for meteorological covariates, such as temperature and humidity, include lagged variables to account for temporal correlations, and estimate the effect size by city or region before pooling the results with random effect. This naturally forms a "many-analyst" experiment setting to analyze decisions in air pollution mortality modelling.

Consider the following excerpt from @ostro2006 modeling the association between daily counts of mortality and ambient particulate matter (PM10):

> Based on previous findings reported in the literature (e.g., Samet et al. 2000), the basic model included a smoothing spline for time with 7 degrees of freedom (df) per year of data. This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation.

This sentence encode the following components of a decision:

-   **variable**: time
-   **method**: smoothing spline
-   **parameter**: degree of freedom (df)
-   **reason**: Based on previous findings reported in the literature (e.g., Samet et al. 2000); This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation.
-   **decision**: 7 degrees of freedom (df) per year of data

This decision can be recorded in a tabular format following the tidy data principle [@wickham2014], which states that each variable forms a column and each observation forms in a row. For our purpose, each row represents a decision made in a paper and an analysis often include multiple decisions. We extract the original text in the paper, without paraphrase or summarization. The decision above is a parameter choice of a statistical method applied to the variable *time*. A data analysis may also include other types of decisions, such as temporal or spatial ones, for example, the choice of lagged exposure for certain variables or whether the model is estimated collectively or separated for individual locations. These decisions don't have a specific method or parameter fields, but should still include variable, type (spatial or temporal), reason, and decision fields.

Given the writing style of authors, multiple decisions may be combined in one sentence and certain fields may be omitted. Consider a different excerpt from @ostro2006:

> Other covariates, such as day of the week and smoothing splines of 1-day lags of average temperature and humidity (each with 3 df), were also included in the model because they may be associated with daily mortality and are likely to vary over time in concert with air pollution levels.

This sentence contains four decisions: two for temperature (the temporal lag and the smoothing spline parameter) and two for humidity, and should be structured as separate entries:

| Paper | ID | variable | method | parameter | type | reason | decision |
|---------|---------|---------|---------|---------|---------|---------|---------|
| ostro | 1 | temperature | smoothing spline | degree of freedom | parameter | 3 degree of freedom | NA |
| ostro | 2 | relative humidity | smoothing spline | degree of freedom | parameter | 3 degree of freedom | NA |
| ostro | 3 | temperature | NA | NA | temporal | 1-day lags | NA |
| ostro | 4 | relative humidity | NA | NA | temporal | 1-day lags | NA |

Notice in the example above, the reason field is recorded as NA. This is because the stated reason ("and are likely to vary over time in concert with air pollution levels") only supports the general inclusion of temporal lags but does not justify the specific choice of 1-day lag over other alternatives, e.g. 2-day average of lags 0 and 1 or single-day lag of 2 days. Similar scenario can happen when a direct decision choice is missing but a reason is provided, as in @katsouyanni2001:

> The inclusion of lagged weather variables and the choice of smoothing parameters for all of the weather variables were done by minimizing Akaike’s information criterion.

## Extract decisions automatically from literature with LLMs

Manually extracting decisions from published papers is labor-intensive and time-consuming. With LLMs, it is now possible to automatically extract this type of information by supplying a set of PDF documents and a prompt for instruction. Text recognition from PDF document relies on Optical Character Recognition (OCR) to convert scanned images into machine-readable text -- a capability currently offered by Antropic Claude and Google Gemini. In the prompt, we assign the LLM a role as an applied statistician and instruct it extract decisions from the PDF in the format, described in @sec-decisions and write the output in a JSON block in a markdown file. We also provide a set of instructions and examples on the possibility of missing of reason and decision fields as discussed in @sec-decisions. Prompt engineering techniques [@chen2025; @xu] are used to optimize the prompt and the full prompt used in this work is provided in the Appendix. We use the `chat_PROVIDER()` functions from the `ellmer` package [@ellmer] in R to obtain the output.

## Validate and standardize LLM outputs

The LLM outputs need to be validated and standardized before further analysis. Validation focuses on ensuring the extracted decisions are correct, while standardization ensure different expressions of the same variable are standardized into the same expression. For example, the expression *mean temperature*, *average temperature*, and *temperature* all refer to the same variable and are standardized to *temperature*. To help with the validation and standardization process, we developed a Shiny application, which provides an interactive interface for users to review and edit the LLM outputs. The Shiny application takes an input of a CSV file that contains the extracted decisions and allows users to perform three types of edits: 1) *overwrite* -- modify the content of a particular cell, 2) *delete* -- remove an irrelevant decision, and 3) *add* -- manually enter a missing decision. @fig-shiny illustrates the *overwrite* action for standardizing the variable *NCtot* (number concentration of particles \<100 nm in diameter) to *pollution*. The user enters a predicate function in the filter condition box on the left panel, and the filtered data will appear interactively on the right panel. The user can then specify the variable to overwrite and the new value. The corresponding cells on the right panel will be updated. This change need to be confirmed by pressing the "Apply changes" button to update to the full dataset. The corresponding `tidyverse` [@tidyverse] code will then be generated on the left panel to be included in an R script, and the edited table can be downloaded for future analysis.

```{r}
#| echo: false
#| label: fig-shiny
#| out.height: "80%"
#| out.width: "80%"
#| fig-cap: 'The Shiny application interface to validate and standardize Large Language Model (LLM)-generated output. (1) The default interface after loading the input CSV file. (2) The table view will update interactively to reflect the edit: for paper with handle "andersen2008size" and id in 4, 5, 6, modify the variable name *NCtot* to *pollutant*. (3) After clicking the Confirm button, the corresponding `tidyverse` code for the modification is generated, and the table view returns to its original unfiltered view with the edit applied. The edited data can be downloaded by clicking the Download CSV button.'
knitr::include_graphics("figures/shiny.png")
```

## Calculate paper similarity and visualization {#sec-paper-similarity}

Once the output has been extracted and validated, these decisions can be treated as data for further analysis. Apart from exploratory data analysis, we propose a paper similarity measure to compare how similar decisions are between paper pairs. A decision is considered comparable between a paper pair if the two papers share the same variable and decision type, e.g. a parameter decision on temperature. Three factors are considered in calculating the similarity between two matched decisions: 1) whether the two decisions are similar, 2) whether the reasons for the decisions are similar, and 3) for parameter type decisions, whether the statistical methods used are the same. Method and choice similarity indicate the same decision being made in the analysis, whereas a similar reason reflects a shared principle for making the choice, even when the choices themselves may differ due to differences in the underlying data. For reasons and choices, we first obtain the text embedding for all the choices and reasons, and calculate the cosine similarity between the matched reason and decisions from the language model `BERT` using the `text` package [@text] in `R`. For methods, we encode them as a binary variable: 1 if the two papers used the same method, and 0 otherwise because semantic similarity cannot fully capture the difference between statistical methods, e.g., the difference between smoothing spline and natural spline is not well represented by the textual difference of "smoothing" and "natural". The paper similarity is then computed as the average decision similarities across all the matched methods, decisions, and reasons. 

Although paper similarity can be calculated based on all available matched decisions, cares should be taken for pairs with only a small number of matches. This can happen because two papers focus on different variables or some decisions have missing choices or reasons (discussed in @sec-decisions). In practice, users may decide to focus on a set of decisions shared among papers or on papers that report a minimal number of shared decisions when calculating paper similarity.

## Summary

@fig-workflow summarises the whole workflow proposed for extracting and analyzing decisions from published literature using LLMs. Once researchers have identified a set of literature of interest, a prompt is needed to instruct LLMs to extract decisions from these literature. The outputs from LLM need to be validated and standardized before further analysis, due to authors' varied writing styles. The validated data can then be used for exploratory data analysis of decisions and one analysis we propose is to calculate paper similarity. This paper similarity metric can be seen as a distance metric among papers, which can be used for clustering and dimension reduction to visualize the decision patterns among papers.

```{r}
#| label: fig-workflow
#| fig-cap: "The workflow for extracting decisions from published literature using Large Language Models (LLMs) and analyzing the extracted decisions. The workflow consists of four main steps: (1) Extract decisions automatically from literature with LLMs, (2) Validate and standardize LLM outputs, (3) Calculate paper similarity and visualization, and (4) visualization with clustering or dimension reduction methods."
#| out.width: "100%"
knitr::include_graphics(here::here("figures/workflow.png"))
```

# Results {#sec-result}

From the `r length(unique(gemini_df$paper))` studies examining the effect of particulate matters ($\text{PM}_{10}$ and $\text{PM}_{2.5}$) on mortality and hospital admission, we focus on the baseline model reported in each paper, excluding secondary models (e.g. lag-distributed models) and sensitivity analysis. We also exclude decisions on other pollutants, such as nitrogen dioxide ($\text{NO}_2$). This yields `r nrow(gemini_df)` decisions extracted using Gemini, averaging approximately `r round(nrow(gemini_df)/length(unique(gemini_df$paper)))` decisions per paper.

## Validation and standardization of LLM outputs {#sec-res-validation}

```{r}
#| label: tbl-review
#| tbl-cap: "Summary of validation and standardization edits made during the review process. "
aa_raw <- readLines(here::here("clean-gemini.R"))
aa <- aa_raw[35:228]
res <- purrr::map_dfr(
  as.list(aa),
  ~tibble(origin = .x, 
          reason = ifelse(str_detect(.x, "#"),
                          str_replace(.x, ".*#\\s*", ""),
                          NA)
          )
  )

reason_df <- tibble(reason2 = c("recode", "irrelevant", "fail to capture", "duplicates", "general", "definition"),
       reason = c("Edit made to recode smoothing parametser unit to per year",
                  "Remove decisions out of scope: other pollutants and sensitivity analysis",
                  "Fix incorrect capture",
                  "Duplicates",
                  "Edit made due to decisions are too general, e.g. minimum of 1 df per year was required",
                  "Remove decisions related to definition of variables, e.g. season"))

res |> 
  mutate(reason2 = str_extract(reason, "^[^:]+")) |> 
  filter(!is.na(reason2), !reason2 == "category") |>
  count(reason2, sort = TRUE) |> 
  left_join(reason_df) |> 
  select(reason, n) |> 
  janitor::adorn_totals("row") |> 
  knitr::kable(col.names = c("Reason", "Count"))
```

@tbl-review summarizes the number of edits made during the review process using the Shiny application. These edits fall into two main categories: 1) correcting LLM outputs and 2) standardizing extracted decision. The first category includes fixing incorrect captures, removing non-decision (e.g. definition of variables), removing duplication, excluding irrelevant decisions (e.g. sensitivity analyses), and excluding decisions whose stated reasons reflect general guidelines rather than actual choices (e.g. "minimum of 1 degree of freedom per year is required").

Standardization addresses variation in how authors express variable names and decisions. For example, variable names such as "mean temperature" and "average temperature" refer to the same variable and should be aligned for comparison for later decision similarity calculation. Variable names are manually standardized into four main categories:

-   **temperature**: "mean temperature", "average temperature", "temperature", "air temperature", "ambient temperature"
-   **humidity**: "dewpoint temperature" and its hyphenated variants, relative humidity", "humidity"
-   **PM**: "pollutant", "pollution", "particulate matter", "particulate", "PM10", "PM2.5"
-   **time**: "date", "time", "trends", "trend"

Notice that "dewpoint temperature" is standardized under humidity because it serves as a proxy for temperature in achieving a 100% relative humidity.

Decisions themselves also require standardization. For example, the smoothing parameter (number of knots and degree of freedom) may be expressed *per year* or *in total*, and temporal lag decision may be expressed in different formats (e.g. "6-day average", "mean of lags 0+1", "lagged exposure up to 6 days"). Smoothing parameter units are manually recoded to a *per year* basis for consistency, asreflected in @tbl-review. Temporal decision show a wider variety, generally falling into two categories:

-   **multi-day average lags**, such as "6-day average", "3-d moving average", "mean of lags 0+1", "cumulative lags, mean 0+1+2" and
-   **single-day lags**, such as "lagged exposure up to 6 days", "lag days from 0 to 5".

This variability makes manual standardization impractical, hence we apply a secondary LLM process (claude-3-7-sonnet-latest) using the `ellmer` package to convert temporal decisions into a consistent format: `multi-day: lag [start]-[end]` and `single-day: lag [start], … ,lag [end]`. For instance, "6-day average" is converted to "multi-day: lag 0-5" and "lagged exposure up to 6 days" is converted to "single-day: lag 0, lag 1, lag 2, lag 3, lag 4, lag 5".

## Exploratory analysis of decision choices

```{r}
#| label: tbl-missing-decisions
#| tbl-cap: "Missingness of decision and reason fields in the Gemini-extracted decisions. Most decisions report the choice (35.5 + 57.1 = 92%), but 57.1% lacks a stated reason."
dt <- gemini_df |> 
  mutate(decision = is.na(decision), reason = is.na(reason), .keep = "used" ) 
  
dt_tbl <- table(dt)
rownames(dt_tbl)  <- c("Non-missing", "Missing")
pct <- round(prop.table(dt_tbl) * 100, 1)
lab <- matrix(
  paste0(dt_tbl, " (", pct, "%)"),
  nrow = nrow(dt_tbl),
  dimnames = dimnames(dt_tbl)
)

lab |> knitr::kable(
  col.names = c("Decision", "Reason", "Non-missing", "Missing"),
  format = "latex", booktabs = TRUE) |> 
   kableExtra::add_header_above(c("", "Decision" = 2), 
                    escape = FALSE) 
```

As raised in @sec-decisions, not all decisions reported in the literature include both the decision choice and the rationale. Some decisions may only report the choice without a stated reason, while others may provide a reason without specifying the exact choice made. @tbl-missing-decisions summarizes the missingness of the decisions and reason for the extracted decisions. While `r pct <- dt |> count(reason, decision) |> mutate(prop = n / sum(n)) |> filter(reason, decision) |> pull(prop); scales::percent_format()(pct)` of decisions are complete for both decision and reasons, `r pct <- dt |> count(reason, decision) |> mutate(prop = n / sum(n)) |> filter(reason, !decision) |> pull(prop); scales::percent_format()(pct)` of decisions lack a stated rationale for the choice. This reflects a common reporting practice in the field, where authors often present the decision itself without providing a justification, e.g. "We decide to use $x$ degree of freedom for variable $y_1$ and $y_2$". This also includes cases where authors provide general guidelines for selecting the parameter, but the rationale is too broad to justify the specific choice made (hence validated as `NA` in @sec-res-validation).

```{r}
#| label: tbl-most-common-decisions
#| tbl-cap: "Count of variable-type decisions in the Gemini-extracted decisions. The most commonly reported decision are the parameter choices and temporal lags for for time, PM, temperature, and humidity."
count_variable_type(gemini_df) |> head(8) |> 
  knitr::kable(col.names = c("Variable", "Type", "Count"),)
```

@tbl-most-common-decisions lists the eight most frequently reported decision: parameter and temporal choice for time, PM, temperature, and humidity. While a wider list of variables have been used in the analysis, these four variables are most commonly included in baseline models. Parameter choices for time, temperature, and humidity are typically made on the use of smoothing parameter for the smoothing method (natural spline and smoothing spline), whereas temporal choices are commonly reported for PM, temperature, and humidity for the number of lag to consider in the model.

```{r}
#| label: tbl-humidity-temperature-decisions
#| tbl-cap: "Options captured for parameter choices for time, humidity, and temperature variables in the Gemini-extracted decisions. The choices for natural spline knots are generally less varied than the degree of freedom choices for smoothing spline. Choices for temperature and humidity tend to be close, given they are both weather related variables, while the choices for time are more varied inherently."
var_method <- gemini_df |> 
  filter(type == "parameter", 
         variable %in% c("humidity", "temperature", "time"),
         method %in% c("smoothing spline", "natural spline")) |> 
  mutate(decision = str_remove(decision, " knots| df"),
          decision = str_replace(decision, " or", ",")) |> 
  separate_longer_delim(decision, delim = c("; ")) |> 
  filter(!is.na(decision), !decision %in% c("smooth function")) |> 
  mutate(id = row_number()) |> 
  select(paper, variable, method, decision)

var_method_df <- var_method |>
  filter(paper != "schwartz1996daily") |> 
  mutate(decision = parse_number(decision)) |> 
  distinct(variable, method, decision) |> 
  arrange(variable, method, decision) |> 
  mutate(decision = as.character(decision)) |> 
  bind_rows(var_method |> 
              filter(paper == "schwartz1996daily") |> 
              mutate(decision = str_remove(decision, "in each neighborhood")) |> 
              select(-paper) 
            ) |> 
  filter(!is.na(decision))

var_method_df |>
   mutate(method == ifelse(method == "natural spline", "Natural spline (knot)", "Smoothing spline (df)")) |> 
  group_by(method, variable) |> 
  summarize(decision = paste0(decision, collapse = ", ")) |> 
  knitr::kable(col.names = c("Method", "Variable", "Decision")) 
```

@tbl-humidity-temperature-decisions presents the parameter-related decisions extracted for spline methods (natural and smoothing spline) applied to variable time, humidity and temperature. These decisions concern the number of knots or degree of freedom, with all values standardized to a *per year* scale for consistency. The selection of knot for natural spline has less variation than the degree of freedom choices for smoothing spline. Choices for temperature and humidity are generally similar, given they are both weather related variables, whereas choices for time are more varied. This tabulation provides a reference set for common parameter choices for future studies and help to identify anomalies and special treatment in practice. For example, the choice of 7.7 degree of freedom reported in @castillejos2000 may prompt analysts to seek further justification. By cross comparing with other reporting, some decisions appear ambiguous. For example, in @moolgavkar2000 and @moolgavkar2003, the reported value of 30 and 100 degrees of freedom for time may be understandable for experienced domain researcher, it could be unclear for junior analysts as to whether they apply to the full 9 year period or on a per-year basis. We also observe a different report style from @schwartz2000, where smoothing spline parameters are expressed as a proportion of the data ("5% of the data" and "5% of the data") rather than fixed numerical value.

```{r}
#| label: tbl-temporal-decisions
#| tbl-cap: "Options captured for temporal lag choices for PM, temperature, and humidity variables in the Gemini-extracted decisions. Both single-day lags and multi-day average lags are commonly used, generally considering up to five days prior (lag 5)."
temporal_lags <- gemini_df |> 
  filter(type == "temporal") |> 
  filter(!paper %in% c("sheppard1999effects", "zanobetti2003temporal", "breitner2009short")) |>
  separate_longer_delim(decision, delim = "; ") |>
  arrange(variable, decision) |> 
  filter(variable %in% c("PM", "temperature", "humidity")) |> 
  mutate(type = ifelse(str_detect(decision, "multi-day"), "multi-day average", "single-day lag"),
         days = ifelse(type == "multi-day average", 
                       str_remove(decision, "multi-day lag "), 
                       str_remove(decision, "single-day "))) |> 
  separate_longer_delim(days, delim = ", ") |> 
  filter(!days %in% c("1-1", "0-0")) |> 
  mutate(days = ifelse(days == "1-3", "0-3", days)) |> 
  group_by(type, variable) |>
  distinct(variable, type, days) |>
  mutate(
    first = as.numeric(ifelse(str_detect(days, "-"), str_extract(days, "^[0-9]+"), str_extract(days, "^[0-9]+"))),
    last = as.numeric(ifelse(str_detect(days, "-"), str_extract(days, "[0-9]+$"), NA))) |> 
  arrange(variable, first, last) |> 
  summarize(decision = paste0(days, collapse = ", ")) |> 
  ungroup() |> 
  mutate(decision = paste0("lag ", str_remove_all(decision, "lag ")))

temporal_lags |> 
  knitr::kable(col.names = c("Lag type", "Variable", "Decision")) 
```

Similarly, @tbl-temporal-decisions summarizes the temporal lag choices for PM, temperature, and humidity. For single-day lags, the lags are considered up to 13 days (approximately two weeks). For multi-day averages, 3-day and 5-day averages are most common, although other choices such as 2-4 day average are also observed as in @lópez-villarrubia2010:

> In particular, lags 0 to 1 and lags 2 to 4 averages of temperature, relative humidity, and barometric pressure were considered as meteorological variables.

## Paper similarity and clustering

```{r}
#count_variable_type(gemini_df)
df <- gemini_df |> filter_var_type(n = 6)
#count_paper_decisions(df) 
# paper_df <- df |> filter_papers(n_value = 3)
# count_paper_pair_decisions(paper_df) 
# paper_df$paper |> unique()

good_pp <- gemini_df |> count(paper) |> filter(n >= 3) |> pull(paper)
paper_df <- df |> filter(paper %in% good_pp)
```

```{r embed}
embed_df <- paper_df |> compute_text_embed()
distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
distance_df <- distance_decision_df |> calc_paper_similarity()
```

```{r}
#| fig.height: 8
#| fig.width: 8
get_most_common_method <- function(df, cols = c("time_parameter_method", 
                                                "humidity_parameter_method", 
                                                "temperature_parameter_method")) {
  df %>%
    rowwise() %>%
    mutate(method = {
      vals <- c_across(all_of(cols))
      vals <- vals[!is.na(vals)]
      if (length(vals) == 0) NA_character_
      else names(sort(table(vals), decreasing = TRUE))[1]
    }) %>%
    ungroup() |> 
    select(paper, method)
}

method_df <- paper_df |> pivot_decision_tbl_wider() |> get_most_common_method() 

bad <- method_df |>
  filter(!method %in% c("LOESS", "natural spline", "smoothing spline")) |>
  pull(paper)
distance_df <- distance_df |> filter(!paper1 %in% bad & !paper2 %in% bad)
method_df <- method_df |> filter(!paper %in% bad)

dist_m <- to_dist_mtx(distance_df)
hmod <- hclust(dist_m, "ave")
ddata <- dendro_data(hmod)
ddata$labels <- ddata$labels |> left_join(method_df, by = c("label" = "paper"))
p2 <- ggplot() +
  geom_segment(data = segment(ddata), aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(data = ddata$labels, 
            aes(x = x, y = y, label = label, color = method, hjust = 0), 
            size = 3) +
  scale_y_reverse(expand = c(0.2, 0)) + 
  scale_color_brewer(palette = "Dark2") + 
  coord_flip() +
  theme_void() + 
  theme(legend.position = 'bottom')
  

mds_df <- run_mds(distance_df) |> left_join(method_df, by = c("paper" = "paper"))

p3 <- mds_df |> ggplot(aes(x = V1, y = V2)) + 
  ggrepel::geom_label_repel(aes(label = paper, color = method)) + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(aspect.ratio = 1, legend.position = 'bottom') 
```

```{r}
#| fig.height: 13
#| fig.width: 13
#| label: fig-mds
#| fig-cap: "The multi-dimensional scaling (MDS) based on paper similarity distance for `length(good_pp)` air pollution mortality modeling papers, colored by the smoothing method used. The MDS reveals the three distinct groups of papers, corresponds to LOESS, natural spline, and smoothing spline. These groups corresponds to the different modeling strategies debated in the European and U.S. studies, as documented in the APHENA project [@APHENA]."
p3  
```

Given the number of decisions reported in @tbl-most-common-decisions, we focus on the six most common variable-type decisions for calculating paper similarity: parameter choices for time, temperature, and humidity, and temporal lag choices for PM, temperature, and humidity. We also restrict our analysis to papers that report at least three of these six decisions, resulting in `r length(good_pp)` papers for the similarity analysis. This ensures that the paper similarity metric is based on a sufficient number of comparable decisions. We use the default text embedding model (BERT) in the `text` package and cosine similarity to compute the similarity score. Sensitivity analysis on different text embedding model is checked in @sec-text-model. Paper similarity is then calculated as the average of decision similarity for each paper pair. The resulting distance matrix is then used for multi-dimensional scaling (MDS) in @fig-mds. The two MDS dimension reveals three clusters correspond to the three smoothing methods used in these analyses: LOESS, natural spline, and smoothing spline. This grouping aligns with the modeling strategies seen in large-scale analysis, such as the U.S. NMMAPS study [@samet2000] and the European APHEA [@katsouyanni1996] and APHEA2 [@katsouyanni2001] project.

To reconcile these differences, the APHENA project [@APHENA] was launched with the aim to "assess the consistency across Europe and North America when estimated using a common analytic protocol and to explore possible explanations for any remaining variation". While multi-dimensional scaling in @fig-mds shows the match of three clusters with three smoothing methods, this is not inconsistent with the APHENA project [@APHENA] that the amount of smoothing to have a more important role than the method of smoothing for estimating the effect of PM on public health variables. The similarity metric we proposed focuses on the variation of choices across analyses, without directly assessing how those choices influence results. By pooling decision choices from multiple studies with LLMs, it becomes much easier to reveal common practices and difference in research practices, highlighting decisions that require further sensitivity analyses to assess their impact. The different smoothing methods revealed in @fig-mds are consistent with the analysis by @peng2006 and @touloumi2006 that compares different smoothing methods and rationale for selecting smoothing parameters.

## Sensitivity analysis

A series of sensitivity analysis has been conducted to explore the reproducibility for using LLMs for text extraction tasks (@sec-llm-reproducibility), discrepancies in decision extraction between different LLM models: Gemini (`gemini-2.0-flash`) and Claude (`claude-3-7-sonnet-latest`) (@sec-llm-models), and the sensitivity of text model for computing the semantic decision similarity (@sec-text-model).

### LLM reproducibility {#sec-llm-reproducibility}

```{r eval = FALSE}
all_geminis <- list.files(here::here("data/gemini"), full.names = TRUE) |>
  purrr::map_dfr( ~ .x |> as_decision_tbl() |> arrange(variable)) |>
  mutate(id = str_remove(paper, "-gemini-[0-9]+"),
         reason = ifelse(reason == "NA", NA, reason ),
         decision = ifelse(decision == "NA", NA, decision ),
         ) |>
  mutate(
    reason = str_replace_all(reason, "/|,|-", ""),
    decision = str_replace_all(decision, "/|,|-", ""),
    reason = ifelse(!is.na(reason), 
                    paste0(tolower(substr(reason, 1, 1)), 
                           substr(reason, 2, nchar(reason))),
                    NA)
  )

all_claudes <- list.files(here::here("data/claude"), full.names = TRUE) |>
  purrr::map_dfr( ~ .x |> as_decision_tbl() |> arrange(variable)) |>
  mutate(id = str_remove(paper, "-claude-[0-9]+"),
         reason = ifelse(reason == "NA", NA, reason ),
         decision = ifelse(decision == "NA", NA, decision ),
         ) |>
  mutate(
    reason = str_replace_all(reason, "/|,|-", ""),
    decision = str_replace_all(decision, "/|,|-", ""),
    reason = ifelse(!is.na(reason), 
                    paste0(tolower(substr(reason, 1, 1)), 
                           substr(reason, 2, nchar(reason))),
                    NA)
  )


paper_id <- all_geminis |> group_split(id) |> map_chr(~.x |> pull(id) |> unique())
diff_df_raw <- paper_id |> map_dfr(function(paper_id) {
  res <- all_geminis |> filter(id == paper_id) |> group_split(paper)
  expand.grid(seq(1, 5, 1), seq(1, 5, 1)) |>
    filter(Var1 < Var2) |>
    rowwise() |>
    mutate(
      cmpr_obj = list(waldo::compare(
        res[[Var1]] |> select(reason, decision),
        res[[Var2]] |> select(reason, decision))), 
      ll = length(cmpr_obj),
      paper = paper_id, 
      same_n = nrow(res[[Var1]]) == nrow(res[[Var2]]))
})

llm_temp_df <- diff_df_raw |> 
  mutate(n_diff = NA) |> 
  mutate(n_diff = ifelse(ll == 2, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 3, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2 + 
                           str_count(cmpr_obj[[3]], "\033\\[33m") + 
                           str_count(cmpr_obj[[3]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 0, 0, n_diff)) |> 
  ungroup() |> 
  select(-cmpr_obj)
#save(all_geminis, file = here::here("data/all_geminis.rda"))
#save(all_claudes, file = here::here("data/all_claudes.rda"))
#save(llm_temp_df, file = here::here("data/llm_temp_df.rda"))
```

```{r}
all_same_length_papers <- llm_temp_df |>
  group_by(paper) |> 
  reframe(a = unique(same_n)) |> 
  count(paper) |> 
  filter(n == 1) |> 
  pull(paper)

tbl <- llm_temp_df |> 
  filter(same_n) |> 
  count(n_diff) |> 
  mutate(prop = n / sum(n) * 100) 
```

We assess the reproducibility of Gemini's text extraction (`gemini-2.0-flash`) by repeating the task five times for each of the `r length(unique(all_geminis$paper |> str_remove("-gemini-[0-9]")))` papers and perform pairwise comparison between runs. This generates $5 \times 4 /2 \times 62 = 620$ possible comparisons for both "reason" and "decisions" fields. Comparisons where the runs produced a different number of decisions were excluded, as this would require manual alignment. After filtering, `r nrow(llm_temp_df |> filter(!is.na(n_diff)))` out of `r nrow(llm_temp_df)` (`r scales::label_percent()(nrow(llm_temp_df |> filter(!is.na(n_diff)))/nrow(llm_temp_df))`) remained. @tbl-gemini-1 prints the decisions in @andersen2008 across two runs and all the four decisions are identical with no difference.

```{r}
#| label: tbl-gemini-1
#| tbl-cap: "Example comparing Gemini's text extraction for @andersen2008 across two runs. The extracted decisions are identical in both runs."
res <-  all_geminis |> 
    filter(str_detect(paper, "andersen2008size")) |>
    group_split(paper)

tibble(Variable = res[[2]]$variable, 
       Run1 = res[[2]]$decision, 
       Run2 = res[[3]]$decision) |> 
  filter(!row_number() %in% c(1, 2)) |> 
  knitr::kable()
```

```{r}
#| label: tbl-gemini-2
#| tbl-cap: "Number of differences in the reason and decision fields across Gemini runs for papers with consistent number of decisions across runs."
load(here::here("data/llm_temp_df.rda"))
tibble(n_diff = 0:11) |> 
  left_join(tbl) |> 
  replace_na(list(n = 0, prop = 0)) |> 
  rename(`Num. of  difference` = n_diff, Count = n, `Proportion (%)` = prop) |> 
  janitor::adorn_totals() |> 
  knitr::kable(digits = 2)
```

@tbl-gemini-2 summarizes the number of differences observed in each pairwise comparison. Among all comparisons, `r scales::label_percent()(tbl$prop[[1]] / 100)` produce the identical text in reason and decision. The discrepancies come from the following two reasons: 1) Gemini extracted different length for the same decision, e.g. in @kan2007, some runs may extract "singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average **of current and previous day concentrations** (lag=01)", while others extract "singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average (lag=01)". Similarity, for decisions, some runs yield "10 df for total mortality", while other runs yield "10 df". 2) Gemini fails to extract reasons in some runs but not others, e.g. in @burnett1998, the first run generates NAs in the reasons, but the remaining four runs are identical. In @ueda2009 and @castillejos2000 , runs 1 and 5 fail to extract the reasons and produce the same incomplete version, whereas runs 2, 3, and 4 produce accurate versions with reasons populated.

### LLM models {#sec-llm-models}

```{r}
#| label: fig-claude-gemini
#| fig.height: 8
#| fig.width: 8
#| fig-cap: "Comparison of decisions extracted by Claude and Gemini. Each point represents a paper, with the x- and y-axes showing the number of decisions extracted by Claude and Gemini, respectively. The dashed 1:1 line marks where both models extract the same number of decisions. More points fall below this line, suggesting Claude extracts more decisions -- often including noise from data pre-processing or secondary data analysis steps -- which requires additional manual validation."
claude_gemini <- all_claudes |> 
  mutate(paper = str_remove(paper, "-claude-1")) |>
  group_by(paper) |> 
  count(paper) |> 
  left_join(all_geminis |> filter(str_detect(paper, "-gemini-1")) |> 
             mutate(paper = str_remove(paper, "-gemini-1")) |>
             group_by(paper) |> 
             count(paper), by = "paper") |> 
  rename(claude = n.x, gemini = n.y)

claude_gemini |> 
  ggplot(aes(x = claude, y = gemini)) + 
  geom_label(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff >= 6),
            aes(label = paper), nudge_x = 0.2) + 
   geom_point(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff >= 6), color = "red") + 
  geom_jitter(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff < 6),
              width = 0.2, height = 0.2) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  #geom_abline(slope = 1, intercept = -4, color = 'purple') +
  #geom_abline(slope = 1, intercept = 4, color = "purple") +
  theme_bw() + 
  theme(aspect.ratio = 1, panel.grid.minor = element_blank()) + 
  xlab("Num. of decisions extracted by Claude") + 
  ylab("Num. of decisions extracted by Gemini")

```

Reading text from PDF document requires Optical Character Recognition (OCR) to convert images into machine-readable text, which currently is only supported by Antropic Claude (`claude-3-7-sonnet-latest`) and Google Gemini (`gemini-2.0-flash`). We compare the number of decisions extracted by Claude and Gemini across all `r  length(unique(all_geminis$paper |> str_remove("-gemini-[0-9]")))` papers in @fig-claude-gemini. Each point represents a paper, with the x- and y-axes showing the number of decisions extracted by Claude and Gemini, respectively. The dashed 1:1 line marks where both models extract the same number of decisions. While both models extract decisions irrelevant to our analysis, such as sensitivity analyses and secondary analyses, Claude's extractions tend to include more of these irrelevant decisions, examples of these include 1) the definition of "cold day" and "hot day" indicators in @dockery1992 ("defined at the 5th/ 95th percentile"), 2) decisions relate to other pollutants: $\text{NO}_2$, $\text{O}_3$, and $\text{SO}_2$ using a "24 hr average on variable" in @huang2009, and 3) the definition of black smoke and in @katsouyanni2001 for secondary analysis ("restrict to days with BS concentrations below 150 $\mu g/m^2$"). While Gemini also capture these irrelevant decisions, such as "0-4 lag days" for air pollution exposure variables (CO, EC, $\text{K}_S$, $\text{NO}_2$, $\text{O}_3$, OC, Pb, S, $\text{SO}_2$, TC, Zn) in @mar2000. However, these cases are less frequent than Claude's extraction and has been validated and standardized in @sec-res-validation.

For both Claude and Gemini, we find they fail to link the general term "weather variables" to the specific weather variables (e.g. @dockery1992 and @burnett2004 for Gemini and @dockery1992 and @katsouyanni2001 for Claude). Although our prompt specified that some decisions may require linking information across sentences and paragraphs to identify the correct variable, this instruction doesn't appear to be applied consistently.

### Text model {#sec-text-model}

```{r eval = FALSE}
t1 <- Sys.time()
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
text_sensitivity_decision_df <- map_dfr(models,  ~{
  print(.x)
  embed_df <- paper_df |> compute_text_embed(text_model = .x) 
  distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
  
  return(distance_decision_df)
}, .id = "id") 
t2 <- Sys.time()
t2 - t1
#save(text_sensitivity_decision_df, file = here::here("data/text_sensitivity_decision_df.rda"))
```

```{r}
#| fig.height: 6.5
#| fig.width: 9
#| fig.cap: "Distribution of decision similarity (left) and multi-dimensional scaling (MDS) of the paper similarity scores (right) computed for five different text models (BERT, BioBERT, RoBERTa, SciBERT, and XLNet). The default language model, BERT, produces the widest variation across the five models, while the similarity scores form XLNet are all close to 1. The model BioBERT, RoBERTa, and SciBERT yield decision similar scores mostly between 0.7 to 1. All the text models shows a similar clustering structure based on the three main smoothing methods (LOESS, natural spline and smoothing spline)."
#| label: fig-text-model
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
models_df <- tibble(id = 1:5, models = models, name = c("BERT", "RoBERTa", "XLNet", "SciBERT", "BioBERT"))

p1 <- text_sensitivity_decision_df |> 
  mutate(id = as.numeric(id)) |> 
  filter(!str_detect(decision, "method")) |> 
  left_join(models_df) |>
  ggplot(aes(x = dist)) + 
  geom_density(alpha = 0.3, fill = "grey") + 
  facet_wrap(vars(name), ncol = 1, scales = "free_y") + 
  scale_x_continuous(breaks = seq(0.2, 1, 0.1)) + 
  xlab("Decision similarity scores") + 
  theme_bw()

text_distance_df <- text_sensitivity_decision_df |>
  nest(data = -id) |>
  mutate(id = as.numeric(id)) |>
  rowwise() |>
  mutate(score = list(calc_paper_similarity(data))) |>
  unnest(score) |>
  select(-data) |>
  left_join(models_df)

# p2 <- text_distance_df |> 
#   ggplot(aes(x = similarity)) + 
#   geom_density(alpha = 0.3, fill = "grey") + 
#   facet_wrap(vars(name), ncol = 1) + 
#   xlab("Paper similarity scores")

mds_df <- text_distance_df |> 
  filter(!paper1 %in% bad & !paper2 %in% bad) |> 
  group_split(id) |> 
  map_dfr(run_mds, .id = "id") |> 
  mutate(id = as.numeric(id)) |> 
  left_join(method_df, by = c("paper" = "paper")) |> 
  left_join(models_df) |> 
  mutate(V2 = ifelse(name == "BioBERT", -V2, V2))


p3 <- mds_df |> ggplot(aes(x = V1, y = V2, gorup = method)) + 
  geom_point(aes(color = method)) +
  ggforce::geom_mark_hull(concavity = 5, aes(color = method), 
                          expand = unit(2.2, "mm")) + 
  facet_wrap(vars(name), ncol = 2) + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(aspect.ratio = 1, legend.position = "bottom")  

(p1 | p3) + plot_layout(widths = c(1.5, 1), guides = 'collect') &
  theme(legend.position = 'bottom')
```

We have conducted sensitivity analysis on the text model for obtaining the decision similarity score from the Gemini outputs. The tested language models tested include 1) BERT by Google [@devlin2019], 2) RoBERTa by Facebook AI [@liu], trained on a larger dataset (160GB v.s. BERT's 15GB), 3) XLNnet by Google Brain [@yang], and two domain-trained BERT models: 4) sciBERT [@beltagy2019], trained on scientific literature, and 5) bioBERT [@lee2020], trained on PubMed and PMC data.

@fig-text-model shows the distribution of the decision similarity and the corresponding multi-dimensional scaling visualization, where distance are calcualted from the paper similarity for each text model. At decision level, the BERT model produces the widest variation across all five models, while the similarity scores from XLNet are all close to 1. While the raw scores are not directly comparable across models due to the difference in the underlying transformer architecture, the multi-dimensional scaling (MDS) based on paper similarity scores shows a similar clustering pattern corresponding to the three main smoothing methods (LOESS, natural spline, and smoothing spline).

# Discussion {#sec-discussion}

## Large-language models for information extraction

Numerous studies have demonstrated the capability of LLMs in information extraction across domains [@harrod2024; @katz2024; @farzi2024; @hu2024; @sciannameo2024; @gu2025; @schilling-wilhelmi2025; @gupta2024; @li2024; @baddour2024; @polak2024]. Our work applies the LLMs to extract analytic decisions in scientific literature, providing further evidence of their effectiveness for information extraction task. Unlike named entity recognition (NER) in clinical data, our task requires capturing more complex analytical decisions and their justifications, which typically span more than just a few tokens. This also requires linking information across sentences and sometimes sections to correctly identify the variables (e.g., linking "weather" to "temperature" and "humidity").

While the extraction of decisions from literature could be largely automated with LLMs, manual validations remains essential to ensure the quality of the extracted decisions for downstream analysis. Most existing applications evaluate LLMs by comparing their outputs to human-annotated datasets, reporting metrics such as precision, recall, and F1 score. Because this approach depends on labeled data, which is not yet fully automated, it is not yet clear how these outputs should be validated for downstream analysis in practice. In our work, we automate some of the manual validation with a secondary LLM (Claude) to standardize the temporal lag choices into two categories: multi-day averages and single-day lags.

With a default temperature of one and the prompt to instruct the model to extract the original text rather than paraphrase, we find that hallucination is not a major issue with Claude and Gemini in this application. Because LLM outputs are inherently probabilistic, we also conduct sensitivity analyses on reproducibility across runs and model providers. The output is generally stable: repeated runs with the Gemini produces consistent results, and different models extracted a similar number of decisions. 

While we optimize the prompt for decision extraction in this work, an alternative approach is to fine-tune a local model to enhance LLM performance. A catered local model could be useful for extraction decisions for a comprehensive literature reviews on a larger scale, but it would require greater model training efforts with labeled data. 

## Extracting other types of decisions

In this work, we focus on modeling decisions for the baseline model in the air pollution epidemiology literature. Analyses in this fields often fit multiple models for different health outcomes and secondary models, such as distributed lag models and multi-pollutant models, are also commonly used to estimate relative risks and multi-pollutants interactions. These increase the complexity of decision extraction with LLMs because authors often only describe the differences from the baseline specification, implicitly assuming other decisions remain unchanged. Hence, LLMs will need to link the decisions across different models and reconstruct the complete set of decisions for each model.

Beyond modeling choices, decisions in data pre-processing are also interesting to compare. For example, @braga2001 aggregated air pollution measures from multiple PM10 monitors within the same location into a single value. Choices about data source, aggregation, imputation, among others, could also have impact on uncertainty of the estimation. However, these decisions are often less well-documented in the literature than the modeling decisions, hence cannot be extracted by LLMs. Proper documentation and reporting of these decisions in future research are needed before our workflow could be applied to pre-processing decisions.

With growing advocacy for reproducibility, papers nowadays are expected to share code and data, if applicable. Code availability provides a useful supplementary source for identifying decisions and cross-checking them against manuscript description. However, while script may reveal what choices were made, the rationale behind these choices is often not documented under the current practice.

<!-- Spatial decisions are generally not well captured because it often conducted uniformly as estimating the city individually to accommodate city heterogeneity. Some papers only consider a handful of cities, while in larger studies the individual city effects are then pooled together using random effect. -->

## Generalizability of the workflow

In principle, our workflow is scalable and generalizable to a random set of applied papers. However, insights about the data analysis practices are more likely to emerge when papers share certain similarities. For example literature on the same topic but authored by different researchers enables comparisons of practices within a field; literature that use the same methodology across disciplines allow comparisons across fields; and literature that considers the same variables can show how those variables are used in different domains.

The LLM prompt for extracting decisions will need to be customized for each application of the workflow. The general LLM prompt structure and the data schema for recording decisions can be retained, while examples within the prompt may be adapted to suit the specific application. The shiny application for interactively validating and standardizing decisions can be reused across applications. Calculating paper similarity requires comparing decisions on the same variable and type across paper pairs. For papers with limited similarities, the number of comparable decisions may be limited. Diagnostic functions are available to display decisions side by side or provide summary statistics on the number of comparable decisions. Uncertainty visualization could be used to highlight the confidence in the similarity metric based on the number of comparable decisions. 

As a new method for collecting analytic decision data from literature, our workflow can be connected to meta-analysis to assess how different decisions influence results. More broadly, it can also be integrated into literature search and recommender systems to suggest similar papers based on the analytic decisions they employ.

# Conclusion {#sec-conclusion}

In this paper, we aim to study how analysts make decisions in their data analysis practice. While classic interviews are often conducted in small scale with toy examples, we developed a pipeline for automatically extracting decisions using LLMs (Claude and Gemini) from scientific literature. We also introduced a method for calculating paper similarity through comparing the similarities among decisions and the similarity metric can be used as a distance to cluster papers by their decision choices and visualization with dimension reduction algorithms, such as multidimensional scaling. We applied this pipeline to a set of air pollution modeling literature that associates daily particulate matter and daily mortality and hospital admission. From the extracted modeling decisions, we identify the most common decision choices in this type of analysis and the paper similarity score calculation revealed the three clusters of paper corresponding to different modeling strategies. These findings are all consistent with the general understanding of the field, as documented in the APHENA project [@APHENA] and other methodological comparison studies [@peng2006; @touloumi2006].

While sensitivity analyses are commonly used to assess the robustness of findings to different analytical choices, the set of choices tested is often limited and selected subjectively by the authors. Our approach offers a new perspective by pooling decisions made in analyses across studies in the fields. This allows for a holistic account on the alternatives in the field and identification of both consensus and divergence within the field, providing insights for future research and methodological development.

# References {.unnumbered}
