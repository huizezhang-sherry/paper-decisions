---
format: acm-pdf
keep-tex: true
pdf-engine: pdflatex
bibliography: references.bib
title: "Dossier: visualizing/ understanding decision choices in data analysis via decision similarity"
abstract: "Decision choices made during data analysis, along with the reasons motivating them, are center to how results are interpreted and to comparisons across similar studies. However, such decisions -- such as selecting the degree of freedom for a smoothing spline and the rationalebehind them -- are rarely studied, since it is impractical to interview authors for all the alternatives and their motivations or to rerun the analysis under different options. In this work, we propose a workflow to automatically extract analytic decisions from the published literature and organize them into structured data using Large Language Models (Claude and Gemini). The pipeline then calculates paper similarity based on the semantic similarity of these extracted decisions and their reasons, and visualizes the results using clustering algorithms. We apply this workflow to a set of studies on the effect of particulate matter on mortality and hospital admission, conducted by researchers worldwide, which naturally provide alteranative analyses of the same question. Our approach offers an efficient way to study decision-making practices and robustness in data analysis compared with traditional interviews or author-focused sensitivity or multiverse analyses."
notebook-links: false
author:
  - name: H. Sherry Zhang
    email: hsherryzhang@utexas.edu
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
  - name: Roger D. Peng
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
# acm-specific metadata
acm-metadata:
  # comment this out to make submission anonymous
  anonymous: true
  # comment this out to build a draft version
  #final: true
  # comment this out to specify detailed document options
  # acmart-options: sigconf, review
  # acm preamble information
  copyright-year: 2025
  acm-year: 2025
  copyright: acmcopyright
  doi: XXXXXXX.XXXXXXX
  conference-acronym: "CHI'26"
  conference-name: CHI Conference on Human Factors in Computing Systems
  conference-date: Apr 13--17, 2026
  conference-location: Barcelona, Spain
  isbn: 978-1-4503-XXXX-X/18/06

  # if present, replaces the list of authors in the page header.
  #shortauthors: Trovato et al.

  # The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
  # Please copy and paste the code instead of the example below.
  ccs: |
  
   \begin{CCSXML}
   <ccs2012>
    <concept>
       <concept_id>10010405.10010497.10010504.10010505</concept_id>
       <concept_desc>Applied computing~Document analysis</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
    <concept>
       <concept_id>10003120.10003121.10011748</concept_id>
       <concept_desc>Human-centered computing~Empirical studies in HCI</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   </ccs2012>
   \end{CCSXML}

   \ccsdesc[300]{Applied computing~Document analysis}
   \ccsdesc[500]{Human-centered computing~Empirical studies in HCI}

  keywords:
    - Large language models
    
  # if uncommented, this produces a teaser figure
  #
  # teaser:
  #   image: sampleteaser
  #   caption: figure caption
  #   description: teaser description
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = TRUE, echo = FALSE, message = FALSE, 
                      warning = FALSE, fig.align = "center", fig.height = 3)
library(tidyverse)
library(dossier)
library(ggdendro)
library(patchwork)
library(text)
load(here::here("data/text_sensitivity_decision_df.rda"))
load(here::here("data/llm_temp_df.rda"))
load(here::here("data/all_geminis.rda"))
gemini_df <- read_csv(here::here("data/gemini_df.csv")) |> as_decision_tbl()
claude_df <- read_csv(here::here("data/claude_df.csv")) |> as_decision_tbl()
```

<!-- This approach treats decisions in data analysis as data to analyse, allowing us to track them over time, compare methodology across papers, and query commonly used approaches. -->

# Introduction

Decisions are made at every stage of data analysis: from initial data collection and pre-processing to modelling choices. Different decision choices can have a direct impact to the final results, which can lead to different interpretation and policy recommendations that follow. When independent analysts analyzing the same dataset even to answer the same research questions, through many-analysts experiments, they often arrive at markedly different conclusions [@silberzahn2018; @botvinik-nezer2020; @gould2025]. This variability in results can be attributed to the flexibility analysts have in making decisions throughout the data analysis process, which @gelman2014 describe as the "garden of forking paths". When such flexibility is misused, data analysis can lead to p-hacking, selective reporting, inflated effect sizes, and other issues, undermining the quality and credibility of the findings.

\[this is not okay --- Multiple recommendations have been proposed to improve data analysis practices, such as pre-registration and multiverse analysis. Bayesian methods also offer a different paradigm to p-value driven inference for interpreting statistical evidence. Most empirical studies of data analysis practices focus on specially designed and simplified analysis scenarios. While informative, these setups may not adequately capture the complexity of the data analysis with significant policy implications. \[In practice, studying the data analysis decisions with actual applications is challenging.\] Analysts may no longer be available for interviews due to job changes, and even when they are, recalling the full set of decisions and thinking process made during the analysis is often infeasible. Moreover, only until the last decades, analysis scripts and reproducible materials were not commonly required by journals for publishing. --- up till here\]

In this work, we develop a tabular format to record analytical decisions in data analysis and automate the extraction of these decisions from published papers using large language models (Gemini and Claude). The workflow also include a component to calculate paper similarity based on both the decisions and the semantic similarity of their rationales, and use clustering methods to visualize papers according to distance based on decision similarity. We apply this workflow to a set of `r length(unique(gemini_df$paper))` air pollution modelling studies estimating the effect size of particulate matter (PM2.5 or PM10) on mortality and hospital admissions, typically modeled using Poisson generalised linear models (GLMs) or generalized additive models (GAMs). Analysis of the extracted decisions reveals common choices in this type of analysis (number of knots or degree of freedom for smoothing methods for time, temperature and humidity) and find three distinct clusters corresponding to different smoothing methods (LOESS, natural spline, and smoothing spline) used in European and U.S. studies, consistent with findings from the APHENA project.

In summary, the contribution of this work includes:

-   A new approach to study data analysis decision choices through automatic extraction of decisions from scientific literature using LLMs,

-   A dataset of decisions and rationale, along with metadata, compiled from 62 studies in air pollution mortality modelling, and

-   A method to construct paper similarities based on the decisions and the semantic similarity of their rationale.

<!-- The rest of the paper is organized as follows. In @sec-background, we review the background on data analysis decisions. @sec-extract-decisions describes the data structure for recording decisions, the use of large language models to process research papers, and the validation of LLM outputs. In @sec-paper-similarity, we present the method for calculating paper similarity based on decision similarities. @sec-result reports the finding of our analysis, including the clustering of papers according to similarity scores and sensitivity analyses related to LLM providers, prompt engineering, and LLM parameters. Finally, @sec-discussion discusses the implications of our study. -->

# Related work {#sec-background}

## Decision-making in data analysis

Data analysis involves making choices at every step, from initial data collection, data pre-processing to model specification, and post-processing. Each decision represents a branching point where analysts choose a specific path to follow, and the vast number of possible choices analysts can take forms what @gelman2014 describe as the "garden of forking paths". While researchers may hope their inferential results are robust to the specific path taken through the garden, in practice, different choices can lead to substantially different conclusions. This has been empirically demonstrated through "many analyst experiments", where independent research groups analyze the same dataset to address the same research questions with their own chosen analytic approach. A classic example is @silberzahn2018, where researchers reported an odds ratio from 0.89 to 2.93 for the effect of soccer players’ skin tone on the number of red cards awarded by referees. Similar variability has been observed in structural equation modeling [@sarstedt2024], applied microeconomics [@huntington-klein2021], neuroimaging [@botvinik-nezer2020], and ecology and evolutionary biology [@gould2025].

Examples like above have rendered decision-making in data analysis as a subject to study in human computer interaction. To understand how analysts making decisions during data analysis and navigating the garden of forking path, researchers have conducted qualitative interviews with analysts on data analysis practices [@kale2019; @alspaugh2019; @liu_understanding_2020]. Visualization tools have also been explored to communicate the decision process through analytic decision graphics (ADG) [@liu2020]. In fairness machine learning literature, @simson2025 contributed a reusable workflow that supports participatory input to democratize decisions in machine learning algorithms related to fairness, privacy, interpretability and performance. Conducting qualitative studies through interviews to study how assumptions and decisions are made in data analysis practices takes a significant amount of time and effort, and the findings may not generalize to other contexts. While published research papers may not provide a complete picture of the decision-making process, they do contain valuable information about the choices made by analysts and the rationale behind them. With recent advances in Large Language Models (LLMs), it has become possible to automatically extract structured information from unstructured text. This could provide a scalable way to study decision-making practices in data analysis.

On top of qualitative studies, software tools have also developed to incorporate potential alternatives in the analysis workflow. The `DeclareDesign` package [@blair2019] introduces the MIDA framework for researchers to declare, diagnose, and redesign their analyses to produce a distribution of the statistic of interest, which has been applied in the randomized controlled trial study [@bishop2024]. The `multiverse` package [@multiverse; @liu2021] provides a framework for researchers to conduct multiverse analysis to systematically explore how different choices affect results and to report the range of plausible outcomes that arise from alternative analytic paths.

## Visualization on scientific literature

With the growing volume of scientific publications and the difficulty of navigating the literature to stay informed, there is increasing interest in developing tools to visualize and recommend scientific papers. These systems link papers based on their similarity and relevance, typically determined by keywords [@isenberg2017], citation information (e.g. citation list, co-citation) [@chen2006], or combinations with other relevant paper metadata (e.g. author, title) [@bethard2010; @chou2011; @dörk2012; @heimerl2016]. Recent approaches incorporate text-based information using topic modelling [@alexander2014], argumentation-based information retrieval [@tbahriti2006], and text embedding [@narechania2022]. While metadata and high-level text-based information are useful for finding relevant papers, researchers also need tools that help them *make sense* of the literature rather than simply *locating* it. In applied data analysis, one interest is to understand how studies differ or align in their analytical approaches. Capturing the decisions and reasoning expressed in analyses on a shared theme enables the calculation of similarity metrics based on these choice and their underlying rationale, which supports clustering and visualizing paper to identify common practices in the field.

# Methods {#sec-extract-decisions}

TODO: a generic summary of the workflow, maybe an illustration

## Record decisions in data analysis {#sec-decisions}

Consider the following excerpt from @ostro2006 that describes the modelling approach to provide evidence of an association between daily counts of mortality and ambient particulate matter (PM10):

> Based on previous findings reported in the literature (e.g., Samet et al. 2000), the basic model included a smoothing spline for time with 7 degrees of freedom (df) per year of data. This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation.

This sentence encode the following components of a decision:

-   **variable**: time
-   **method**: smoothing spline
-   **parameter**: degree of freedom (df)
-   **reason**: Based on previous findings reported in the literature (e.g., Samet et al. 2000); This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation.
-   **decision**: 7 degrees of freedom (df) per year of data

To record these decisions in a tabular format, we follow the tidy data principle [@wickham2014], which states each variable should be in a column and each observation in a row. For our purpose, each row represents a decision made by the authors in a paper and an analysis often include multiple decisions. To retain the original context of the decision, we extract the original text in the paper, without paraphrase or summarization. The decision choice above is a parameter choice of a statistical method applied to the variable. Analyses also include other types of decisions, such as temporal and spatial treatments, for example, the choice of lagged exposure for certain variables or whether the model is estimated collectively or separated for individual locations. These decisions don't have a specific method or parameter, but should still be recorded with the variable, type (spatial or temporal), reason, and decision fields.

Given the writing style and the quality of the analysis itself, multiple decisions may be combined in one sentence and certain fields, e.g. decision and reason, may be omitted. Consider the following excerpt from @ostro2006:

> Other covariates, such as day of the week and smoothing splines of 1-day lags of average temperature and humidity (each with 3 df), were also included in the model because they may be associated with daily mortality and are likely to vary over time in concert with air pollution levels.

This sentence contains four decisions: two for temperature (the temporal lag and the smoothing spline parameter) and two for humidity and should be structured as separate entries:

| Paper | ID | variable | method | parameter | type | reason | decision |
|---------|---------|---------|---------|---------|---------|---------|---------|
| ostro | 1 | temperature | smoothing spline | degree of freedom | parameter | 3 degree of freedom | NA |
| ostro | 2 | relative humidity | smoothing spline | degree of freedom | parameter | 3 degree of freedom | NA |
| ostro | 3 | temperature | NA | NA | temporal | 1-day lags | NA |
| ostro | 4 | relative humidity | NA | NA | temporal | 1-day lags | NA |

Notice in the example above, the reason field are recorded as NA. This is because the stated rationale ("and are likely to vary over time in concert with air pollution levels") only supports the general inclusion of temporal lags but does not justify the specific choice of 1-day lag over other alternatives, for example, 2-day average of lags 0 and 1 and single-day lag of 2 days. Similar scenario can happen when a direct decision is missing while a reason is provided ("done by minimizing Akaike’s information criterion"), as in @katsouyanni2001:

> The inclusion of lagged weather variables and the choice of smoothing parameters for all of the weather variables were done by minimizing Akaike’s information criterion.

## Extract decisions automatically from literature with LLMs

Manually extracting decisions from published papers is labor-intensive and time-consuming. With Large Language Models (LLMs), it has become possible to automatically extract structured information from unstructured text by supplying a set of PDF documents and a prompt for instruction. Text recognition from PDF document relies on Optical Character Recognition (OCR) to convert scanned images into machine-readable text -- capability currently offered by Antropic Claude and Google Gemini. In the prompt, we assign the LLM a role as an applied statistician and instruct it to generate a markdown file containing a JSON block that extract decisions from the PDF in the format described in @sec-decisions. We also provide a set of instructions and examples on the potential missing of reason and decision fields. Prompt engineering techniques [@chen2025; @xu] are used to optimize the prompt script. The full prompt feed to the LLM is provided in the Appendix. We use the `chat_PROVIDER()` functions from the `ellmer` package [@ellmer] in R to obtain the output with Gemini and Claude API.

## Validate and standardize LLM outputs

The LLM outputs need to be validated and standardized before further analysis. Validation focuses on ensuring the correctness of the extracted decisions by LLMs, while standardization aims to ensure consistency in variable and model names across papers, given authors may express the same concept in different ways. For example, "mean temperature", "average temperature", and "temperature" all refer to the same variable, which can be all standardized to "temperature" for consistency. To help with the validation and standardization process, we developed a Shiny application that provides an interactive interface for users to review and edit the LLM outputs. A Shiny application takes a CSV of extracted decisions as input and allows three types of edits: 1) *overwrite* -- modify the content of a particular cell, 2) *delete* -- remove a particular irrelevant decision, and 3) *add* -- manually enter a missing decision. @fig-shiny illustrates the *overwrite* action for standardizing the variable NCtot (The number concentration of urban background particles \<100 nm in diameter) to "pollution": the user enters a predicate function in the filter condition box on the left panel, and the filtered data will appear interactively in the right panel. The user can then specify the variable to overwrite and the new value and the corresponding cells in the right panel will be updated. This change need to be confirmed by pressing the "Apply changes" button to update the full dataset. The corresponding `tidyverse` [@tidyverse] code will then be generated in the left panel to be included in an R script, and the edited table can be downloaded for future analysis.

```{r}
#| echo: false
#| label: fig-shiny
#| out.height: "80%"
#| out.width: "80%"
#| fig-cap: 'The Shiny application interface to validate and standardize Large Language Model (LLM)-generated output. (1) the default interface after loading the input CSV file. (2) The table view will update interactively to reflect the edit: for paper with handle "andersen2008size" and id in 4, 5, 6, replace the variable NCtot with "pollutant". (3) After clicking the Confirm button, the corresponding `tidyverse` code is generated, and the table view returns to its original unfiltered view with the edits applied. The edited data can be downloaded by clicking the Download CSV button.'
knitr::include_graphics("figures/shiny.png")
```

## Calculate paper similarity and visualization {#sec-paper-similarity}

Once the output has been extracted and validated, the decisions can be treated as data for further analysis. In this section, we construct a distance metric between pairs of papers based on the similarity of their decision choices. This metric can then be used as a distance matrix among papers for clustering, dimension reduction, and visualization.

For each paper pair, a decision is considered comparable if the papers share the same variable and decision type, for example, a parameter decision on temperature or the temporal decision on humidity. For two decisions to be considered similar, both the decision choice and the rationale are taken into account. A similar choice indicates a similar final decisions are made in the analysis, whereas a similar reason reflects a shared rationale or justification for the choice, even when the choices themselves differ, potentially due to differences in the underlying data. To assign numerical value for measuring the similarity, we use the semantic similarity from text model (default to BERT). For parameter type decisions, the statistical method used also contributes to the similarity of the decision. Since semantic similarity cannot fully capture the difference betweenit statistical methods (the difference between smoothing spline and natural spline is not well represented by the textual difference of "smoothing" and "natural"), method similarity is encoded as binary: 1 if the two papers used the same method, and 0 otherwise. The paper similarity is then computed as the average similarity across all the matched methods, decisions, and reasons. The resulting paper similarity metric can be interpreted as a distance measure to cluster and visualize papers based on their decision choices.

Because analyses vary in the decisions they report, the number of matched decisions differs across paper pairs. In practice, some studies may not fully report the decision and reason for every choice made, leading to missing data for the matched decisions. Although paper similarity can be calculated based on all available matched decisions, cares should be taken for pairs with only a small number of matches, as the paper similarity may be overly influenced by one or two decisions. To address this, users may focus on a set of decisions shared across papers and on papers that report a minimal number of these decisions when calculating paper similarity.

# Results {#sec-result}

In the study of the health effects of outdoor air pollution, one area of interest is the association between short-term, day-to-day changes in particulate matter air pollution and daily mortality counts. This question has been studied extensively by researchers across the globe and in the US, it serves to provide scientific evidence for to guide public policy on setting the National Ambient Air Quality Standards (NAAQS) for air pollutants. While individual modelling choices vary, these studies often share a common structure: they adjust for meteorological covariates such as temperature and humidity, apply temporal or spatial treatments, like including lagged variables and may estimate the effect by city or region before combining results. This naturally forms a "many-analyst" experiment setting where different researchers analyze similar data to address the same scientific question and the analyses are documented in published papers.

From the `r length(unique(gemini_df$paper))` studies examining the effect of particulate matters ($\text{PM}_{10}$ and $\text{PM}_{2.5}$) on mortality and hospital admission, we focus on the baseline model reported in each paper, excluding secondary models (e.g. lag-distributed models) and sensitivity analysis. We also exclude decisions on other pollutants, such as nitrogen dioxide ($\text{NO}_2$). This yields `r nrow(gemini_df)` decisions extracted using Gemini, averaging approximately `r round(nrow(gemini_df)/length(unique(gemini_df$paper)))` decisions per paper.

## Validation and standardization of LLM outputs {#sec-res-validation}

```{r}
#| label: tbl-review
#| tbl-cap: "Summary of validation and standardization edits made during the review process. "
aa_raw <- readLines(here::here("clean-gemini.R"))
aa <- aa_raw[35:228]
res <- purrr::map_dfr(
  as.list(aa),
  ~tibble(origin = .x, 
          reason = ifelse(str_detect(.x, "#"),
                          str_replace(.x, ".*#\\s*", ""),
                          NA)
          )
  )

reason_df <- tibble(reason2 = c("recode", "irrelevant", "fail to capture", "duplicates", "general", "definition"),
       reason = c("Edit made to recode smoothing parametser unit to per year",
                  "Remove decisions out of scope: other pollutants and sensitivity analysis",
                  "Fix incorrect capture",
                  "Duplicates",
                  "Edit made due to decisions are too general, e.g. minimum of 1 df per year was required",
                  "Remove decisions related to definition of variables, e.g. season"))

res |> 
  mutate(reason2 = str_extract(reason, "^[^:]+")) |> 
  filter(!is.na(reason2), !reason2 == "category") |>
  count(reason2, sort = TRUE) |> 
  left_join(reason_df) |> 
  select(reason, n) |> 
  janitor::adorn_totals("row") |> 
  knitr::kable(col.names = c("Reason", "Count"))
```

@tbl-review summarizes the number of edits made during the review process using the Shiny application. These edits fall into two main categories: 1) correcting LLM outputs and 2) standardizing extracted decision. The first category includes fixing incorrect captures, removing non-decision (e.g. definition of variables), removing duplication, excluding irrelevant decisions (e.g. sensitivity analyses), and excluding decisions whose stated reasons reflect general guidelines rather than actual choices (e.g. "minimum of 1 degree of freedom per year is required").

Standardization addresses variation in how authors express variable names and decisions. For example, variable names such as "mean temperature" and "average temperature" refer to the same variable and should be aligned for comparison for later decision similarity calculation. Variable names are manually standardized into four main categories:

-   **temperature**: "mean temperature", "average temperature", "temperature", "air temperature", "ambient temperature"
-   **humidity**: "dewpoint temperature" and its hyphenated variants, relative humidity", "humidity"
-   **PM**: "pollutant", "pollution", "particulate matter", "particulate", "PM10", "PM2.5"
-   **time**: "date", "time", "trends", "trend"

Notice that "dewpoint temperature" is standardized under humidity because it serves as a proxy for temperature in achieving a 100% relative humidity.

Decisions themselves also require standardization. For example, the smoothing parameter (number of knots and degree of freedom) may be expressed *per year* or *in total*, and temporal lag decision may be expressed in different formats (e.g. "6-day average", "mean of lags 0+1", "lagged exposure up to 6 days"). Smoothing parameter units are manually recoded to a *per year* basis for consistency, asreflected in @tbl-review. Temporal decision show a wider variety, generally falling into two categories:

-   **multi-day average lags**, such as "6-day average", "3-d moving average", "mean of lags 0+1", "cumulative lags, mean 0+1+2" and
-   **single-day lags**, such as "lagged exposure up to 6 days", "lag days from 0 to 5".

This variability makes manual standardization impractical, hence we apply a secondary LLM process (claude-3-7-sonnet-latest) using the `ellmer` package to convert temporal decisions into a consistent format: `multi-day: lag [start]-[end]` and `single-day: lag [start], … ,lag [end]`. For instance, "6-day average" is converted to "multi-day: lag 0-5" and "lagged exposure up to 6 days" is converted to "single-day: lag 0, lag 1, lag 2, lag 3, lag 4, lag 5".

## Exploratory analysis of decision choices

```{r}
#| label: tbl-missing-decisions
#| tbl-cap: "Missingness of decision and reason fields in the Gemini-extracted decisions. Most decisions report the choice (35.5 + 57.1 = 92%), but 57.1% lacks a stated reason."
dt <- gemini_df |> 
  mutate(decision = is.na(decision), reason = is.na(reason), .keep = "used" ) 
  
dt_tbl <- table(dt)
rownames(dt_tbl)  <- c("Non-missing", "Missing")
pct <- round(prop.table(dt_tbl) * 100, 1)
lab <- matrix(
  paste0(dt_tbl, " (", pct, "%)"),
  nrow = nrow(dt_tbl),
  dimnames = dimnames(dt_tbl)
)

lab |> knitr::kable(
  col.names = c("Decision", "Reason", "Non-missing", "Missing"),
  format = "latex", booktabs = TRUE) |> 
   kableExtra::add_header_above(c("", "Decision" = 2), 
                    escape = FALSE) 
```

As raised in @sec-decisions, not all decisions reported in the literature include both the decision choice and the rationale. Some decisions may only report the choice without a stated reason, while others may provide a reason without specifying the exact choice made. @tbl-missing-decisions summarizes the missingness of the decisions and reason for the extracted decisions. While `r pct <- dt |> count(reason, decision) |> mutate(prop = n / sum(n)) |> filter(reason, decision) |> pull(prop); scales::percent_format()(pct)` of decisions are complete for both decision and reasons, `r pct <- dt |> count(reason, decision) |> mutate(prop = n / sum(n)) |> filter(reason, !decision) |> pull(prop); scales::percent_format()(pct)` of decisions lack a stated rationale for the choice. This reflects a common reporting practice in the field, where authors often present the decision itself without providing a justification, e.g. "We decide to use $x$ degree of freedom for variable $y_1$ and $y_2$". This also includes cases where authors provide general guidelines for selecting the parameter, but the rationale is too broad to justify the specific choice made (hence validated as `NA` in @sec-res-validation).

```{r}
#| label: tbl-most-common-decisions
#| tbl-cap: "Count of variable-type decisions in the Gemini-extracted decisions. The most commonly reported decision are the parameter choices and temporal lags for for time, PM, temperature, and humidity."
count_variable_type(gemini_df) |> head(8) |> 
  knitr::kable(col.names = c("Variable", "Type", "Count"),)
```

@tbl-most-common-decisions lists the eight most frequently reported decision: parameter and temporal choice for time, PM, temperature, and humidity. While a wider list of variables have been used in the analysis, these four variables are most commonly included in baseline models. Parameter choices for time, temperature, and humidity are typically made on the use of smoothing parameter for the smoothing method (natural spline and smoothing spline), whereas temporal choices are commonly reported for PM, temperature, and humidity for the number of lag to consider in the model.

```{r}
#| label: tbl-humidity-temperature-decisions
#| tbl-cap: "Options captured for parameter choices for time, humidity, and temperature variables in the Gemini-extracted decisions. The choices for natural spline knots are generally less varied than the degree of freedom choices for smoothing spline. Choices for temperature and humidity tend to be close, given they are both weather related variables, while the choices for time are more varied inherently."
var_method <- gemini_df |> 
  filter(type == "parameter", 
         variable %in% c("humidity", "temperature", "time"),
         method %in% c("smoothing spline", "natural spline")) |> 
  mutate(decision = str_remove(decision, " knots| df"),
          decision = str_replace(decision, " or", ",")) |> 
  separate_longer_delim(decision, delim = c("; ")) |> 
  filter(!is.na(decision), !decision %in% c("smooth function")) |> 
  mutate(id = row_number()) |> 
  select(paper, variable, method, decision)

var_method_df <- var_method |>
  filter(paper != "schwartz1996daily") |> 
  mutate(decision = parse_number(decision)) |> 
  distinct(variable, method, decision) |> 
  arrange(variable, method, decision) |> 
  mutate(decision = as.character(decision)) |> 
  bind_rows(var_method |> 
              filter(paper == "schwartz1996daily") |> 
              mutate(decision = str_remove(decision, "in each neighborhood")) |> 
              select(-paper) 
            ) |> 
  filter(!is.na(decision))

var_method_df |>
   mutate(method == ifelse(method == "natural spline", "Natural spline (knot)", "Smoothing spline (df)")) |> 
  group_by(method, variable) |> 
  summarize(decision = paste0(decision, collapse = ", ")) |> 
  knitr::kable(col.names = c("Method", "Variable", "Decision")) 
```

@tbl-humidity-temperature-decisions presents the parameter-related decisions extracted for spline methods (natural and smoothing spline) applied to variable time, humidity and temperature. These decisions concern the number of knots or degree of freedom, with all values standardized to a *per year* scale for consistency. The selection of knot for natural spline has less variation than the degree of freedom choices for smoothing spline. Choices for temperature and humidity are generally similar, given they are both weather related variables, whereas choices for time are more varied. This tabulation provides a reference set for common parameter choices for future studies and help to identify anomalies and special treatment in practice. For example, the choice of 7.7 degree of freedom reported in @castillejos2000 may prompt analysts to seek further justification. By cross comparing with other reporting, some decisions appear ambiguous. For example, in @moolgavkar2000 and @moolgavkar2003, the reported value of 30 and 100 degrees of freedom for time may be understandable for experienced domain researcher, it could be unclear for junior analysts as to whether they apply to the full 9 year period or on a per-year basis. We also observe a different report style from @schwartz2000, where smoothing spline parameters are expressed as a proportion of the data ("5% of the data" and "5% of the data") rather than fixed numerical value.

```{r}
#| label: tbl-temporal-decisions
#| tbl-cap: "Options captured for temporal lag choices for PM, temperature, and humidity variables in the Gemini-extracted decisions. Both single-day lags and multi-day average lags are commonly used, generally considering up to five days prior (lag 5)."
temporal_lags <- gemini_df |> 
  filter(type == "temporal") |> 
  filter(!paper %in% c("sheppard1999effects", "zanobetti2003temporal", "breitner2009short")) |>
  separate_longer_delim(decision, delim = "; ") |>
  arrange(variable, decision) |> 
  filter(variable %in% c("PM", "temperature", "humidity")) |> 
  mutate(type = ifelse(str_detect(decision, "multi-day"), "multi-day average", "single-day lag"),
         days = ifelse(type == "multi-day average", 
                       str_remove(decision, "multi-day lag "), 
                       str_remove(decision, "single-day "))) |> 
  separate_longer_delim(days, delim = ", ") |> 
  filter(!days %in% c("1-1", "0-0")) |> 
  mutate(days = ifelse(days == "1-3", "0-3", days)) |> 
  group_by(type, variable) |>
  distinct(variable, type, days) |>
  mutate(
    first = as.numeric(ifelse(str_detect(days, "-"), str_extract(days, "^[0-9]+"), str_extract(days, "^[0-9]+"))),
    last = as.numeric(ifelse(str_detect(days, "-"), str_extract(days, "[0-9]+$"), NA))) |> 
  arrange(variable, first, last) |> 
  summarize(decision = paste0(days, collapse = ", ")) |> 
  ungroup() |> 
  mutate(decision = paste0("lag ", str_remove_all(decision, "lag ")))

temporal_lags |> 
  knitr::kable(col.names = c("Lag type", "Variable", "Decision")) 
```

Similarly, @tbl-temporal-decisions summarizes the temporal lag choices for PM, temperature, and humidity. For single-day lags, the lags are considered up to 13 days (approximately two weeks). For multi-day averages, 3-day and 5-day averages are most common, although other choices such as 2-4 day average are also observed as in @lópez-villarrubia2010:

> In particular, lags 0 to 1 and lags 2 to 4 averages of temperature, relative humidity, and barometric pressure were considered as meteorological variables.

## Paper similarity and clustering

```{r}
#count_variable_type(gemini_df)
df <- gemini_df |> filter_var_type(n = 6)
#count_paper_decisions(df) 
# paper_df <- df |> filter_papers(n_value = 3)
# count_paper_pair_decisions(paper_df) 
# paper_df$paper |> unique()

good_pp <- gemini_df |> count(paper) |> filter(n >= 3) |> pull(paper)
paper_df <- df |> filter(paper %in% good_pp)
```

```{r embed}
embed_df <- paper_df |> compute_text_embed()
distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
distance_df <- distance_decision_df |> calc_paper_similarity()
```

```{r}
#| fig.height: 8
#| fig.width: 8
#| label: fig-cluster-paper-1
#| fig-cap: "The dendrogram (left) and multi-dimensional scaling (MDS) (right) based on paper similarity distance for 62 air pollution mortality modelling literature. The papers are colored by the most common smoothing method used. The MDS reveals the three distinct groups of papers. This grouping corresponds to the modelling strategies differ in the European and U.S. studies, documented in ALPHENA."
get_most_common_method <- function(df, cols = c("time_parameter_method", 
                                                "humidity_parameter_method", 
                                                "temperature_parameter_method")) {
  df %>%
    rowwise() %>%
    mutate(method = {
      vals <- c_across(all_of(cols))
      vals <- vals[!is.na(vals)]
      if (length(vals) == 0) NA_character_
      else names(sort(table(vals), decreasing = TRUE))[1]
    }) %>%
    ungroup() |> 
    select(paper, method)
}

method_df <- paper_df |> pivot_decision_tbl_wider() |> get_most_common_method() 

bad <- method_df |>
  filter(!method %in% c("LOESS", "natural spline", "smoothing spline")) |>
  pull(paper)
distance_df <- distance_df |> filter(!paper1 %in% bad & !paper2 %in% bad)
method_df <- method_df |> filter(!paper %in% bad)

dist_m <- to_dist_mtx(distance_df)
hmod <- hclust(dist_m, "ave")
ddata <- dendro_data(hmod)
ddata$labels <- ddata$labels |> left_join(method_df, by = c("label" = "paper"))
p2 <- ggplot() +
  geom_segment(data = segment(ddata), aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(data = ddata$labels, 
            aes(x = x, y = y, label = label, color = method, hjust = 0), 
            size = 3) +
  scale_y_reverse(expand = c(0.2, 0)) + 
  scale_color_brewer(palette = "Dark2") + 
  coord_flip() +
  theme_void() + 
  theme(legend.position = 'bottom')
  

mds_df <- run_mds(distance_df) |> left_join(method_df, by = c("paper" = "paper"))

p3 <- mds_df |> ggplot(aes(x = V1, y = V2)) + 
  ggrepel::geom_label_repel(aes(label = paper, color = method)) + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(aspect.ratio = 1)  + 
  theme(legend.position = "none")
p2
```

```{r}
#| fig.height: 11
#| fig.width: 11
#| label: fig-cluster-paper
#| fig-cap: "The dendrogram (left) and multi-dimensional scaling (MDS) (right) based on paper similarity distance for 62 air pollution mortality modelling literature. The papers are colored by the most common smoothing method used. The MDS reveals the three distinct groups of papers. This grouping corresponds to the modelling strategies differ in the European and U.S. studies, documented in ALPHENA."
#(p2 | p3) + plot_layout(widths = c(1, 2))
p3 
```

For computing the decision similarity score, we include the first 6 most common variable-type decisions as suggested in @tbl-most-common-decisions. @fig-cluster-paper shows the clustering of the `r length(unique(good_pp))` papers based on the decision similarity scores. The dendrogram is generated using hierarchical clustering, and the labels are colored according to the most common smoothing method used in each paper. The clustering reveals three distinct groups of papers, which reflect the modelling strategies differ in the European (LOESS) and U.S. (...) studies \[more on the APHENA\].

# Discussion {#sec-discussion}

Prompt engineering: these models may paraphrase or hallucinate unless explicitly told not to since it is generative in nature based on the predicted probability of the next word from the text and the instruction.

In this section, we examine the reproducibility for using LLMs for text extraction tasks in @sec-llm-reproducibility, discrepancies between different LLM models: Gemini (`gemini-2.0-flash`) and Claude (`claude-3-7-sonnet-latest`) in @sec-llm-models, and the sensitivity of our paper similarity calculation pipeline to the choice of text model used for computing decision similarity scores in @sec-text-model.

## LLM reproducibility {#sec-llm-reproducibility}

```{r eval = FALSE}
all_geminis <- list.files(here::here("data/gemini"), full.names = TRUE) |>
  purrr::map_dfr( ~ .x |> as_decision_tbl() |> arrange(variable)) |>
  mutate(id = str_remove(paper, "-gemini-[0-9]+"),
         reason = ifelse(reason == "NA", NA, reason ),
         decision = ifelse(decision == "NA", NA, decision ),
         ) |>
  mutate(
    reason = str_replace_all(reason, "/|,|-", ""),
    decision = str_replace_all(decision, "/|,|-", ""),
    reason = ifelse(!is.na(reason), 
                    paste0(tolower(substr(reason, 1, 1)), 
                           substr(reason, 2, nchar(reason))),
                    NA)
  )

diff_df_raw <- paper_id |> map_dfr(function(paper_id) {
  res <- all_geminis |> filter(id == paper_id) |> group_split(paper)
  expand.grid(seq(1, 5, 1), seq(1, 5, 1)) |>
    filter(Var1 < Var2) |>
    rowwise() |>
    mutate(
      cmpr_obj = list(waldo::compare(
        res[[Var1]] |> select(reason, decision),
        res[[Var2]] |> select(reason, decision))), 
      ll = length(cmpr_obj),
      paper = paper_id, 
      same_n = nrow(res[[Var1]]) == nrow(res[[Var2]]))
})

llm_temp_df <- diff_df_raw |> 
  mutate(n_diff = NA) |> 
  mutate(n_diff = ifelse(ll == 2, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 3, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2 + 
                           str_count(cmpr_obj[[3]], "\033\\[33m") + 
                           str_count(cmpr_obj[[3]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 0, 0, n_diff)) |> 
  ungroup() |> 
  select(-cmpr_obj)
#save(all_geminis, file = here::here("data/all_geminis.rda"))
#save(llm_temp_df, file = here::here("data/llm_temp_df.rda"))
```

```{r}
all_same_length_papers <- llm_temp_df |>
  group_by(paper) |> 
  reframe(a = unique(same_n)) |> 
  count(paper) |> 
  filter(n == 1) |> 
  pull(paper)

tbl <- llm_temp_df |> 
  filter(same_n) |> 
  count(n_diff) |> 
  mutate(prop = n / sum(n) * 100) 
```

For our text extraction task, we test the reproducibility of Gemini (`gemini-2.0-flash`) by repeating the text extraction task 5 times for each of the `r length(unique(gemini_df$paper))` papers. For each of the 31 papers, five runs yield $5 \times 4 /2 = 10$ pairwise comparisons per field and including both the “reason” and “decision” fields results in a total of $31 \times 10 \times 2 = 620$ pairs. We exclude the pairs that have different number of decisions since it would require manually align the decision to compare and this left us with `r nrow(llm_temp_df |> filter(!is.na(n_diff)))` out of `r nrow(llm_temp_df)` (`r scales::label_percent()(nrow(llm_temp_df |> filter(!is.na(n_diff)))/nrow(llm_temp_df))`) pairwise comparisons. @tbl-gemini-1 shows an example of such comparison in @andersen2008, where all the four reasons are identical among the two runs, hence a zero number of difference.

```{r}
#| label: tbl-gemini-1
#| tbl-cap: "An example of comparing the text extraction in decisions in Andersen 2008."
res <-  all_geminis |> 
    filter(str_detect(paper, "andersen2008size")) |>
    group_split(paper)

tibble(Variable = res[[2]]$variable, 
       Run1 = res[[2]]$decision, 
       Run2 = res[[3]]$decision) |> 
  filter(!row_number() %in% c(1, 2)) |> 
  knitr::kable()
```

@tbl-gemini-2 summarizes the number of differences observed in each pairwise comparison. Among all comparisons, `r scales::label_percent()(tbl$prop[[1]] / 100)` produce the identical text in reason and decision. The discrepancies come from the following reasons:

-   Gemini extracted different length for the same decision, e.g. in @kan2007, some runs may extract "singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average **of current and previous day concentrations** (lag=01)", while others extract "singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average (lag=01)". Similarity, for decisions, some runs may yield "10 df for total mortality", while other runs yield "10 df". Similar extraction appears in @breitner2009.
-   Gemini fails to extract reasons in some runs but not others, e.g. in @burnett1998, the first run generates NAs in the reasons, but the remaining four runs are identical. In @ueda2009 and @castillejos2000 , runs 1 and 5 fail to extract the reasons and produce the same incomplete version, whereas runs 2, 3, and 4 produce accurate versions with reasons populated.

```{r}
#| label: tbl-gemini-2
#| tbl-cap: "Number of differences in the reason and decision fields across Gemini runs for papers with consistent number of decisions across runs."
load(here::here("data/llm_temp_df.rda"))
tibble(n_diff = 0:11) |> 
  left_join(tbl) |> 
  replace_na(list(n = 0, prop = 0)) |> 
  rename(`Num. of  difference` = n_diff, Count = n, `Proportion (%)` = prop) |> 
  janitor::adorn_totals() |> 
  knitr::kable(digits = 2)
```

## LLM models {#sec-llm-models}

Reading text from PDF document requires Optical Character Recognition (OCR) to convert images into machine-readable text, which currently is only supported by Antropic Claude (`claude-3-7-sonnet-latest`) and Google Gemini (`gemini-2.0-flash`).

We compare the number of decisions extracted by Claude and Gemini across all `r length(unique(gemini_df$paper))` papers in @fig-claude-gemini. Each point represents a paper, with the x- and y-axes showing the number of decisions extracted by Claude and Gemini, respectively. The dashed 1:1 line marks where both models extract the same number of decisions. Most points fall below this line, indicating that Claude extracts more decisions -- often from data pre-processing or secondary data analysis steps requiring more manual validation -- whereas Gemini focuses more on modelling choices relevant to our analysis. Some of these decisions captured by Claude are

-   the definition of "cold day" and "hot day" indicators in @dockery1992 ("defined at the 5th/ 95th percentile"),
-   the choice to summarize $\text{NO}_2$, $\text{O}_3$, and $\text{SO}_2$ using a "24 hr average on variable" in @huang2009, and
-   the definition of black smoke and in @katsouyanni2001 for secondary analysis ("restrict to days with BS concentrations below 150 $\mu g/m^2$").

Gemini sometimes also include irrelevant decisions, such as in @mar2000, where secondary analysis choices like "0-4 lag days" for air pollution exposure variables (CO, EC, $\text{K}_S$, $\text{NO}_2$, $\text{O}_3$, OC, Pb, S, $\text{SO}_2$, TC, Zn) are captured. However, these cases are less frequent, resulting in outputs with less noise overall.

For both Claude and Gemini, we find they fail to link the general term "weather variables" to the specific weather variables. For example Gemini misses this link in @dockery1992 and @burnett2004, while Claude does so in @dockery1992 and @katsouyanni2001. Although our prompt specified that some decisions may require linking information across sentences and paragraphs to identify the correct variable, this instruction doesn't appear to be applied consistently.

```{r eval = FALSE}
#| label: fig-claude-gemini
#| fig.height: 6
#| fig.width: 6
#| fig-cap: "Comparison of decisions extracted by Claude and Gemini. Each point represents a paper, with the x- and y-axes showing the number of decisions extracted by Claude and Gemini, respectively. The dashed 1:1 line marks where both models extract the same number of decisions. Most points fall below this line, suggesting Claude extracts more decisions -- often including noise from data pre-processing or secondary data analysis steps -- which requires additional manual validation."
claude_gemini <- claude_df |> 
  mutate(paper = str_remove(paper, "-claude-1")) |>
  group_by(paper) |> 
  count(paper) |> 
  left_join(gemini_df |> 
           mutate(paper = str_remove(paper, "-gemini-1")) |>
             group_by(paper) |> 
             count(paper), by = "paper") |> 
  rename(claude = n.x, gemini = n.y)

claude_gemini |> 
  ggplot(aes(x = claude, y = gemini)) + 
  geom_label(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff >= 5),
            aes(label = paper), nudge_y = -0.7, nudge_x = 2) + 
   geom_point(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff >= 5)) + 
  geom_jitter(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff < 5),
              width = 0.2, height = 0.2) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  #geom_abline(slope = 1, intercept = -4, color = 'purple') +
  #geom_abline(slope = 1, intercept = 4, color = "purple") +
  theme_bw() + 
  theme(aspect.ratio = 1, panel.grid.minor = element_blank()) + 
  xlab("Num. of decisions extracted by Claude") + 
  ylab("Num. of decisions extracted by Gemini") + 
  xlim(0, 20) + 
  ylim(0, 20)

```

## Text model {#sec-text-model}

We have conducted sensitivity analysis on the text model for obtaining the decision similarity score from the Gemini outputs. The tested language models tested include

1)  BERT by Google [@devlin2019],
2)  RoBERTa by Facebook AI [@liu], trained on a larger dataset (160GB v.s. BERT's 15GB),
3)  XLNnet by Google Brain [@yang], and

two domain-trained BERT models:

4)  sciBERT [@beltagy2019], trained on scientific literature, and
5)  bioBERT [@lee2020], trained on PubMed and PMC data.

@fig-text-density presents the distribution of the decision similarity (left) and paper similarity (right) for each text model. At decision level, the BERT model produces the widest variation across all five models, while the similarity scores from XLNet are all close to 1. These scores are not comparable across models since the difference of the underlying transformer architecture. However, the paper similarity scores from each model are comparable and @fig-text-mds shows the multi-dimensional scaling (MDS) of the paper similarity scores from each text model: all showing a similar clustering pattern of the three main smoothing methods.

```{r eval = FALSE}
t1 <- Sys.time()
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
text_sensitivity_decision_df <- map_dfr(models,  ~{
  print(.x)
  embed_df <- paper_df |> compute_text_embed(text_model = .x) 
  distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
  
  return(distance_decision_df)
}, .id = "id") 
t2 <- Sys.time()
t2 - t1
#save(text_sensitivity_decision_df, file = here::here("data/text_sensitivity_decision_df.rda"))
```

```{r}
#| fig.height: 6
#| fig.width: 10
#| fig.cap: "Distribution of decision similarity (left) and paper similarity (right) scores for five different text models (BERT, BioBERT, RoBERTa, SciBERT, and XLNet). The default language model, BERT, produces the widest variation across the five models, while the similarity scores form XLNet are all close to 1. The model BioBERT, RoBERTa, and SciBERT yield decision similar scores mostly between 0.7 to 1."
#| label: fig-text-density
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
models_df <- tibble(id = 1:5, models = models, name = c("BERT", "RoBERTa", "XLNet", "SciBERT", "BioBERT"))

p1 <- text_sensitivity_decision_df |> 
  mutate(id = as.numeric(id)) |> 
  filter(!str_detect(decision, "method")) |> 
  left_join(models_df) |>
  ggplot(aes(x = dist, fill = name)) + 
  geom_density(alpha = 0.3) + 
  facet_wrap(vars(name), ncol = 1, scales = "free_y") + 
  scale_x_continuous(breaks = seq(0.2, 1, 0.1)) + 
  xlab("Decision similarity scores")

text_distance_df <- text_sensitivity_decision_df |> 
  nest(data = -id) |> 
  mutate(id = as.numeric(id)) |> 
  rowwise() |> 
  mutate(score = list(calc_paper_similarity(data))) |> 
  unnest(score) |> 
  select(-data) |> 
  left_join(models_df)

p2 <- text_distance_df |> 
  ggplot(aes(x = similarity, fill = name)) + 
  geom_density(alpha = 0.3) + 
  facet_wrap(vars(name), ncol = 1) + 
  xlab("Paper similarity scores")

(p1 | p2) + plot_layout(guides = 'collect') &
  theme_bw() + 
  theme(legend.position = 'bottom') 
```

```{r}
#| fig.height: 6
#| fig.width: 10
#| fig.cap: "The multi-dimensional scaling (MDS) of the paper similarity scores from each text model: all showing a similar clustering pattern of the three main smoothing methods. The points are colored by the most common method used in the paper, and the hulls are drawn around each method group."
#| label: fig-text-mds

mds_df <- text_distance_df |> 
  filter(!paper1 %in% bad & !paper2 %in% bad) |> 
  group_split(id) |> 
  map_dfr(run_mds, .id = "id") |> 
  mutate(id = as.numeric(id)) |> 
  left_join(method_df, by = c("paper" = "paper")) |> 
  left_join(models_df) |> 
  mutate(V2 = ifelse(name == "BioBERT", -V2, V2))

mds_df |> ggplot(aes(x = V1, y = V2, gorup = method)) + 
  geom_point(aes(color = method)) +
  ggforce::geom_mark_hull(concavity = 5, aes(color = method), 
                          expand = unit(2.2, "mm")) + 
  facet_wrap(vars(name)) + 
  theme_bw() + 
  theme(aspect.ratio = 1, legend.position = "bottom")  
```

## Others

-   **TODO** something about result validation of LLM output: We also observe data quality with the extraction: for example in @lee2006, the variable recorded is "smoothing parameter". Authors are unclear about the delivery

There are other decisions in an analysis that are worth comparing and documenting. For example data pre-processing decisions, e.g. how pollutant series are defined and collected, treatment on missing values, etc. Again, for a complete review of the field, these decisions ideally would be included, but for our demonstration of idea, we focus on the modelling decisions. Spatial decisions are generally not well captured because it often conducted uniformly as estimating the city individually to accommodate city heterogeneity. Some papers only consider a handful of cities, while in larger studies the individual city effects are then pooled together using random effect.

The variation in the choice of parameters degree of freedom or knot for smoothing can motivate separate investigation on the sensitivity analysis. For instance, parameters that exhibit a wide range of choices across studies may indicate areas of uncertainty or debate within the field, suggesting that further investigation is needed to assess their impact on study outcomes [@peng2006; @touloumi2006].

With LLMs, the extraction of decisions from literature could be largely automated, but manual review is still needed to ensure the quality of the extracted decisions. We also find secondary LLMs can be used to standardize the extracted decisions, such as for temporal lag choices from text expressing this decision in various ways. In this work, we use prompt engineering to optimize the prompt for extracting decisions from general LLMs (Claude and Gemini). Fine-tuning a local model is an alternative approach for a locally-trained model. While it could potentially yield more accurate extraction and hence less manual review, for a systematic literature review, it would require substantially more training efforts and a labelled decision dataset. We also find sometimes the prompt is not fully followed throughout the extraction (example). Claude and Gemini...

Currently, only one model per paper - some have comparison of GLM and GAM, compare different pollutants, stratify by ....

With the advocacy for reproducibility in science, it is expected that more papers will share their code and data. The availability of the code could be a supplementary source for understanding the decisions made in the analysis and cross comparison of the manuscript with the code. However, given the lack of comments in the current practice, we are not there to extract reasons for the decisions encoded in the script.

# Conclusion {#sec-conclusion}

In this paper, \[we study how decisions are made in practical data analysis\]. We developed a pipeline for automatically extracting decisions using LLMs (Claude and Gemini) and introduced a method for calculating paper similarity through decision similarity. This similarity metric enables us to cluster papers by their decision choices and visualization through hierarchical clustering and multidimensional scaling. We applied this pipeline to mortality/ hospital admission -- PM modelling literature and extracted key modelling decisions, such as the choice of smoothing methods and parameters for time, temperature, and humidity, and revealed paper clusters that correspond to different modelling strategies, as documented in the APHENA project.

While sensitivity analyses are commonly used to assess the robustness of findings to different analytical choices, the set of choices tested is often limited and selected subjectively by the authors. Our approach offers a new perspective by pooling decisions made in analyses across studies in the fields. This allows for a holistic account on the alternatives in the field and identification of both consensus and divergence within the field, providing insights for future research and methodological development.

# References {.unnumbered}
