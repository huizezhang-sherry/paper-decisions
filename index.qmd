---
format: 
  tandf-pdf:
    linestretch: 2
keep-tex: true
pdf-engine: pdflatex
bibliography: references.bib
title: "An LLM-based Pipeline for Understanding Decision Choices in Data Analysis from Published Literature"
abstract: "Decision choices, such as those made when building regression models, and their rationale are essential for interpreting results and understanding uncertainty in an analysis. However, these decisions are rarely studied because tracing every alternatives considered by authors is often impractical, and reworking a completed analysis is generally of limited interest. Consequently, researchers must manually review large bodies of published analyses to identify common choices and understand how choices are made. In this work, we propose a workflow to automatically extract analytic decisions and their reasons from published literature using Large Language Models. Our method also introduces a paper similarity measure based on decision similarity and visualization methods using clustering algorithms. As an example, this workflow is applied to analyses studying the effect of particulate matter on mortality. This approach enables scalable and automated studies of decision choices in applied data analysis, providing an alternative to existing qualitative and interview-based studies. "
keywords:
 - decision choices
 - data analysis
 - Large Language Models
notebook-links: false
csl: chicago-author-date.csl
author:
  - name: H. Sherry Zhang
    email: hsherryzhang@utexas.edu
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
  - name: Roger D. Peng
    affiliation:
      name: University of Texas at Austin
      city: Austin
      state: Texas
      country: USA
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, 
                      warning = FALSE, fig.align = "center", fig.height = 3)
library(tidyverse)
library(dossier)
library(ggdendro)
library(patchwork)
load(here::here("data/text_sensitivity_decision_df.rda"))
load(here::here("data/llm_temp_df.rda"))
load(here::here("data/all_geminis.rda"))
load(here::here("data/all_claudes.rda"))
load(here::here("data/distance_df.rda"))
gemini_df <- read_csv(here::here("data/gemini_df.csv")) |> as_decision_tbl()
```

<!-- This approach treats decisions in data analysis as data to analyse, allowing us to track them over time, compare methodology across papers, and query commonly used approaches. -->

<!-- As a new approach to study decision-making practices in data analysis at scale, what we propose can be applied to other domain of interest to understand the choices in the other applied field. This complements existing qualitative and interview-based studies and many analyst experiments that puts together a set of analysts to analyze the same dataset (maybe not impact in applied research). -->

<!-- There are also meta-analysis to synthesize results from multiple studies, but these focus on the results and not consider the decisions made in the analysis. -->

<!-- Conducting qualitative studies through interviews to study how assumptions and decisions are made in data analysis practices takes a significant amount of time and effort, and the findings may not generalize to other contexts. While published research papers may not provide a complete picture of the decision-making process, they do contain valuable information about the choices made by analysts and the rationale behind them. -->

<!-- For junior researchers who lack guidance, this variability may lead to over reliance on default statistical software settings or arbitrary choices made without clear justification. -->

# Introduction

Data analysis is a complex and iterative process, and decisions are made at every stage of data analysis, from initial data collection, pre-processing, to modeling. One might expect well-trained researchers to make similar choices when faced with the same analytical task, yet evidence suggests otherwise. "Many-analyst" experiments show that independent analysts often arrive at markedly different conclusions, even when analyzing the same dataset to answer the same research question [@silberzahn2018; @botvinik-nezer2020; @gould2025]. This variation in analytical decision-making, described by @gelman2014 as the "garden of forking paths," can undermine the quality and credibility of reported results and raise uncertainty in the findings.

A common approach to investigate uncertainty in data analysis decisions is sensitivity analysis, where researchers systematically vary key decisions in their analysis to assess the robustness of their findings. Multiverse analysis extends this idea by evaluating *all* plausible combinations of decision choices to examine how results vary across the full decision space [@multiverse; @blair2019]. However, what one analyst considers reasonable may not reflect the full range of options used in practice. Even when a reasonable set of alternatives is tested, the stability shown by sensitivity analysis may be less relevant to other researchers with similar problems, who are often more interested in understanding the rationale behind decision choices. Ideally, decision-making in applied research can be studied by following experienced analysts throughout the entire analysis process to capture their reasoning. In reality, this is rarely feasible and not scalable. While individual studies may not capture the full range of decision choices used in practice, crowdsourcing decisions from a collection of studies on a shared theme creates a "many-analyst" setting that reveals how analysts make choices and justify them in practice. This process now has the possibility to be automated at scale, given recent advances in information extraction with Large Language Models (LLMs) [@harrod2024; @katz2024; @farzi2024; @hu2024; @sciannameo2024; @gu2025; @schilling-wilhelmi2025; @gupta2024; @li2024; @baddour2024; @polak2024].

In this work, we propose a new approach to studying data analysis decisions by automatically extracting decisions from scientific literature using LLMs. We develop a tabular schema to record decisions, automate the extraction process with LLMs, and introduce a new paper similarity measure based on decision similarity. This similarity measure can serve as a distance metric in dimension reduction methods to visualize papers according to their decisions. We apply this workflow to a set of `r length(unique(gemini_df$paper))` air pollution modeling studies that estimate the effect of particulate matter (PM2.5 or PM10) on mortality and hospital admissions, typically analyzed using Poisson generalized linear models (GLMs) or generalized additive models (GAMs). Analysis of the extracted decisions reveals common choices in this class of studies, including the use of smoothing methods on PM and weather variables and the temporal lags for time and weather variables. Multi-dimensional scaling on the paper similarity distance finds three distinct clusters corresponding to the smoothing methods used: LOESS, natural spline, and smoothing spline. These findings align with the APHENA project [@APHENA], which synthesizes research from multiple studies in Europe and North America. In this workflow, we also provide detailed documentation on the validation and standardization of LLM outputs. We outline the validation and standardization process, including the use of a developed Shiny application in R for reviewing decisions and the types of edits made through validation, the use of a secondary LLM to standardize reported choices of temporal lag decisions, and sensitivity analysis on reproducibility across runs and model providers. 

In summary, the contribution of this work includes:

-   A scalable and automated approach to study data analysis decisions through extracting of decisions from published scientific literature using LLMs,

-   A new method to construct paper similarities based on decision choices and the semantic similarity of their rationales,

-   Practices for validating and standardizing LLM outputs, including a shiny GUI tool for editing outputs, the use of secondary LLM for standardizing unstructured responses, and sensitivity analysis on reproducibility across runs and model providers,

-   A data schema for recording decisions in data analysis in a tidy format, and

-   A dataset of decisions, along with metadata, compiled from `r length(unique(gemini_df$paper))` studies in air pollution mortality modeling literature.

<!-- The rest of the paper is organized as follows. In @sec-background, we review the background on data analysis decisions. @sec-extract-decisions describes the data structure for recording decisions, the use of large language models to process research papers, and the validation of LLM outputs. In @sec-paper-similarity, we present the method for calculating paper similarity based on decision similarities. @sec-result reports the finding of our analysis, including the clustering of papers according to similarity scores and sensitivity analyses related to LLM providers, prompt engineering, and LLM parameters. Finally, @sec-discussion discusses the implications of our study. -->

# Related work {#sec-background}

## Analytic decision making in data analysis

Data analysis is a complex and iterative process [@jun2022; @jun2022hypothesis; @jun2019] that involves data collection, data cleaning, visualization, modeling, and communication. At each stage, analysts make decisions informed by domain practices, statistical knowledge, and the data. These decisions, such as which variables to include in a model, how to handle missing data, and how hyper-parameters are chosen, act as branching points in the analysis workflow. The full set of possible paths through these branching points forms what @gelman2014 describes as the "garden of forking paths". While one might expect well-trained researchers to make similar choices when facing similar decisions, empirical evidence suggests otherwise. "Many analyst experiments" show that independent research groups analyzing the same dataset to address the same research questions can arrive at widely different conclusions. For example, @silberzahn2018 asks 29 teams of analysts to conduct an analysis to address the same research questions *whether soccer players with dark skin tone are more likely than those with light skin tone to receive red cards from referees*. Researchers reported an estimated effect size from 0.89 to 2.93 in odds ratio, with 21 unique combinations of covariates used among all 29 analyses. 70% of the teams found a statistically significant positive effect, while others didn't. This great discrepancy among researchers when performing data analysis tasks is also observed in other domains, for example, in structural equation modeling [@sarstedt2024], applied microeconomics [@huntington-klein2021], neuroimaging [@botvinik-nezer2020], and ecology and evolutionary biology [@gould2025].

Examples like the above illustrate how analytical decisions introduce uncertainty into data analysis. These uncertainties have been widely discussed in the literature, given their impact for policy recommendation [@APHENA] and domain applications, e.g., fairness machine learning [@simson2025]. Through experiments [@wicherts2016; @simmons2011], research has shown that analysts' decisions can lead to p-hacking and inflated effect size when not properly used. Hence, guidelines and checklists have been developed to recommend the best practices to guide statistical analysis. In medicine and biostatistics, pre-registration is a common practice to regulate analysts making decisions after seeing the data. Given the nuanced nature of data analysis, more work has examined how analysts make decisions in practice through interviews in both academia and industry. These studies include qualitative analysis of the decisions made [@kale2019; @liu2020], interviews with data analysts about exploratory data analysis practice in industry [@alspaugh2019; @kandel2012], and about how they consider alternatives in data analysis [@liu_understanding_2020].

In addition to qualitative studies, software tools have been developed to help researchers account for alternatives and uncertainties and make informed decisions in data analysis. Examples include `Tea` [@jun2019], which supports general statistical analysis; `Tisane` [@jun2022], which guides choices in generalized linear mixed-effects models (GLMMs); and `MetaExplore` [@kale2023], which accounts for epistemic uncertainty (decision uncertainty) in meta-analysis. The `DeclareDesign` package [@blair2019] proposes the MIDA framework for researchers to declare, diagnose, and redesign their analyses to account for uncertainties of reporting the statistic of interest. Multiverse analysis proposes a different method to allow researchers to evaluate all plausible combinations of decision choices to examine how results vary in the full decision space. Work has been done on the software tools to support multiverse analysis [@multiverse; @götz2024] and visualization of multiverse results [@liu2021], and debugging tools [@gu2023].

## Automatic information extraction with LLMs

In natural language processing, information extraction is a task focused on extracting structured information from unstructured text. Earlier approaches in information extraction tasks relied on rule-based systems and regular expressions. More recent advances, including conditional random fields [@lafferty], word embeddings such as word2vec [@mikolov2013], and transformer-based architectures like BERT [@devlin2019], have led to the current use of LLM to extract information with prompts. Using LLMs to extract unstructured text offers the advantage of automating the process at scale. Applications have been seen in epidemiology data [@harrod2024], scientific literature [@katz2024], clinical data [@farzi2024; @hu2024; @sciannameo2024; @gu2025], chemistry knowledge [@schilling-wilhelmi2025], and polymer science [@gupta2024], climate extreme impact [@li2024], phenotypes [@baddour2024], and material properties [@polak2024]. An easier task in information extraction is called Named Entity Recognition (NER) to identify short span information (1-4 tokens) like person names and locations from unstructured text [@nadeau2007]. An example of this is extracting patients' information and vitals in clinical data. Extracting decisions from published literature is a more general task than NER, since justification of a decision typically spans more than just a few words. Our task also requires linking information across sentences, sometimes sections, to correctly identify the variables a decision refers to.

## Visualization on scientific literature

With the growing volume of scientific publications and the difficulty of navigating the literature, there is an increasing interest in developing systems to visualize and recommend scientific papers. These systems link papers based on their similarity and relevance, typically determined by keywords [@isenberg2017], citation information [@chen2006], e.g., citation list and co-citation, or combinations with other relevant paper metadata [@bethard2010; @chou2011; @dörk2012; @heimerl2016], e.g., author and title. Recent approaches incorporate text-based information using topic modeling [@alexander2014], argumentation-based information retrieval [@tbahriti2006], and text embedding [@narechania2022]. While metadata and high-level text-based information are useful for finding relevant papers, researchers also need tools that help them *make sense* of the literature rather than simply *locating* it. In applied data analysis, one interest is to understand how studies differ or align in their decision choices. Capturing the decision choices and reasons that justify the choices from analyses enables the calculation of similarity among papers and can be piped into dimension reduction methods and visualization for a global view of analysis practice in the field or recommend similar papers based on decision similarities.

# Methods {#sec-extract-decisions}

In this section, we present the workflow for extracting decisions from published literature using LLMs. We first describe the data structure for recording decisions, followed by the four main steps in the workflow: 1) automatic extraction of decisions from literature with LLMs, 2) validation and standardization of LLM outputs, 3) exploratory data analysis on analytic decisions, and 4) visualization of paper similarity using clustering or dimension reduction methods. The section concludes with an illustration summarizing the workflow.

## Record decisions in data analysis {#sec-decisions}

To analyze decisions, we first need to translate free-text descriptions of decisions in academic papers into a tabular format. We record decision following the tidy data principle [@wickham2014], which states that each variable forms a column and each observation forms a row. For our purpose, each row represents a single decision made in a paper, and an analysis typically involves multiple decisions. 

A decision generally consists of three components: the context, the reason, and the decision itself. To characterize the context of a decision, additional information is often required, such as what variable the decision is acted on, what statistical method the decision uses, and what parameter of the method is being decided. Some decisions do not involve a specific method or parameter, for example, a model may be estimated separately for each city rather than jointly across all cities. To account for this, we introduce a type identifier to distinguish whether a decision is parameter-, spatial-, or temporal-based. Method and parameter specifications are not required for spatial or temporal decisions at the model level. The resulting structure for recording decisions is as follows:

- `type`: one of "parameter", "spatial", or "temporal".
- `variable`: the variable to which the statistical method is applied.
- `method`: the statistical method used, (e.g. "LOESS", "smoothing spline", "natural spline").
- `parameter`: the parameter of the method being decided, (e.g. "degrees of freedom", "number of knots"). 
- `reason`: the justification for the decision. 
- `decision`: the final choice made.
- `reference`: any cited sources supporting the decision.

For the ease of tracking, we extract the original text describing each decision verbatim, without paraphrasing or summary. 


<!-- Consider the following excerpt from @ostro2006 modeling the association between daily counts of mortality and ambient particulate matter (PM10): -->

<!-- > Based on previous findings reported in the literature (e.g., Samet et al. 2000), the basic model included a smoothing spline for time with 7 degrees of freedom (df) per year of data. This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation. -->

<!-- This sentence encodes the following components of a decision: -->

<!-- -   **variable**: time -->
<!-- -   **method**: smoothing spline -->
<!-- -   **parameter**: degree of freedom (df) -->
<!-- -   **reason**: Based on previous findings reported in the literature (e.g., Samet et al. 2000); This number of degrees of freedom controls well for seasonal patterns in mortality and reduces and often eliminates autocorrelation. -->
<!-- -   **decision**: 7 degrees of freedom (df) per year of data -->

<!-- This decision can be recorded in a tabular format following the tidy data principle [@wickham2014], which states that each variable forms a column and each observation forms a row. For our purpose, each row represents a decision made in a paper, and an analysis often includes multiple decisions. We extract the original text in the paper, without paraphrasing or summary. The decision above is a parameter choice of a statistical method applied to the variable *time*. A data analysis may also include other types of decisions, such as temporal or spatial ones, for example, the choice of lagged exposure for certain variables or whether the model is estimated collectively or separated for individual locations. These decisions don't have a specific method or parameter fields, but should still include variable, type (spatial or temporal), reason, and decision fields. -->

<!-- Given the writing style of authors, multiple decisions may be combined in one sentence, and certain fields may be omitted. Consider a different excerpt from @ostro2006: -->

<!-- > Other covariates, such as day of the week and smoothing splines of 1-day lags of average temperature and humidity (each with 3 df), were also included in the model because they may be associated with daily mortality and are likely to vary over time in concert with air pollution levels. -->

<!-- This sentence contains four decisions: two for temperature (the temporal lag and the smoothing spline parameter) and two for humidity, and should be structured as separate entries: -->

<!-- | Paper | ID | variable | method | parameter | type | reason | decision | -->
<!-- |---------|---------|---------|---------|---------|---------|---------|---------| -->
<!-- | ostro | 1 | temperature | smoothing spline | degree of freedom | parameter | 3 degree of freedom | NA | -->
<!-- | ostro | 2 | relative humidity | smoothing spline | degree of freedom | parameter | 3 degree of freedom | NA | -->
<!-- | ostro | 3 | temperature | NA | NA | temporal | 1-day lags | NA | -->
<!-- | ostro | 4 | relative humidity | NA | NA | temporal | 1-day lags | NA | -->

<!-- Notice in the example above, the reason field is recorded as NA. This is because the stated reason ("and are likely to vary over time in concert with air pollution levels") only supports the general inclusion of temporal lags but does not justify the specific choice of 1-day lag over other alternatives, e.g., 2-day average of lags 0 and 1 or single-day lag of 2 days. Similar scenarios can happen when a direct decision choice is missing, but a reason is provided, as in @katsouyanni2001: -->

<!-- > The inclusion of lagged weather variables and the choice of smoothing parameters for all of the weather variables were done by minimizing Akaike’s information criterion. -->

## Extract decisions automatically from literature with LLMs

Manually extracting decisions from published papers is labor-intensive and time-consuming. Large language models (LLMs) make it possible to automate this process by providing a collection of PDF documents along with a structured prompt. In the prompt, the LLM is assigned the role of an applied statistician and instructed to extract decisions from each PDF document according to the format described in @sec-decisions. The prompt also notes that the reason and decision fields may be missing, with examples provided. We use the `ellmer` package [@ellmer] in R to interface with Anthropic Claude and Google Gemini for this task and the full prompt is provided in the Appendix.

## Validate and standardize LLM outputs

The LLM outputs need to be validated and standardized before further analysis. Validation focuses on ensuring the extracted decisions are correct, while standardization ensures that semantically equivalent terms are represented in a consistent form. For example, the expressions "mean temperature", "average temperature", and "temperature" all refer to the same variable and are standardized to "temperature". To help with the validation and standardization process, we developed a Shiny application for interactively reviewing and editing the LLM outputs. The Shiny application takes a CSV file as the input and allows users to perform three types of edits: 1) *overwrite*: modify the content of a particular cell, 2) *delete*: remove an irrelevant row (decision), and 3) *add*: manually enter a row (decision). 

@fig-shiny illustrates the *overwrite* action for standardizing the variable "NCtot" (number concentration of particles \<100 nm in diameter) to "pollution". The user enters a predicate function in the filter condition box on the left panel, and the filtered data will appear on the right panel. The user can then specify the variable to overwrite and the new value. The corresponding cells on the right panel will be updated. This change needs to be confirmed by pressing the "Apply changes" button to update to the full dataset. The corresponding `tidyverse` [@tidyverse] code will then be generated on the left panel to be included in an R script, and the edited table can be downloaded for future analysis.

```{r}
#| echo: false
#| label: fig-shiny
#| out.height: "80%"
#| out.width: "80%"
#| fig-cap: 'The Shiny application interface to validate and standardize Large Language Model (LLM)-generated output. (1) The default interface after loading the input CSV file. (2) The table view will update interactively to reflect the edit: for paper with handle "andersen2008size" and id in 4, 5, 6, modify the variable name *NCtot* to *pollutant*. (3) After clicking the Confirm button, the corresponding `tidyverse` code for the modification is generated, and the table view returns to its original unfiltered view with the edit applied. The edited data can be downloaded by clicking the Download CSV button.'
knitr::include_graphics("figures/shiny.png")
```

## Conduct exploratory data analysis on analytic decisions {#sec-paper-similarity}

Once the output has been extracted and validated, these decisions can be treated as data for exploratory data analysis. Analysts may summarize the missingness of decision choices and their associated reasons to better understand the reporting practice in the field. They may also examine the most frequent reported choices to identify common analytical practices. Grouping similar decision choices can further highlight patterns of uncommon practices that may require closer examination. 

Analysts may also be interested in whether there is structure in the decision choices across papers. For example, do certain papers tend to make similar choices? To address this question, we propose a paper similarity measure based on decision similarity. A decision is considered comparable between two papers if they share the same variable and decision type, for example, a parameter decision on temperature. To quantify similarity between matched decisions, we consider three aspects: 1) similarity of the decision choice, 2) similarity of the stated reasons, and 3) for parameter decisions, similarity of the statistical method used. Similarity in choice and method reflects the same analytical decision, while similarity in reason reflects a shared rationale, even when the choices differ due to differences in data. 

To measure similarity in choices and reasons, we obtain text embeddings and compute cosine similarity using the BERT model via the `text` package [@text] in `R`. Method similarity is encoded as a binary indicator (1 if the two papers used the same method, and 0 otherwise) since semantic similarity in text cannot reliably distinguish statistical methods. For example, the textual difference between "smoothing spline" and "natural spline" does not fully capture their methodological distinction. The overall paper similarity is computed as the average similarity across all matched methods, decisions, and reasons. 

Because each individual component is between 0 and 1, the overall similarity also lies between 0 and 1, with a value of 1 indicating identical decisions across all matched decisions. This metric does not account for the number of matched decisions between two papers, and a high (low) similarity score may be driven by a small number of matched decisions. Analysts may therefore choose to restrict attention to a set of common decisions shared across papers, or to pairs of papers that report a minimal number of matched decisions when computing paper similarity.

This similarity score can be used as a distance metric in dimension reduction methods, such as multidimensional scaling (MDS), to visualize how papers relate to each other based on their decisions across the literature. 

## Summary

@fig-workflow summarises the workflow proposed for extracting and analyzing decisions from published literature using LLMs. After identifying a set of relevant papers, a prompt has been designed to guide the LLM in extracting decisions from the documents. The extracted outputs are then validated and standardized before further analysis. The resulting dataset can then be used for exploratory data analysis and in addition, we propose a paper similarity metric tol quantify similarity in decision-making across papers. This measure can be interpreted as a distance metric between papers and used for clustering and dimension reduction to visualize patterns in decision-making across the literature.

```{r}
#| label: fig-workflow
#| fig-cap: "The workflow for extracting decisions from published literature using Large Language Models (LLMs) and analyzing the extracted decisions. The workflow consists of four main steps: (1) Extract decisions automatically from literature with LLMs, (2) Validate and standardize LLM outputs, (3) Conduct exploratory data analysis on analytic decisions, and (4) visualization with clustering or dimension reduction methods."
#| out.width: "100%"
knitr::include_graphics(here::here("figures/workflow.png"))
```

# Application {#sec-result}

In the study of the health effects of outdoor air pollution, one area of interest is the association between short-term, day-to-day changes in particulate matter air pollution and daily mortality counts. This question has been studied extensively by researchers across the globe, and it serves to provide scientific evidence in the US to guide public policy on setting the National Ambient Air Quality Standards (NAAQS) for air pollutants. While individual modeling choices vary, these studies often share a common structure: they adjust for meteorological covariates, such as temperature and humidity, include lagged variables to account for temporal correlations, and estimate the effect size by city or region before pooling the results with random effect. This naturally forms a "many-analyst" experiment setting to analyze decisions in air pollution mortality modelling.

We apply the workflow to extract the decisions in `r length(unique(gemini_df$paper))` studies reviewed in @atkinson_epidemiological_2014 that estimate the effect of particulate matter ($\text{PM}_{10}$ and $\text{PM}_{2.5}$) on mortality and hospital admission using Gemini (`gemini-2.0-flash`). We focus on the baseline model reported in each paper, excluding secondary models (e.g., lag-distributed models), multi-pollutant models, and alternatives tested in the sensitivity analysis, which are discussed in @sec-discussion. This yields `r nrow(gemini_df)` decisions extracted, averaging `r round(nrow(gemini_df)/length(unique(gemini_df$paper)))` decisions per paper.

## Validation and standardization of LLM outputs {#sec-res-validation}

```{r}
#| label: tbl-review
#| tbl-cap: "Summary of validation and standardization edits made during the review process. "
aa_raw <- readLines(here::here("clean-gemini.R"))
aa <- aa_raw[35:228]
res <- purrr::map_dfr(
  as.list(aa),
  ~tibble(origin = .x, 
          reason = ifelse(str_detect(.x, "#"),
                          str_replace(.x, ".*#\\s*", ""),
                          NA)
          )
  )

reason_df <- tibble(reason2 = c("recode", "irrelevant", "fail to capture", "duplicates", "general", "definition"),
       reason = c("Edit made to recode smoothing parameter unit to per year",
                  "Remove decisions out of scope: other pollutants and sensitivity analysis",
                  "Fix incorrect capture",
                  "Duplicates",
                  "Edit made due to decisions are too general, e.g. minimum of 1 df per year was required",
                  "Remove decisions related to definition of variables, e.g. season"))

res |> 
  mutate(reason2 = str_extract(reason, "^[^:]+")) |> 
  filter(!is.na(reason2), !reason2 == "category") |>
  count(reason2, sort = TRUE) |> 
  left_join(reason_df) |> 
  select(reason, n) |> 
  janitor::adorn_totals("row") |> 
  knitr::kable(col.names = c("Reason", "Count"))
```

@tbl-review summarizes the number of edits made during the review process using the Shiny application. Validation includes fixing incorrect captures, removing non-decision (e.g., definition of variables), removing duplication, excluding irrelevant decisions (e.g., sensitivity analyses), and excluding decisions whose stated reasons reflect general guidelines rather than actual choices (e.g., "minimum of 1 degree of freedom per year is required").

Standardization is performed on the variable names of decisions and choices. The variable name in the decisions are standardized into four main categories:

-   **temperature**: "mean temperature", "average temperature", "temperature", "air temperature", "ambient temperature"
-   **humidity**: "dewpoint temperature" and its hyphenated variants, relative humidity", "humidity"
-   **PM**: "pollutant", "pollution", "particulate matter", "particulate", "PM10", "PM2.5"
-   **time**: "date", "time", "trends", "trend"

Notice that "dewpoint temperature" is standardized under humidity because it serves as a proxy for temperature in achieving a 100% relative humidity.

Decisions themselves also require standardization. For example, the smoothing parameter (number of knots and degree of freedom) may be expressed as *per year* or *in total*, and temporal lag decision may be expressed in different formats (e.g., "6-day average", "mean of lags 0+1", "lagged exposure up to 6 days"). Decision choices on the smoothing parameter are manually recoded to a *per year* basis, as in @tbl-review. Temporal decisions show a wider variety, which makes manual standardization impractical. However, we observe that they generally fall into two categories:

-   **multi-day average lags**: "6-day average", "3-d moving average", "mean of lags 0+1", "cumulative lags, mean 0+1+2", and
-   **single-day lags**: "lagged exposure up to 6 days", "lag days from 0 to 5"

Hence we apply a secondary LLM (claude-3-7-sonnet-latest) to convert temporal decisions into a consistent format: `multi-day: lag [start]-[end]` and `single-day: lag [start], … ,lag [end]`. This converts "6-day average" into "multi-day: lag 0-5" and "lagged exposure up to 6 days" into "single-day: lag 0, lag 1, lag 2, lag 3, lag 4, lag 5".

## Exploratory analysis of decision choices

```{r}
#| label: tbl-missing-decisions
#| tbl-cap: "Missingness of decision and reason fields in the Gemini-extracted decisions. Most decisions report the choice (35.5 + 57.1 = 92%), but 57.1% lacks a stated reason."
dt <- gemini_df |> 
  mutate(decision = is.na(decision), reason = is.na(reason), .keep = "used" ) 
  
dt_tbl <- table(dt)
rownames(dt_tbl)  <- c("Non-missing", "Missing")
pct <- round(prop.table(dt_tbl) * 100, 1)
lab <- matrix(
  paste0(dt_tbl, " (", pct, "%)"),
  nrow = nrow(dt_tbl),
  dimnames = dimnames(dt_tbl)
)

lab |> knitr::kable(
  col.names = c("Decision", "Reason", "Non-missing", "Missing"),
  format = "latex", booktabs = TRUE) |> 
   kableExtra::add_header_above(c("", "Decision" = 2), 
                    escape = FALSE) 
```

In practice, data analysis decisions in academic papers are generally not presented individually in the format described in @sec-decisions. Authors may combine multiple related decisions into a single sentence for brevity, or omit certain components, not providing a reason for a decision or not stating the exact choice made. @tbl-missing-decisions summarizes the missingness of the decisions and the reason. While `r pct <- dt |> count(reason, decision) |> mutate(prop = n / sum(n)) |> filter(!reason, !decision) |> pull(prop); scales::percent_format()(pct)` of decisions are complete in both decision choices and reasons, `r pct <- dt |> count(reason, decision) |> mutate(prop = n / sum(n)) |> filter(reason, !decision) |> pull(prop); scales::percent_format()(pct)` of decisions lack a stated rationale for the choice. This reflects a common reporting practice in the field, where authors often report the decision choice used without an explicit reason.

```{r}
#| label: tbl-most-common-decisions
#| tbl-cap: "Count of variable-type decisions in the Gemini-extracted decisions. The most commonly reported decision are the parameter choices and temporal lags for for time, PM, temperature, and humidity."
count_variable_type(gemini_df) |> head(8) |> 
  knitr::kable(col.names = c("Variable", "Type", "Count"),)
```

@tbl-most-common-decisions lists the eight most frequently reported decisions: parameter and temporal choice for `time`, `PM`, `temperature`, and `humidity.` While a wider list of variables has been used in the analysis, these four variables are most commonly included in baseline models. This includes the smoothing parameter used for time, temperature, and humidity in the smoothing method (natural spline and smoothing spline) and temporal lag choices for PM, temperature, and humidity.

```{r}
#| label: tbl-humidity-temperature-decisions
#| tbl-cap: "Options captured for parameter choices for time, humidity, and temperature variables in the Gemini-extracted decisions. The choices for natural spline knots are generally less varied than the degree of freedom choices for smoothing spline. Choices for temperature and humidity tend to be close, given they are both weather related variables, while the choices for time are more varied inherently."
var_method <- gemini_df |> 
  filter(type == "parameter", 
         variable %in% c("humidity", "temperature", "time"),
         method %in% c("smoothing spline", "natural spline")) |> 
  mutate(decision = str_remove(decision, " knots| df"),
          decision = str_replace(decision, " or", ",")) |> 
  separate_longer_delim(decision, delim = c("; ")) |> 
  filter(!is.na(decision), !decision %in% c("smooth function")) |> 
  mutate(id = row_number()) |> 
  select(paper, variable, method, decision)

var_method_df <- var_method |>
  filter(paper != "schwartz1996daily") |> 
  mutate(decision = parse_number(decision)) |> 
  distinct(variable, method, decision) |> 
  arrange(variable, method, decision) |> 
  mutate(decision = as.character(decision)) |> 
  bind_rows(var_method |> 
              filter(paper == "schwartz1996daily") |> 
              mutate(decision = str_remove(decision, "in each neighborhood")) |> 
              select(-paper) 
            ) |> 
  filter(!is.na(decision))

var_method_df |>
   mutate(method == ifelse(method == "natural spline", "Natural spline (knot)", "Smoothing spline (df)")) |> 
  group_by(method, variable) |> 
  summarize(decision = paste0(decision, collapse = ", ")) |> 
  knitr::kable(col.names = c("Method", "Variable", "Decision")) 
```

@tbl-humidity-temperature-decisions presents the number of knots or degree of freedom used in two spline methods (natural and smoothing spline) applied to variable `time`, `humidity`,and `temperature`, with all values standardized to a *per year* scale. The choices of knots for natural spline have less variation than the degree of freedom choices for smoothing spline. Choices for temperature and humidity are generally similar, given that they are both weather-related variables, whereas choices for time are more varied. This tabulation provides a reference set for common parameter choices for future studies and helps to identify anomalies and special treatment in practice. For example, the choice of 7.7 degree of freedom reported in @castillejos2000 may prompt analysts to seek further justification for its use. By cross-comparing with other reporting, some decisions appear ambiguous. For example, in @moolgavkar2000 and @moolgavkar2003, the reported value of 30 and 100 degrees of freedom for time may be understandable for experienced domain researchers, but it can be unclear for junior analysts as to whether they refer to the parameter used for the full study period or on a per-year basis, which is often clear in other papers. We also observe a different report style from @schwartz2000, where smoothing spline parameters are expressed as a proportion of the data ("5% of the data" and "5% of the data"), rather than a fixed numerical value.

```{r}
#| label: tbl-temporal-decisions
#| tbl-cap: "Options captured for temporal lag choices for PM, temperature, and humidity variables in the Gemini-extracted decisions. Both single-day lags and multi-day average lags are commonly used, generally considering up to five days prior (lag 5)."
temporal_lags <- gemini_df |> 
  filter(type == "temporal") |> 
  filter(!paper %in% c("sheppard1999effects", "zanobetti2003temporal", "breitner2009short")) |>
  separate_longer_delim(decision, delim = "; ") |>
  arrange(variable, decision) |> 
  filter(variable %in% c("PM", "temperature", "humidity")) |> 
  mutate(type = ifelse(str_detect(decision, "multi-day"), "multi-day average", "single-day lag"),
         days = ifelse(type == "multi-day average", 
                       str_remove(decision, "multi-day lag "), 
                       str_remove(decision, "single-day "))) |> 
  separate_longer_delim(days, delim = ", ") |> 
  filter(!days %in% c("1-1", "0-0")) |> 
  mutate(days = ifelse(days == "1-3", "0-3", days)) |> 
  group_by(type, variable) |>
  distinct(variable, type, days) |>
  mutate(
    first = as.numeric(ifelse(str_detect(days, "-"), str_extract(days, "^[0-9]+"), str_extract(days, "^[0-9]+"))),
    last = as.numeric(ifelse(str_detect(days, "-"), str_extract(days, "[0-9]+$"), NA))) |> 
  arrange(variable, first, last) |> 
  summarize(decision = paste0(days, collapse = ", ")) |> 
  ungroup() |> 
  mutate(decision = paste0("lag ", str_remove_all(decision, "lag ")))

temporal_lags |> 
  knitr::kable(col.names = c("Lag type", "Variable", "Decision")) 
```

Similarly, @tbl-temporal-decisions summarizes the temporal lag choices for `PM`, `temperature`, and `humidity.` For single-day lags, the lags are considered up to 13 days (approximately two weeks) while for multi-day averages, 3-day and 5-day averages are the most common, although other choices such as 2-4 day average are also observed [@lópez-villarrubia2010].

## Paper similarity calculation, clustering analysis, and visualization

```{r}
#count_variable_type(gemini_df)
df <- gemini_df |> filter_var_type(n = 6)
#count_paper_decisions(df) 
# paper_df <- df |> filter_papers(n_value = 3)
# count_paper_pair_decisions(paper_df) 
# paper_df$paper |> unique()

good_pp <- gemini_df |> count(paper) |> filter(n >= 3) |> pull(paper)
paper_df <- df |> filter(paper %in% good_pp)
```

```{r embed, eval = FALSE}
embed_df <- paper_df |> compute_text_embed()
distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
distance_df <- distance_decision_df |> calc_paper_similarity()
save(distance_decision_df, file = here::here("data/distance_decision_df.rda"))
save(distance_df, file = here::here("data/distance_df.rda"))
```

Given the number of decisions reported in @tbl-most-common-decisions, we focus on the six most common variable-type decisions for calculating paper similarity: parameter choices for time, temperature, and humidity, and temporal lag choices for PM, temperature, and humidity. This choice allows us to compare papers based on a common set of decisions that are widely reported in the literature. We also restrict our analysis to papers that report at least three of these six decisions, resulting in `r length(good_pp)` papers for the paper similarity calculation. This ensures that the paper similarity metric is based on a sufficient number of comparable decisions. We use the default text embedding model (BERT) in the `text` package and cosine similarity to compute the similarity score. Sensitivity analysis on different text embedding models is checked in @sec-text-model. Paper similarity is then calculated as the average of decision similarity for each paper pair. The resulting similarity score is then used as the distance matrix in multi-dimensional scaling (MDS). 

@fig-mds displays the multidimensional scaling of the papers based on their pairwise (dis)similarities. The two MDS dimensions reveal three separated groups that align with the smoothing strategy adopted in the original analyses: LOESS, natural spline, and smoothing spline. The visual separation is supported by the silhouette diagnostic in @fig-sil, where all the studies exhibit a positive silhouette, suggesting no mis-classification and an average silhouette width is 0.723. This clustering pattern is consistent with the historical methodological divide documented in the APHENA project. Studies from the European APHEA [@katsouyanni1996] and APHEA2 [@katsouyanni2001] projects more commonly employed LOESS and smoothing splines, whereas the U.S.-based NMMAPS study [@samet2000] predominantly relies on natural splines.


```{r}
#| fig.height: 8
#| fig.width: 8
get_most_common_method <- function(df, cols = c("time_parameter_method", 
                                                "humidity_parameter_method", 
                                                "temperature_parameter_method")) {
  df %>%
    rowwise() %>%
    mutate(method = {
      vals <- c_across(all_of(cols))
      vals <- vals[!is.na(vals)]
      if (length(vals) == 0) NA_character_
      else names(sort(table(vals), decreasing = TRUE))[1]
    }) %>%
    ungroup() |> 
    select(paper, method)
}

method_df <- paper_df |> pivot_decision_tbl_wider() |> get_most_common_method() 

bad <- method_df |>
  filter(!method %in% c("LOESS", "natural spline", "smoothing spline")) |>
  pull(paper)
distance_df <- distance_df |> filter(!paper1 %in% bad & !paper2 %in% bad)
method_df <- method_df |> filter(!paper %in% bad)

dist_m <- to_dist_mtx(distance_df)
hmod <- hclust(dist_m, "ave")
ddata <- dendro_data(hmod)
ddata$labels <- ddata$labels |> left_join(method_df, by = c("label" = "paper"))
p2 <- ggplot() +
  geom_segment(data = segment(ddata), aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_text(data = ddata$labels, 
            aes(x = x, y = y, label = label, color = method, hjust = 0), 
            size = 3) +
  scale_y_reverse(expand = c(0.2, 0)) + 
  scale_color_brewer(palette = "Dark2") + 
  coord_flip() +
  theme_void() + 
  theme(legend.position = 'bottom')
  
mds_df <- run_mds(distance_df) |> left_join(method_df, by = c("paper" = "paper"))

p3 <- mds_df |> ggplot(aes(x = V1, y = V2)) + 
  ggrepel::geom_label_repel(aes(label = paper, color = method), 
                            max.overlaps = Inf ) + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(aspect.ratio = 1, legend.position = 'bottom') 
```

```{r}
#| fig.height: 10
#| fig.width: 10
#| label: fig-mds
#| fig-cap: "The multi-dimensional scaling (MDS) based on paper similarity distance for 56 air pollution mortality modeling papers, colored by the smoothing method used. The MDS reveals the three distinct groups of papers, corresponds to LOESS, natural spline, and smoothing spline. These groups corresponds to the different modeling strategies debated in the European and U.S. studies, as documented in the APHENA project [@APHENA]."
p3  
```

```{r}
#| fig.height: 3
#| fig.width: 8
#| label: fig-sil
#| fig-cap: "Medoid silhouette width for the three clusters identified from the multi-dimensional scaling (MDS) using paper similarity score as the distance. Each bar represents a study's medoid silhouette value, with the dashed line indiciating the overall average silhouette width (0.723). This diagnostic plot suggests strong within-cluster cohesion and clear separation between clusters corresponding to the three smoothing methods used: LOESS, natural spline, and smoothing spline."
centroid_df <- mds_df |> 
  filter(!is.na(method)) |> 
  group_by(method) |> 
  summarize(V1 = mean(V1), V2 = mean(V2)) |> 
  mutate(id = row_number())

library(Silhouette)
mat <- proxy::dist(mds_df[,2:3], centroid_df |> select(-method, -id))
sil <- Silhouette(mat) |> 
  left_join(centroid_df |> select(id, method), by = c("cluster" = "id")) |> 
  group_by(cluster) |> 
  arrange(cluster, desc(sil_width)) |> 
  ungroup(cluster) |> 
  mutate(id = row_number())

sil |> 
  ggplot() + 
  geom_col(ggplot2::aes(x = id, y = sil_width, fill = method, color = method))  + 
  geom_hline(yintercept =  mean(sil$sil_width), linetype = "dashed")  + 
  geom_text(aes(
    x = 43, y = mean(sil$sil_width) + 0.05, 
    label = round(mean(sil$sil_width), 3)
    )) + 
  scale_fill_brewer(palette = "Dark2", name = "method", 
                    aesthetics = c("color", "fill") ) + 
  theme_minimal() + 
  theme(legend.position = 'bottom', 
        axis.line.x = element_blank(), 
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank())  + 
  ylab("Medoid Silhouette Width") +
  xlab("") 
```

## Sensitivity analysis

A series of sensitivity analysis have been conducted to explore the reproducibility across runs (@sec-llm-reproducibility), model providers (@sec-llm-models), and the sensitivity of text model for computing the semantic decision similarity (@sec-text-model).

### LLM reproducibility {#sec-llm-reproducibility}

```{r eval = FALSE}
all_geminis <- list.files(here::here("data/gemini"), full.names = TRUE) |>
  purrr::map_dfr( ~ .x |> as_decision_tbl() |> arrange(variable)) |>
  mutate(id = str_remove(paper, "-gemini-[0-9]+"),
         reason = ifelse(reason == "NA", NA, reason ),
         decision = ifelse(decision == "NA", NA, decision ),
         ) |>
  mutate(
    reason = str_replace_all(reason, "/|,|-", ""),
    decision = str_replace_all(decision, "/|,|-", ""),
    reason = ifelse(!is.na(reason), 
                    paste0(tolower(substr(reason, 1, 1)), 
                           substr(reason, 2, nchar(reason))),
                    NA)
  )

all_claudes <- list.files(here::here("data/claude"), full.names = TRUE) |>
  purrr::map_dfr( ~ .x |> as_decision_tbl() |> arrange(variable)) |>
  mutate(id = str_remove(paper, "-claude-[0-9]+"),
         reason = ifelse(reason == "NA", NA, reason ),
         decision = ifelse(decision == "NA", NA, decision ),
         ) |>
  mutate(
    reason = str_replace_all(reason, "/|,|-", ""),
    decision = str_replace_all(decision, "/|,|-", ""),
    reason = ifelse(!is.na(reason), 
                    paste0(tolower(substr(reason, 1, 1)), 
                           substr(reason, 2, nchar(reason))),
                    NA)
  )


paper_id <- all_geminis |> group_split(id) |> map_chr(~.x |> pull(id) |> unique())
diff_df_raw <- paper_id |> map_dfr(function(paper_id) {
  res <- all_geminis |> filter(id == paper_id) |> group_split(paper)
  expand.grid(seq(1, 5, 1), seq(1, 5, 1)) |>
    filter(Var1 < Var2) |>
    rowwise() |>
    mutate(
      cmpr_obj = list(waldo::compare(
        res[[Var1]] |> select(reason, decision),
        res[[Var2]] |> select(reason, decision))), 
      ll = length(cmpr_obj),
      paper = paper_id, 
      same_n = nrow(res[[Var1]]) == nrow(res[[Var2]]))
})

llm_temp_df <- diff_df_raw |> 
  mutate(n_diff = NA) |> 
  mutate(n_diff = ifelse(ll == 2, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 3, 
                         str_count(cmpr_obj[[2]], "\033\\[33m") + 
                           str_count(cmpr_obj[[2]], "\033\\[32m")/2 + 
                           str_count(cmpr_obj[[3]], "\033\\[33m") + 
                           str_count(cmpr_obj[[3]], "\033\\[32m")/2, 
                         n_diff),
         n_diff = ifelse(ll == 0, 0, n_diff)) |> 
  ungroup() |> 
  select(-cmpr_obj)
#save(all_geminis, file = here::here("data/all_geminis.rda"))
#save(all_claudes, file = here::here("data/all_claudes.rda"))
#save(llm_temp_df, file = here::here("data/llm_temp_df.rda"))
```

```{r}
all_same_length_papers <- llm_temp_df |>
  group_by(paper) |> 
  reframe(a = unique(same_n)) |> 
  count(paper) |> 
  filter(n == 1) |> 
  pull(paper)

tbl <- llm_temp_df |> 
  filter(same_n) |> 
  count(n_diff) |> 
  mutate(prop = n / sum(n) * 100) 
```

```{r}
#| label: tbl-gemini-1
#| tbl-cap: "Example comparing Gemini's text extraction for @andersen2008 across two runs. The extracted decisions are identical in both runs."
res <-  all_geminis |> 
    filter(str_detect(paper, "andersen2008size")) |>
    group_split(paper)

tibble(Variable = res[[2]]$variable, 
       Run1 = res[[2]]$decision, 
       Run2 = res[[3]]$decision) |> 
  filter(!row_number() %in% c(1, 2)) |> 
  knitr::kable()
```

```{r}
#| label: tbl-gemini-2
#| tbl-cap: "Number of differences in the reason and decision fields across Gemini runs for papers with consistent number of decisions across runs."
load(here::here("data/llm_temp_df.rda"))
tibble(n_diff = 0:11) |> 
  left_join(tbl) |> 
  replace_na(list(n = 0, prop = 0)) |> 
  rename(`Num. of  difference` = n_diff, Count = n, `Proportion (%)` = prop) |> 
  janitor::adorn_totals() |> 
  knitr::kable(digits = 2)
```

We assess the reproducibility across runs of Gemini (`gemini-2.0-flash`) by repeating the text extract task five times and performing pairwise comparison between runs. This generates $5 \times 4 /2 \times 62 = 620$ possible comparisons for both "reason" and "decisions" fields. Comparisons are excluded when two runs produced a different number of decisions, since this would require manual alignment. This leaves `r nrow(llm_temp_df |> filter(!is.na(n_diff)))` out of `r nrow(llm_temp_df)` (`r scales::label_percent()(nrow(llm_temp_df |> filter(!is.na(n_diff)))/nrow(llm_temp_df))`) extractions to compare. @tbl-gemini-1 prints a comparison of decisions in @andersen2008 across two runs, and all four decisions are identical with no difference. @tbl-gemini-2 summarizes the number of differences observed in each pairwise comparison. Among all comparisons, `r scales::label_percent()(tbl$prop[[1]] / 100)` produces the identical text in reason and decision. The discrepancies mainly come from the following two reasons:

1)  Gemini extracted the same decision in different lengths. For example, in @kan2007, some runs may extract "singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average **of current and previous day concentrations** (lag=01)", while others extract "singleday lag models underestimate the cumulative effect of pollutants on mortality 2day moving average (lag=01)".

2)  Gemini fails to extract reasons in some runs but not others. For example, in @burnett1998, the first run generates `NA` in the reason, but the remaining four runs are identical, with the reason populated. In @ueda2009 and @castillejos2000 , runs 1 and 5 fail to extract the reason and produce the same incomplete version, whereas runs 2, 3, and 4 produce accurate versions with reason populated.

### LLM models {#sec-llm-models}

```{r}
#| label: fig-claude-gemini
#| fig.height: 8
#| fig.width: 8
#| fig-cap: "Comparison of decisions extracted by Claude and Gemini. Each point represents a paper, with the x- and y-axis showing the number of decisions extracted by Claude and Gemini, respectively. The dashed 1:1 line marks where both models extract the same number of decisions. More points fall below this line, suggesting Claude extracts more decisions -- often including noise from data pre-processing or secondary data analysis steps -- which requires additional manual validation."
claude_gemini <- all_claudes |> 
  mutate(paper = str_remove(paper, "-claude-1")) |>
  group_by(paper) |> 
  count(paper) |> 
  left_join(all_geminis |> filter(str_detect(paper, "-gemini-1")) |> 
             mutate(paper = str_remove(paper, "-gemini-1")) |>
             group_by(paper) |> 
             count(paper), by = "paper") |> 
  rename(claude = n.x, gemini = n.y)

claude_gemini |> 
  ggplot(aes(x = claude, y = gemini)) + 
  geom_label(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff >= 6),
            aes(label = paper), nudge_x = 0.2) + 
   geom_point(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff >= 6), color = "red") + 
  geom_jitter(data = claude_gemini |> 
               mutate(diff = abs(claude - gemini)) |> filter(diff < 6),
              width = 0.2, height = 0.2) + 
  geom_abline(slope = 1, intercept = 0, linetype = "dashed") +
  #geom_abline(slope = 1, intercept = -4, color = 'purple') +
  #geom_abline(slope = 1, intercept = 4, color = "purple") +
  theme_bw() + 
  theme(aspect.ratio = 1, panel.grid.minor = element_blank()) + 
  xlab("Num. of decisions extracted by Claude") + 
  ylab("Num. of decisions extracted by Gemini")

```

We compare the number of decisions extracted by Gemini (`gemini-2.0-flash`) and Claude (`claude-3-7-sonnet-latest`) across all `r  length(unique(all_geminis$paper |> str_remove("-gemini-[0-9]")))` papers. In @fig-claude-gemini, each point represents a paper, with the x- and y-axis showing the number of decisions extracted by Claude and Gemini, respectively. The dashed 1:1 line marks where both models extract the same number of decisions. In general, the two models produce a similar number of decisions. However, more points fall below this line, suggesting Claude extracts more decisions, often including noise from data pre-processing or secondary data analysis steps. Examples of papers with large discrepancies include @mar2000 (Claude: 10 vs. Gemini: 28), @ito2006 (Claude: 25 vs. Gemini: 19), @ko2007 (Claude: 8 vs. Gemini: 16), among others. For both Claude and Gemini, we find they sometimes fail to link the general term "weather variables" to the specific weather variables (e.g., @dockery1992 and @burnett2004 for Gemini and @dockery1992 and @katsouyanni2001 for Claude). Although our prompt specified that some decisions may require linking information across sentences and paragraphs to identify the correct variable, this instruction doesn't appear to be applied consistently.

### Text model {#sec-text-model}

```{r eval = FALSE}
t1 <- Sys.time()
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
text_sensitivity_decision_df <- map_dfr(models,  ~{
  print(.x)
  embed_df <- paper_df |> compute_text_embed(text_model = .x) 
  distance_decision_df <- calc_decision_similarity(paper_df, embed = embed_df)
  
  return(distance_decision_df)
}, .id = "id") 
t2 <- Sys.time()
t2 - t1
#save(text_sensitivity_decision_df, file = here::here("data/text_sensitivity_decision_df.rda"))
```

```{r}
#| fig.height: 6.5
#| fig.width: 9
#| fig.cap: "Distribution of decision similarity (left) and multi-dimensional scaling (MDS) of the paper similarity scores (right) computed for five different text models (BERT, BioBERT, RoBERTa, SciBERT, and XLNet). The default language model, BERT, produces the widest variation across the five models, while the similarity scores form XLNet are all close to 1. The model BioBERT, RoBERTa, and SciBERT yield decision similar scores mostly between 0.7 to 1. All the text models shows a similar clustering structure based on the three main smoothing methods (LOESS, natural spline and smoothing spline)."
#| label: fig-text-model
models <- c("bert-base-uncased", "roberta-base", "xlnet-base-cased", 
           "allenai/scibert_scivocab_uncased", "dmis-lab/biobert-large-cased-v1.1-squad")
models_df <- tibble(id = 1:5, models = models, name = c("BERT", "RoBERTa", "XLNet", "SciBERT", "BioBERT"))

p1 <- text_sensitivity_decision_df |> 
  mutate(id = as.numeric(id)) |> 
  filter(!str_detect(decision, "method")) |> 
  left_join(models_df) |>
  ggplot(aes(x = dist)) + 
  geom_density(alpha = 0.3, fill = "grey") + 
  facet_wrap(vars(name), ncol = 1, scales = "free_y") + 
  scale_x_continuous(breaks = seq(0.2, 1, 0.1)) + 
  xlab("Decision similarity scores") + 
  theme_bw()

text_distance_df <- text_sensitivity_decision_df |>
  nest(data = -id) |>
  mutate(id = as.numeric(id)) |>
  rowwise() |>
  mutate(score = list(calc_paper_similarity(data))) |>
  unnest(score) |>
  select(-data) |>
  left_join(models_df)

# p2 <- text_distance_df |> 
#   ggplot(aes(x = similarity)) + 
#   geom_density(alpha = 0.3, fill = "grey") + 
#   facet_wrap(vars(name), ncol = 1) + 
#   xlab("Paper similarity scores")

mds_df <- text_distance_df |> 
  filter(!paper1 %in% bad & !paper2 %in% bad) |> 
  group_split(id) |> 
  map_dfr(run_mds, .id = "id") |> 
  mutate(id = as.numeric(id)) |> 
  left_join(method_df, by = c("paper" = "paper")) |> 
  left_join(models_df) |> 
  mutate(V2 = ifelse(name == "BioBERT", -V2, V2))


p3 <- mds_df |> ggplot(aes(x = V1, y = V2, gorup = method)) + 
  geom_point(aes(color = method)) +
  ggforce::geom_mark_hull(concavity = 5, aes(color = method), 
                          expand = unit(2.2, "mm")) + 
  facet_wrap(vars(name), ncol = 2) + 
  scale_color_brewer(palette = "Dark2") + 
  theme_bw() + 
  theme(aspect.ratio = 1, legend.position = "bottom")  

p1 + p3 + plot_layout(widths = c(1.5, 1), guides = 'collect') & 
  theme(legend.position = 'bottom')
```

We have conducted sensitivity analyses on the text model for calculating the decision similarity score from the Gemini outputs. The tested language models include 1) BERT [@devlin2019] by Google, 2) RoBERTa [@liu] by Facebook AI, trained on a larger dataset (160GB v.s. BERT's 15GB), 3) XLNnet [@yang] by Google Brain, and two domain-trained BERT models: 4) sciBERT [@beltagy2019], trained on scientific literature, and 5) bioBERT [@lee2020], trained on PubMed and PMC data.

@fig-text-model shows the distribution of the decision similarity and the corresponding multi-dimensional scaling visualization, where distances are calculated from the paper similarity for each text model. At the decision level, the BERT model produces the widest variation across all five models, while the similarity scores from XLNet are all close to 1. While the raw scores are not directly comparable across models due to the difference in the underlying transformer architecture, the visualizations from multi-dimensional scaling (MDS) based on paper similarity scores all show a similar clustering pattern corresponding to the three main smoothing methods (LOESS, natural spline, and smoothing spline).

# Discussion {#sec-discussion}

## Large-language models for information extraction

Numerous studies [@harrod2024; @katz2024; @farzi2024; @hu2024; @sciannameo2024; @gu2025; @schilling-wilhelmi2025; @gupta2024; @li2024; @baddour2024; @polak2024] have demonstrated the capability of LLMs for information extraction tasks. Our work applies the LLMs to extract analytic decisions in scientific literature, providing further evidence of their effectiveness. Our task requires capturing more complex analytical decisions and their justifications, which typically span more than just a few tokens, like in named entity recognition. Our task also requires linking information across sentences and sometimes sections to correctly identify the variables of a decision (e.g., linking "weather" to "temperature" and "humidity"). While LLM has performed well on extracting decisions from the literature, manual validations are still required to ensure the quality of the extracted decisions for downstream analysis. Most existing applications evaluate LLMs by comparing their outputs to human-annotated datasets, reporting metrics such as precision, recall, and F1 score. Because this approach depends on labeled data, and it is not yet clear how these outputs should be validated for downstream analysis in practice. In our work, we automate some of the manual validation with a secondary LLM (Claude) to standardize the temporal lag choices in different expressions into two categories.

With a default temperature of one and the prompt to instruct the model to extract the original text rather than paraphrase, we find that hallucination is not a major issue with Claude and Gemini in this application. Since LLM outputs are inherently probabilistic, we also conduct sensitivity analyses on reproducibility across runs and model providers. The output is generally stable: repeated runs with the Gemini produce consistent results, and different models extracted a similar number of decisions.

While we optimize the prompt for decision extraction in this work, an alternative approach is to fine-tune a local model to enhance LLM performance. A catered local model could be useful for extraction decisions for a comprehensive literature review on a larger scale, but it would require greater model training efforts with labeled data.

## Extracting other types of decisions

In this work, we focus on modeling decisions for the baseline model in the air pollution epidemiology literature. Analyses in this field often fit multiple models for different health outcomes and use secondary models, such as distributed lag models and multi-pollutant models, to estimate relative risks and multi-pollutant interactions. These increase the complexity of decision extraction with LLMs because authors often only describe the differences from the baseline specification, implicitly assuming other decisions remain unchanged. Hence, LLMs will need to link the decisions across different models and reconstruct the complete set of decisions for each model.

Beyond modeling choices, decisions in data pre-processing are also interesting to compare. For example, @braga2001 aggregated air pollution measures from multiple PM10 monitors within the same location into a single value. Pre-processing choices such as data source, aggregation method, imputation also have an impact on the uncertainty of the estimated effect size of particulate matter. However, these decisions are often not properly and adequately described in the manuscript, making it impossible to extract by LLMs. Proper documentation and reporting standards in pre-processing decisions are needed before our workflow could be applied to pre-processing decisions.

With growing advocacy for reproducibility, papers nowadays are expected to share code and data, if applicable. Code availability provides a useful supplementary source for identifying decision choices and cross-checking them against descriptions in the manuscript. However, while the script may reveal what choices were made, the rationales behind these choices are often not documented under the current practice.

<!-- Spatial decisions are generally not well captured because it often conducted uniformly as estimating the city individually to accommodate city heterogeneity. Some papers only consider a handful of cities, while in larger studies the individual city effects are then pooled together using random effect. -->

## Generalizability of the workflow

In principle, our workflow is scalable and generalizable to a random set of applied papers. However, insights about the data analysis practices are more likely to be revealed when papers share certain similarities. For example, literature on the same topic but different authors allows for understanding of common practices within a field, literature using the same methodology across different disciplines allows comparisons of the same statistical method across fields; and literature that considers the same variables can show how those variables are used in different domains.

Our LLM prompt for extracting decisions will need to be customized for each application of the workflow. The general prompt structure and the data schema for recording decisions can be reused, while examples within the prompt may be adapted to suit the specific application. The shiny application for interactively validating and standardizing decisions can be reused across applications. Calculating paper similarity requires comparing decisions on the same variable and type across paper pairs. For papers with limited similarities, the number of comparable decisions may be limited. Diagnostic functions are available to display decisions side by side or provide summary statistics on the number of comparable decisions. Uncertainty visualization on the paper similarity score can be used to highlight the confidence with respect to the number of comparable decisions.

As a new method for collecting analytic decision data from literature, our workflow can be connected to meta-analysis to assess how different decisions influence results. More broadly, it can also be integrated into literature search and recommender systems to suggest similar papers based on the analytic decisions they employ.

# Conclusion {#sec-conclusion}

In this paper, we developed a scalable and generalizable pipeline for automatically extracting analytical decisions using LLMs from scientific literature to study how analysts make decisions in data analysis. We also introduced a method for calculating paper similarity through comparing the similarities among decision choices, and the similarity metric can be used as a distance to cluster papers by their decision choices and visualization with dimension reduction algorithms, such as multidimensional scaling. We applied this pipeline to a set of air pollution modeling literature that associates daily particulate matter and daily mortality and hospital admissions. From the extracted modeling decisions, we identify the most common decision choices in this type of analysis, and the paper similarity score calculation revealed the three clusters of paper corresponding to different smoothing methods. 

Many work on studying decision-making in data analysis conduct qualitative interviews with a small number of analysts to understand their decision-making process. "Many-analysts" studies gather together analysts in a controlled experiment to observe analysts conduct the analysis. Our approach is also observational in nature, but we "observe" analysts in real-world problems with real data that have policy implications, while being scalable and cost-effective for a broader exploration of decision-making practices in different contexts and disciplines. Compared to sensitivity analysis or multiverse analysis, our approach offers a different perspective by pooling together decisions made in analyses across the field to reveal the options considered to highlight uncertainty in decisions that require further sensitivity analyses to assess their impact [@peng2006; @touloumi2006].

# Acknowledgement

The article has been created using Quarto [@Allaire_Quarto_2022] in R
[@R]. The source code for reproducing the work reported in this paper
can be found at: <https://github.com/huizezhang-sherry/paper-decisions>. The tools developed to support this methodology are available as an R package, `dossier` on GitHub at <https://github.com/huizezhang-sherry/dossier>.


# References {.unnumbered}
